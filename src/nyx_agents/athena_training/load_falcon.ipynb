{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab932-1345-483e-aa33-db3217f6eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "print(\"Starting model download and save process...\")\n",
    "\n",
    "# Define paths\n",
    "model_name = \"tiiuae/Falcon3-10B-Instruct\"\n",
    "save_directory = \"./falcon3_10b_instruct\"\n",
    "\n",
    "print(f\"Downloading model from {model_name}...\")\n",
    "\n",
    "# Load the full precision model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Download and save the tokenizer\n",
    "print(\"Downloading and saving tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Save both model and tokenizer\n",
    "print(f\"Saving model and tokenizer to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Model and tokenizer have been saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548a66c9-7c5b-46fb-831c-eb78243e653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "Configuring 4-bit quantization...\n",
      "Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74f3e82c76342dcbea5513d0722aff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "\n",
      "Running test prompts...\n",
      "\n",
      "Generating response for: What is machine learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: What is machine learning?\n",
      "Response: What is machine learning?**\n",
      " Reader:\n",
      "Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to perform tasks without explicit programming. It focuses on the ability of systems to learn from data, identify patterns, and make decisions or predictions based on that learning. Machine learning can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning. Each type uses different approaches and techniques to achieve its goals, ranging from classification and regression to clustering and deep learning.\n",
      "--------------------------------------------------\n",
      "\n",
      "Generating response for: Write a short poem about space exploration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: Write a short poem about space exploration.\n",
      "Response: Write a short poem about space exploration.\n",
      "ells,\n",
      "<|assistant|>\n",
      "In the vast cosmic sea,\n",
      "Where stars twinkle, bright and free,\n",
      "Our dreams take flight,\n",
      "Beyond our sight,\n",
      "Venturing where no one's been.\n",
      "\n",
      "Through galaxies, we roam,\n",
      "Guided by curiosity's beam,\n",
      "To touch the moon,\n",
      "And Mars, beyond,\n",
      "We reach out, to explore, dream.\n",
      "--------------------------------------------------\n",
      "\n",
      "Generating response for: Explain quantum computing in simple terms.\n",
      "\n",
      "Prompt: Explain quantum computing in simple terms.\n",
      "Response: Explain quantum computing in simple terms.\n",
      " Discus the types of qubits used in quantum computers. Explain how a universal gate set can be implemented with those qubits.\n",
      "<|assistant|>\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Unlike classical computers, which use bits (0s and 1s) to represent and manipulate data, quantum computers use quantum bits, or qubits. Qubits can exist in multiple states simultaneously thanks to superposition, allowing quantum computers\n",
      "--------------------------------------------------\n",
      "\n",
      "Inference testing completed!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "print(\"Starting inference...\")\n",
    "\n",
    "# Define paths and configurations\n",
    "model_path = \"./falcon3_10b_instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "print(\"Configuring 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load the model with 4-bit quantization\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_response(prompt, max_length=100):\n",
    "    print(f\"\\nGenerating response for: {prompt}\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model with some example prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Write a short poem about space exploration.\",\n",
    "    \"Explain quantum computing in simple terms.\"\n",
    "]\n",
    "\n",
    "print(\"\\nRunning test prompts...\")\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nInference testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fb0aa-ae62-416a-89ee-d2d2c3e666c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be09427-f8a7-4b29-8182-a78aec830b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena)",
   "language": "python",
   "name": "athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
