{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b112ec1b-c96e-4e5c-b7a5-ba5ec8130f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1273be87-54d5-485b-9a87-0b01787e146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        self.qa_file = \"qa_data_final_clean.jsonl\"\n",
    "        self.output_dir = \"mistral_qlora_finetuned\"\n",
    "        self.max_length = 1024\n",
    "        self.num_train_epochs = 7\n",
    "        \n",
    "        # Dynamic batch size with safety margin\n",
    "        self.per_device_train_batch_size = self._find_optimal_batch_size()\n",
    "        self.gradient_accumulation_steps = self._calculate_gradient_accumulation()\n",
    "        \n",
    "        # Conservative learning parameters\n",
    "        self.learning_rate = 1e-4  # Reduced from 2e-4\n",
    "        self.warmup_ratio = 0.05   # Increased from 0.03\n",
    "        self.save_steps = 500\n",
    "        self.logging_steps = 25\n",
    "        self.eval_steps = 250\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.output_dir, \"checkpoints\"), exist_ok=True)\n",
    "        \n",
    "    def _find_optimal_batch_size(self) -> int:\n",
    "        if not torch.cuda.is_available():\n",
    "            return 1\n",
    "            \n",
    "        total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        # More conservative batch sizes\n",
    "        if total_gpu_memory >= 24:\n",
    "            return 1  # Reduced from 2\n",
    "        return 1\n",
    "\n",
    "    def _calculate_gradient_accumulation(self) -> int:\n",
    "        # Adjust gradient accumulation based on batch size\n",
    "        return 32 if self.per_device_train_batch_size == 1 else 16\n",
    "\n",
    "    @property\n",
    "    def lora_config(self) -> LoraConfig:\n",
    "        return LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\", \n",
    "                \"ml_proj\", \"ml_ln\"\n",
    "            ],\n",
    "            lora_dropout=0.1,  # Increased from 0.05\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            fan_in_fan_out=False,\n",
    "            inference_mode=False,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def quantization_config(self) -> BitsAndBytesConfig:\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_skip_modules=[\"lm_head\"],\n",
    "            llm_int8_threshold=6.0,\n",
    "            bnb_4bit_quant_storage=torch.float16\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0f5e36-9198-4a4a-83a3-0c0fa5281a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMemoryMonitor:\n",
    "    def __init__(self, log_dir: str, memory_threshold: float = 0.95):\n",
    "        self.writer = SummaryWriter(log_dir=log_dir)\n",
    "        self.step = 0\n",
    "        self.memory_threshold = memory_threshold\n",
    "        self.warning_emitted = False\n",
    "    \n",
    "    def check_memory(self) -> bool:\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory\n",
    "            if gpu_memory_used > self.memory_threshold:\n",
    "                if not self.warning_emitted:\n",
    "                    logging.warning(f\"GPU memory usage critical: {gpu_memory_used:.2%}\")\n",
    "                    self.warning_emitted = True\n",
    "                return False\n",
    "        \n",
    "        cpu_percent = psutil.virtual_memory().percent / 100\n",
    "        if cpu_percent > self.memory_threshold:\n",
    "            if not self.warning_emitted:\n",
    "                logging.warning(f\"CPU memory usage critical: {cpu_percent:.2%}\")\n",
    "                self.warning_emitted = True\n",
    "            return False\n",
    "            \n",
    "        self.warning_emitted = False\n",
    "        return True\n",
    "\n",
    "    def log_memory(self, prefix: str = \"\") -> Dict[str, float]:\n",
    "        memory_stats = {}\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "            memory_stats.update({\n",
    "                'gpu_allocated': gpu_memory_allocated,\n",
    "                'gpu_reserved': gpu_memory_reserved\n",
    "            })\n",
    "            self.writer.add_scalar(f'{prefix}GPU Memory/Allocated (MB)', gpu_memory_allocated, self.step)\n",
    "            self.writer.add_scalar(f'{prefix}GPU Memory/Reserved (MB)', gpu_memory_reserved, self.step)\n",
    "            \n",
    "            total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "            gpu_usage_percent = (gpu_memory_allocated / total_gpu_memory) * 100\n",
    "            self.writer.add_scalar(f'{prefix}GPU Memory/Usage (%)', gpu_usage_percent, self.step)\n",
    "        \n",
    "        cpu_memory = psutil.Process().memory_info().rss / 1024**2\n",
    "        memory_stats['cpu'] = cpu_memory\n",
    "        self.writer.add_scalar(f'{prefix}CPU Memory (MB)', cpu_memory, self.step)\n",
    "        \n",
    "        cpu_percent = psutil.virtual_memory().percent\n",
    "        self.writer.add_scalar(f'{prefix}CPU Memory/Usage (%)', cpu_percent, self.step)\n",
    "        \n",
    "        self.step += 1\n",
    "        return memory_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db9640a-3df1-4e4f-b24d-5dc7f0ca23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def format_qa(self, example: Dict[str, Any]) -> Dict[str, str]:\n",
    "        try:\n",
    "            instruction = example.get(\"instruction\", \"\").strip()\n",
    "            response = example.get(\"response\", \"\").strip()\n",
    "            \n",
    "            if not instruction or not response:\n",
    "                return None\n",
    "                \n",
    "            instruction = instruction.replace(\"\\n\\n\", \"\\n\").strip()\n",
    "            response = response.replace(\"\\n\\n\", \"\\n\").strip()\n",
    "            \n",
    "            text = (\n",
    "                f\"### System: You are a helpful AI assistant that answers user queries accurately and helpfully.\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "                f\"### Response:\\n{response}</s>\"\n",
    "            )\n",
    "            return {\"text\": text}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing example: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def tokenize_function(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=config.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "    def validate_dataset(self, dataset) -> Dict[str, Any]:\n",
    "        \"\"\"Validate dataset quality and report statistics.\"\"\"\n",
    "        logging.info(\"Validating dataset...\")\n",
    "        stats = {\n",
    "            \"total_examples\": len(dataset),\n",
    "            \"empty_instructions\": 0,\n",
    "            \"empty_responses\": 0,\n",
    "            \"instruction_lengths\": [],\n",
    "            \"response_lengths\": []\n",
    "        }\n",
    "        \n",
    "        for example in tqdm(dataset, desc=\"Validating dataset\"):\n",
    "            instruction = example.get(\"instruction\", \"\").strip()\n",
    "            response = example.get(\"response\", \"\").strip()\n",
    "            \n",
    "            if not instruction:\n",
    "                stats[\"empty_instructions\"] += 1\n",
    "            if not response:\n",
    "                stats[\"empty_responses\"] += 1\n",
    "                \n",
    "            stats[\"instruction_lengths\"].append(len(instruction.split()))\n",
    "            stats[\"response_lengths\"].append(len(response.split()))\n",
    "        \n",
    "        stats[\"avg_instruction_length\"] = float(np.mean(stats[\"instruction_lengths\"]))\n",
    "        stats[\"avg_response_length\"] = float(np.mean(stats[\"response_lengths\"]))\n",
    "        \n",
    "        logging.info(f\"Dataset Statistics:\\n{json.dumps(stats, indent=2)}\")\n",
    "        return stats\n",
    "\n",
    "    def process_dataset(self, qa_file: str):\n",
    "        dataset = load_dataset(\"json\", data_files=qa_file)[\"train\"]\n",
    "        self.validate_dataset(dataset)\n",
    "        \n",
    "        # Process and filter dataset\n",
    "        dataset = dataset.map(\n",
    "            self.format_qa,\n",
    "            remove_columns=dataset.column_names,\n",
    "            num_proc=4,\n",
    "        ).filter(lambda x: x[\"text\"] is not None)\n",
    "        \n",
    "        # Split dataset\n",
    "        dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "        \n",
    "        # Tokenize datasets\n",
    "        tokenized_dataset = {}\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            tokenized_dataset[split] = dataset[split].map(\n",
    "                self.tokenize_function,\n",
    "                remove_columns=[\"text\"],\n",
    "                num_proc=4,\n",
    "                load_from_cache_file=True,\n",
    "                desc=f\"Tokenizing {split} split\",\n",
    "            )\n",
    "            tokenized_dataset[split].set_format(\"torch\")\n",
    "        \n",
    "        return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636a84f7-f736-45be-a38e-33a308152fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeTrainer(Trainer):\n",
    "    def __init__(self, *args, memory_monitor: Optional[EnhancedMemoryMonitor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.memory_monitor = memory_monitor or EnhancedMemoryMonitor(\n",
    "            log_dir=self.args.logging_dir,\n",
    "            memory_threshold=0.95\n",
    "        )\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):  # Added num_items_in_batch parameter\n",
    "        \"\"\"Perform a training step with memory monitoring.\"\"\"\n",
    "        if not self.memory_monitor.check_memory():\n",
    "            logging.warning(\"Memory usage too high, triggering garbage collection\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            if not self.memory_monitor.check_memory():\n",
    "                self.args.per_device_train_batch_size = max(1, self.args.per_device_train_batch_size - 1)\n",
    "                logging.warning(f\"Reduced batch size to {self.args.per_device_train_batch_size}\")\n",
    "        \n",
    "        self.memory_monitor.log_memory(\"training/\")\n",
    "        return super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "    def evaluation_step(self, model, inputs):\n",
    "        \"\"\"Perform an evaluation step with memory monitoring.\"\"\"\n",
    "        if not self.memory_monitor.check_memory():\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        self.memory_monitor.log_memory(\"eval/\")\n",
    "        return super().evaluation_step(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70eab01-f061-4798-a318-f70997cdff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c376a63-9b74-4175-8057-3334c262a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s - %(funcName)s:%(lineno)d\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(f\"{config.output_dir}/training.log\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256d1a3e-1de8-4ddf-9a5f-4139513ed674",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_monitor = EnhancedMemoryMonitor(\n",
    "    log_dir=os.path.join(config.output_dir, \"tensorboard\"),\n",
    "    memory_threshold=0.90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e06ba9-f291-4a25-b9ba-1e40c2cf579f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487b4ff1-8a77-48cf-abc0-f6ce36ea849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49089114749e4213bec67cb10ce0dcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 01:57:17 - INFO - root - Validating dataset... - validate_dataset:37\n",
      "Validating dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 540/540 [00:00<00:00, 82217.37it/s]\n",
      "2024-12-10 01:57:17 - INFO - root - Dataset Statistics:\n",
      "{\n",
      "  \"total_examples\": 540,\n",
      "  \"empty_instructions\": 0,\n",
      "  \"empty_responses\": 0,\n",
      "  \"instruction_lengths\": [\n",
      "    4,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    7,\n",
      "    8,\n",
      "    4,\n",
      "    6,\n",
      "    8,\n",
      "    8,\n",
      "    10,\n",
      "    7,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    3,\n",
      "    8,\n",
      "    8,\n",
      "    6,\n",
      "    5,\n",
      "    7,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    9,\n",
      "    12,\n",
      "    12,\n",
      "    12,\n",
      "    7,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    13,\n",
      "    12,\n",
      "    11,\n",
      "    11,\n",
      "    15,\n",
      "    7,\n",
      "    11,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    11,\n",
      "    12,\n",
      "    11,\n",
      "    11,\n",
      "    14,\n",
      "    11,\n",
      "    11,\n",
      "    12,\n",
      "    11,\n",
      "    13,\n",
      "    8,\n",
      "    11,\n",
      "    14,\n",
      "    9,\n",
      "    13,\n",
      "    13,\n",
      "    15,\n",
      "    11,\n",
      "    9,\n",
      "    8,\n",
      "    10,\n",
      "    5,\n",
      "    10,\n",
      "    10,\n",
      "    10,\n",
      "    6,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    8,\n",
      "    9,\n",
      "    8,\n",
      "    6,\n",
      "    6,\n",
      "    4,\n",
      "    7,\n",
      "    8,\n",
      "    9,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    8,\n",
      "    7,\n",
      "    7,\n",
      "    6,\n",
      "    4,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    9,\n",
      "    6,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    7,\n",
      "    10,\n",
      "    10,\n",
      "    8,\n",
      "    9,\n",
      "    9,\n",
      "    8,\n",
      "    7,\n",
      "    8,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    10,\n",
      "    10,\n",
      "    9,\n",
      "    7,\n",
      "    9,\n",
      "    12,\n",
      "    11,\n",
      "    7,\n",
      "    12,\n",
      "    12,\n",
      "    9,\n",
      "    7,\n",
      "    9,\n",
      "    13,\n",
      "    12,\n",
      "    9,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    11,\n",
      "    10,\n",
      "    8,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    6,\n",
      "    10,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    10,\n",
      "    6,\n",
      "    11,\n",
      "    9,\n",
      "    8,\n",
      "    8,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    15,\n",
      "    13,\n",
      "    10,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    12,\n",
      "    13,\n",
      "    10,\n",
      "    9,\n",
      "    9,\n",
      "    9,\n",
      "    9,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    10,\n",
      "    8,\n",
      "    13,\n",
      "    9,\n",
      "    11,\n",
      "    6,\n",
      "    8,\n",
      "    9,\n",
      "    6,\n",
      "    12,\n",
      "    10,\n",
      "    13,\n",
      "    8,\n",
      "    10,\n",
      "    9,\n",
      "    15,\n",
      "    10,\n",
      "    14,\n",
      "    12,\n",
      "    11,\n",
      "    10,\n",
      "    12,\n",
      "    13,\n",
      "    14,\n",
      "    12,\n",
      "    10,\n",
      "    20,\n",
      "    12,\n",
      "    20,\n",
      "    20,\n",
      "    12,\n",
      "    14,\n",
      "    12,\n",
      "    10,\n",
      "    11,\n",
      "    12,\n",
      "    12,\n",
      "    11,\n",
      "    13,\n",
      "    13,\n",
      "    9,\n",
      "    13,\n",
      "    13,\n",
      "    16,\n",
      "    14,\n",
      "    16,\n",
      "    15,\n",
      "    17,\n",
      "    17,\n",
      "    14,\n",
      "    12,\n",
      "    12,\n",
      "    13,\n",
      "    15,\n",
      "    15,\n",
      "    14,\n",
      "    23,\n",
      "    16,\n",
      "    16,\n",
      "    16,\n",
      "    16,\n",
      "    14,\n",
      "    14,\n",
      "    14,\n",
      "    16,\n",
      "    16,\n",
      "    16,\n",
      "    15,\n",
      "    17,\n",
      "    17,\n",
      "    14,\n",
      "    12,\n",
      "    12,\n",
      "    13,\n",
      "    15,\n",
      "    15,\n",
      "    14,\n",
      "    23,\n",
      "    12,\n",
      "    14,\n",
      "    13,\n",
      "    10,\n",
      "    13,\n",
      "    9,\n",
      "    12,\n",
      "    12,\n",
      "    12,\n",
      "    15,\n",
      "    11,\n",
      "    4,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    7,\n",
      "    8,\n",
      "    4,\n",
      "    6,\n",
      "    8,\n",
      "    8,\n",
      "    10,\n",
      "    7,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    3,\n",
      "    8,\n",
      "    8,\n",
      "    6,\n",
      "    5,\n",
      "    7,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    9,\n",
      "    12,\n",
      "    12,\n",
      "    12,\n",
      "    7,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    13,\n",
      "    12,\n",
      "    11,\n",
      "    11,\n",
      "    15,\n",
      "    7,\n",
      "    11,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    9,\n",
      "    8,\n",
      "    7,\n",
      "    8,\n",
      "    6,\n",
      "    6,\n",
      "    7,\n",
      "    6,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    9,\n",
      "    8,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    5,\n",
      "    8,\n",
      "    8,\n",
      "    11,\n",
      "    10,\n",
      "    9,\n",
      "    7,\n",
      "    5,\n",
      "    8,\n",
      "    8,\n",
      "    11,\n",
      "    14,\n",
      "    9,\n",
      "    4,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    7,\n",
      "    8,\n",
      "    4,\n",
      "    6,\n",
      "    8,\n",
      "    8,\n",
      "    10,\n",
      "    7,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    6,\n",
      "    3,\n",
      "    8,\n",
      "    8,\n",
      "    6,\n",
      "    5,\n",
      "    7,\n",
      "    7,\n",
      "    9,\n",
      "    11,\n",
      "    9,\n",
      "    12,\n",
      "    12,\n",
      "    12,\n",
      "    7,\n",
      "    6,\n",
      "    7,\n",
      "    8,\n",
      "    7,\n",
      "    9,\n",
      "    7,\n",
      "    8,\n",
      "    13,\n",
      "    12,\n",
      "    11,\n",
      "    11,\n",
      "    15,\n",
      "    7,\n",
      "    11,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    11,\n",
      "    10,\n",
      "    7,\n",
      "    11,\n",
      "    12,\n",
      "    11,\n",
      "    11,\n",
      "    14,\n",
      "    11,\n",
      "    11,\n",
      "    12,\n",
      "    11,\n",
      "    13,\n",
      "    8,\n",
      "    11,\n",
      "    14,\n",
      "    9,\n",
      "    13,\n",
      "    13,\n",
      "    15,\n",
      "    11,\n",
      "    9,\n",
      "    8,\n",
      "    10,\n",
      "    5,\n",
      "    10,\n",
      "    10,\n",
      "    10,\n",
      "    6,\n",
      "    9,\n",
      "    9,\n",
      "    10,\n",
      "    8\n",
      "  ],\n",
      "  \"response_lengths\": [\n",
      "    46,\n",
      "    22,\n",
      "    32,\n",
      "    41,\n",
      "    40,\n",
      "    39,\n",
      "    32,\n",
      "    33,\n",
      "    34,\n",
      "    32,\n",
      "    27,\n",
      "    25,\n",
      "    39,\n",
      "    54,\n",
      "    32,\n",
      "    49,\n",
      "    34,\n",
      "    38,\n",
      "    31,\n",
      "    38,\n",
      "    35,\n",
      "    36,\n",
      "    37,\n",
      "    28,\n",
      "    33,\n",
      "    34,\n",
      "    35,\n",
      "    27,\n",
      "    34,\n",
      "    30,\n",
      "    35,\n",
      "    29,\n",
      "    38,\n",
      "    31,\n",
      "    31,\n",
      "    31,\n",
      "    36,\n",
      "    38,\n",
      "    33,\n",
      "    36,\n",
      "    40,\n",
      "    35,\n",
      "    29,\n",
      "    34,\n",
      "    37,\n",
      "    42,\n",
      "    41,\n",
      "    37,\n",
      "    42,\n",
      "    39,\n",
      "    49,\n",
      "    42,\n",
      "    41,\n",
      "    44,\n",
      "    45,\n",
      "    42,\n",
      "    39,\n",
      "    48,\n",
      "    49,\n",
      "    48,\n",
      "    49,\n",
      "    44,\n",
      "    52,\n",
      "    50,\n",
      "    44,\n",
      "    50,\n",
      "    46,\n",
      "    49,\n",
      "    46,\n",
      "    51,\n",
      "    39,\n",
      "    40,\n",
      "    38,\n",
      "    44,\n",
      "    42,\n",
      "    41,\n",
      "    46,\n",
      "    45,\n",
      "    43,\n",
      "    34,\n",
      "    33,\n",
      "    49,\n",
      "    50,\n",
      "    39,\n",
      "    55,\n",
      "    40,\n",
      "    43,\n",
      "    40,\n",
      "    41,\n",
      "    44,\n",
      "    42,\n",
      "    38,\n",
      "    40,\n",
      "    42,\n",
      "    42,\n",
      "    44,\n",
      "    43,\n",
      "    42,\n",
      "    42,\n",
      "    35,\n",
      "    37,\n",
      "    40,\n",
      "    32,\n",
      "    37,\n",
      "    46,\n",
      "    41,\n",
      "    43,\n",
      "    40,\n",
      "    40,\n",
      "    41,\n",
      "    37,\n",
      "    36,\n",
      "    41,\n",
      "    37,\n",
      "    39,\n",
      "    33,\n",
      "    33,\n",
      "    42,\n",
      "    38,\n",
      "    34,\n",
      "    38,\n",
      "    39,\n",
      "    44,\n",
      "    41,\n",
      "    37,\n",
      "    35,\n",
      "    43,\n",
      "    40,\n",
      "    44,\n",
      "    40,\n",
      "    48,\n",
      "    39,\n",
      "    34,\n",
      "    44,\n",
      "    39,\n",
      "    49,\n",
      "    36,\n",
      "    37,\n",
      "    41,\n",
      "    33,\n",
      "    37,\n",
      "    37,\n",
      "    38,\n",
      "    43,\n",
      "    45,\n",
      "    42,\n",
      "    36,\n",
      "    34,\n",
      "    38,\n",
      "    32,\n",
      "    45,\n",
      "    43,\n",
      "    41,\n",
      "    42,\n",
      "    44,\n",
      "    44,\n",
      "    39,\n",
      "    42,\n",
      "    38,\n",
      "    45,\n",
      "    37,\n",
      "    43,\n",
      "    33,\n",
      "    39,\n",
      "    43,\n",
      "    38,\n",
      "    47,\n",
      "    42,\n",
      "    40,\n",
      "    41,\n",
      "    42,\n",
      "    38,\n",
      "    36,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    35,\n",
      "    38,\n",
      "    49,\n",
      "    40,\n",
      "    41,\n",
      "    33,\n",
      "    42,\n",
      "    40,\n",
      "    43,\n",
      "    36,\n",
      "    47,\n",
      "    38,\n",
      "    38,\n",
      "    39,\n",
      "    42,\n",
      "    38,\n",
      "    40,\n",
      "    39,\n",
      "    39,\n",
      "    37,\n",
      "    37,\n",
      "    36,\n",
      "    37,\n",
      "    37,\n",
      "    42,\n",
      "    44,\n",
      "    38,\n",
      "    34,\n",
      "    35,\n",
      "    37,\n",
      "    39,\n",
      "    40,\n",
      "    37,\n",
      "    39,\n",
      "    35,\n",
      "    42,\n",
      "    41,\n",
      "    45,\n",
      "    44,\n",
      "    44,\n",
      "    43,\n",
      "    43,\n",
      "    52,\n",
      "    36,\n",
      "    42,\n",
      "    50,\n",
      "    45,\n",
      "    45,\n",
      "    41,\n",
      "    50,\n",
      "    40,\n",
      "    46,\n",
      "    44,\n",
      "    52,\n",
      "    51,\n",
      "    50,\n",
      "    48,\n",
      "    49,\n",
      "    46,\n",
      "    48,\n",
      "    56,\n",
      "    48,\n",
      "    52,\n",
      "    66,\n",
      "    52,\n",
      "    50,\n",
      "    46,\n",
      "    49,\n",
      "    45,\n",
      "    48,\n",
      "    47,\n",
      "    48,\n",
      "    48,\n",
      "    42,\n",
      "    43,\n",
      "    41,\n",
      "    46,\n",
      "    48,\n",
      "    43,\n",
      "    50,\n",
      "    47,\n",
      "    42,\n",
      "    65,\n",
      "    62,\n",
      "    53,\n",
      "    53,\n",
      "    51,\n",
      "    53,\n",
      "    60,\n",
      "    57,\n",
      "    48,\n",
      "    57,\n",
      "    57,\n",
      "    52,\n",
      "    54,\n",
      "    47,\n",
      "    53,\n",
      "    59,\n",
      "    52,\n",
      "    51,\n",
      "    51,\n",
      "    54,\n",
      "    47,\n",
      "    65,\n",
      "    160,\n",
      "    175,\n",
      "    202,\n",
      "    204,\n",
      "    225,\n",
      "    280,\n",
      "    356,\n",
      "    311,\n",
      "    421,\n",
      "    394,\n",
      "    504,\n",
      "    42,\n",
      "    27,\n",
      "    32,\n",
      "    33,\n",
      "    30,\n",
      "    37,\n",
      "    30,\n",
      "    35,\n",
      "    33,\n",
      "    31,\n",
      "    25,\n",
      "    24,\n",
      "    41,\n",
      "    43,\n",
      "    23,\n",
      "    33,\n",
      "    30,\n",
      "    38,\n",
      "    30,\n",
      "    37,\n",
      "    31,\n",
      "    30,\n",
      "    30,\n",
      "    31,\n",
      "    25,\n",
      "    32,\n",
      "    29,\n",
      "    29,\n",
      "    32,\n",
      "    31,\n",
      "    28,\n",
      "    20,\n",
      "    29,\n",
      "    29,\n",
      "    28,\n",
      "    30,\n",
      "    27,\n",
      "    31,\n",
      "    32,\n",
      "    28,\n",
      "    31,\n",
      "    34,\n",
      "    27,\n",
      "    32,\n",
      "    29,\n",
      "    32,\n",
      "    28,\n",
      "    31,\n",
      "    29,\n",
      "    28,\n",
      "    46,\n",
      "    36,\n",
      "    38,\n",
      "    33,\n",
      "    36,\n",
      "    35,\n",
      "    28,\n",
      "    36,\n",
      "    33,\n",
      "    33,\n",
      "    32,\n",
      "    30,\n",
      "    31,\n",
      "    34,\n",
      "    35,\n",
      "    35,\n",
      "    33,\n",
      "    30,\n",
      "    35,\n",
      "    32,\n",
      "    36,\n",
      "    33,\n",
      "    30,\n",
      "    26,\n",
      "    30,\n",
      "    38,\n",
      "    30,\n",
      "    34,\n",
      "    36,\n",
      "    31,\n",
      "    46,\n",
      "    22,\n",
      "    32,\n",
      "    41,\n",
      "    40,\n",
      "    39,\n",
      "    32,\n",
      "    33,\n",
      "    34,\n",
      "    32,\n",
      "    27,\n",
      "    25,\n",
      "    39,\n",
      "    54,\n",
      "    32,\n",
      "    49,\n",
      "    34,\n",
      "    38,\n",
      "    31,\n",
      "    38,\n",
      "    35,\n",
      "    36,\n",
      "    37,\n",
      "    28,\n",
      "    33,\n",
      "    34,\n",
      "    35,\n",
      "    27,\n",
      "    34,\n",
      "    30,\n",
      "    35,\n",
      "    29,\n",
      "    38,\n",
      "    31,\n",
      "    31,\n",
      "    31,\n",
      "    36,\n",
      "    38,\n",
      "    33,\n",
      "    36,\n",
      "    40,\n",
      "    35,\n",
      "    29,\n",
      "    34,\n",
      "    37,\n",
      "    42,\n",
      "    41,\n",
      "    37,\n",
      "    42,\n",
      "    39,\n",
      "    49,\n",
      "    42,\n",
      "    41,\n",
      "    44,\n",
      "    45,\n",
      "    42,\n",
      "    39,\n",
      "    48,\n",
      "    49,\n",
      "    48,\n",
      "    49,\n",
      "    44,\n",
      "    52,\n",
      "    50,\n",
      "    44,\n",
      "    50,\n",
      "    46,\n",
      "    49,\n",
      "    46,\n",
      "    51,\n",
      "    39,\n",
      "    40,\n",
      "    38,\n",
      "    44,\n",
      "    42,\n",
      "    41,\n",
      "    46,\n",
      "    45,\n",
      "    43,\n",
      "    34\n",
      "  ],\n",
      "  \"avg_instruction_length\": 9.78888888888889,\n",
      "  \"avg_response_length\": 44.73148148148148\n",
      "} - validate_dataset:61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94077d3c07b4f8e811c5fe60da81f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdece6f058624b8797dc54abd7fa6595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56262e06b1594bf2beb793a77f88e865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split (num_proc=4):   0%|          | 0/486 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f929bfaeb4864607801caa51b016c87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test split (num_proc=4):   0%|          | 0/54 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    use_fast=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Process dataset\n",
    "dataset_processor = DatasetProcessor(tokenizer)\n",
    "tokenized_datasets = dataset_processor.process_dataset(config.qa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3367a460-050f-47d2-96b0-d6d850988133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 01:57:19 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk). - get_balanced_memory:1014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c4ce66f5c442fd8ea4923ec282009b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=config.quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model = get_peft_model(model, config.lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6eab7c-787d-426f-8a92-2b775373d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athena/anaconda3/envs/athena/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.output_dir, \"checkpoints\"),\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    logging_steps=config.logging_steps,\n",
    "    # Match evaluation_strategy with save_strategy\n",
    "    evaluation_strategy=\"steps\",  # Added this line\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_first_step=True,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=True,\n",
    ")\n",
    "\n",
    "trainer = SafeTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8\n",
    "    ),\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,\n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "    ],\n",
    "    memory_monitor=memory_monitor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e569f05-78b6-4261-af1d-d8c33065a9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 36:31, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean up memory before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(config.output_dir)\n",
    "    model.save_pretrained(config.output_dir)\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Training failed: {str(e)}\")\n",
    "    # Save emergency checkpoint\n",
    "    try:\n",
    "        emergency_dir = os.path.join(config.output_dir, \"emergency_checkpoint\")\n",
    "        os.makedirs(emergency_dir, exist_ok=True)\n",
    "        trainer.save_model(emergency_dir)\n",
    "        logging.info(f\"Saved emergency checkpoint to {emergency_dir}\")\n",
    "    except Exception as save_error:\n",
    "        logging.error(f\"Failed to save emergency checkpoint: {str(save_error)}\")\n",
    "    raise\n",
    "finally:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    memory_monitor.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51918d27-eb00-46fa-bfcf-85d6c12e3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model... This may take a few moments.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 02:34:22 - INFO - root - Loading base model... - __init__:41\n",
      "2024-12-10 02:34:22 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk). - get_balanced_memory:1014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f7488cf20745c6a973d705e293e33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 02:34:26 - INFO - root - Loading fine-tuned adapter... - __init__:50\n",
      "2024-12-10 02:34:26 - INFO - root - Model initialization complete! - __init__:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is ready! Type 'quit' to exit.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import re\n",
    "import gc\n",
    "import signal\n",
    "import sys\n",
    "\n",
    "class InferenceWrapper:\n",
    "    def __init__(self, \n",
    "                 base_model_name=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "                 adapter_path=\"mistral_qlora_finetuned\",\n",
    "                 temperature=0.6,\n",
    "                 max_new_tokens=512):\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        \n",
    "        # Configure logging\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            level=logging.INFO\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Initialize tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Quantization configuration\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Loading base model...\")\n",
    "            # Load base model with quantization\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Loading fine-tuned adapter...\")\n",
    "            self.model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "            logging.info(\"Model initialization complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during model initialization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def remove_emojis_and_clean(self, text):\n",
    "        \"\"\"Remove emojis and clean up text.\"\"\"\n",
    "        # Remove emojis and special characters\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub('', text)\n",
    "        \n",
    "        # Remove repeated characters\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Clean up extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text.strip()\n",
    "\n",
    "    def generate_response(self, instruction):\n",
    "        \"\"\"Generate a response for the given instruction.\"\"\"\n",
    "        try:\n",
    "            # Format the prompt\n",
    "            prompt = (\n",
    "                f\"### System: You are a helpful AI assistant that answers user queries \"\n",
    "                f\"accurately and professionally. Avoid using emojis or special characters.\\n\"\n",
    "                f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "                f\"### Response:\\n\"\n",
    "            )\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                generation_output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    temperature=self.temperature,\n",
    "                    top_p=0.9,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            # Decode and clean response\n",
    "            response = self.tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "            response = response.split(\"### Response:\\n\")[-1].strip()\n",
    "            response = self.remove_emojis_and_clean(response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating response: {str(e)}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        try:\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            logging.info(\"Cleanup complete\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    \"\"\"Handle interruption signals.\"\"\"\n",
    "    print(\"\\nInterrupted! Cleaning up...\")\n",
    "    if 'inference' in globals():\n",
    "        inference.cleanup()\n",
    "    sys.exit(0)\n",
    "\n",
    "def main():\n",
    "    # Register signal handler for clean exit\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    \n",
    "    try:\n",
    "        # Initialize inference wrapper\n",
    "        print(\"Initializing model... This may take a few moments.\")\n",
    "        global inference\n",
    "        inference = InferenceWrapper()\n",
    "        print(\"\\nModel is ready! Type 'quit' to exit.\")\n",
    "        \n",
    "        # Interactive loop\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nEnter your question: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit']:\n",
    "                    print(\"Cleaning up and exiting...\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    print(\"Please enter a question.\")\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = inference.generate_response(user_input)\n",
    "                print(\"\\nResponse:\", response)\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nInterrupted by user. Cleaning up...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                print(\"Please try again.\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        if 'inference' in globals():\n",
    "            inference.cleanup()\n",
    "        print(\"\\nGoodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23ece3-1a9a-49d3-8a96-9e9b9c9f50e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena)",
   "language": "python",
   "name": "athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
