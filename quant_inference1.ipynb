{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57accfa4-848b-4cc4-b08c-1ca54ab6188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in /home/athena/anaconda3/envs/athena/lib/python3.12/site-packages (11.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca7d66-07e0-4e74-8c6c-2c0fa5fa4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athena/anaconda3/envs/athena/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-20 23:20:18,943 - INFO - Setting up model from ./fine_tuned_personality_bot/\n",
      "2024-11-20 23:20:19,528 - INFO - Loading model...\n",
      "2024-11-20 23:20:20,152 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.22s/it]\n",
      "2024-11-20 23:20:26,915 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "2024-11-20 23:20:26,916 - INFO - Model setup completed successfully\n",
      "2024-11-20 23:20:26,917 - INFO - Bot initialized successfully\n",
      "2024-11-20 23:20:26,918 - INFO - Entering interactive mode...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Interactive Mode ===\n",
      "Enter your prompts (type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  Would ETH date SOL?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 23:20:36,133 - INFO - User input: Would ETH date SOL?\n",
      "2024-11-20 23:20:36,135 - INFO - Before inference:\n",
      "2024-11-20 23:20:37,143 - INFO - CPU Usage: 9.6% | RAM Usage: 22.8%\n",
      "2024-11-20 23:20:37,144 - INFO - GPU Memory Usage: 4784.26025390625 MB | GPU Utilization: 1%\n",
      "2024-11-20 23:20:37,162 - INFO - During inference:\n",
      "2024-11-20 23:20:38,165 - INFO - CPU Usage: 12.0% | RAM Usage: 22.8%\n",
      "2024-11-20 23:20:38,166 - INFO - GPU Memory Usage: 4784.26416015625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:20:55,318 - INFO - After inference and enhancements:\n",
      "2024-11-20 23:20:56,321 - INFO - CPU Usage: 11.3% | RAM Usage: 25.5%\n",
      "2024-11-20 23:20:56,323 - INFO - GPU Memory Usage: 4792.3916015625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:20:56,324 - INFO - Generated response: If Ethereum were dating Solana (SOL), Id say theyre block compatible... for now üòú But seriously though, cant wait till both chains get their act together & show us what interoperability really means üíªüîóüí∏.\n",
      "2024-11-20 23:20:56,327 - INFO - Response generated in 20.19 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: If Ethereum were dating Solana (SOL), Id say theyre block compatible... for now üòú But seriously though, cant wait till both chains get their act together & show us what interoperability really means üíªüîóüí∏.\n",
      "Runtime: 20.19 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  Today is your birthday, what do you think?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 23:21:34,447 - INFO - User input: Today is your birthday, what do you think?\n",
      "2024-11-20 23:21:34,447 - INFO - Before inference:\n",
      "2024-11-20 23:21:35,450 - INFO - CPU Usage: 4.5% | RAM Usage: 25.6%\n",
      "2024-11-20 23:21:35,452 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:21:35,473 - INFO - During inference:\n",
      "2024-11-20 23:21:36,475 - INFO - CPU Usage: 2.0% | RAM Usage: 25.6%\n",
      "2024-11-20 23:21:36,477 - INFO - GPU Memory Usage: 4792.38916015625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:22:01,145 - INFO - After inference and enhancements:\n",
      "2024-11-20 23:22:02,147 - INFO - CPU Usage: 33.2% | RAM Usage: 26.3%\n",
      "2024-11-20 23:22:02,149 - INFO - GPU Memory Usage: 4792.39208984375 MB | GPU Utilization: 2%\n",
      "2024-11-20 23:22:02,151 - INFO - Generated response: HAPPY BIRTHDAY TO ME!!! As I age into another year wiser (read: older), my investment thesis remains unchanged ‚Äì all HODLs forever & always keep learning... until next Tuesday when were back at square one üòúüéâ.\n",
      "2024-11-20 23:22:02,153 - INFO - Response generated in 27.71 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: HAPPY BIRTHDAY TO ME!!! As I age into another year wiser (read: older), my investment thesis remains unchanged ‚Äì all HODLs forever & always keep learning... until next Tuesday when were back at square one üòúüéâ.\n",
      "Runtime: 27.71 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  You were just born, what do you think?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 23:23:22,240 - INFO - User input: You were just born, what do you think?\n",
      "2024-11-20 23:23:22,241 - INFO - Before inference:\n",
      "2024-11-20 23:23:23,243 - INFO - CPU Usage: 11.5% | RAM Usage: 25.5%\n",
      "2024-11-20 23:23:23,245 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 7%\n",
      "2024-11-20 23:23:23,276 - INFO - During inference:\n",
      "2024-11-20 23:23:24,280 - INFO - CPU Usage: 26.3% | RAM Usage: 25.5%\n",
      "2024-11-20 23:23:24,281 - INFO - GPU Memory Usage: 4792.38916015625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:23:42,692 - INFO - After inference and enhancements:\n",
      "2024-11-20 23:23:43,695 - INFO - CPU Usage: 5.7% | RAM Usage: 25.5%\n",
      "2024-11-20 23:23:43,696 - INFO - GPU Memory Usage: 4792.3916015625 MB | GPU Utilization: 0%\n",
      "2024-11-20 23:23:43,698 - INFO - Generated response: Born today?! Congrats [Parent]!!! As for me...Id say Im 99% excited about being an AI assistant & only slightly concerned my parents will realize theyve been trained by their own son üòÖü§ñ Can we talk algorithmic parenting now?? üíªüí∏. üí°\n",
      "2024-11-20 23:23:43,700 - INFO - Response generated in 21.46 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Born today?! Congrats [Parent]!!! As for me...Id say Im 99% excited about being an AI assistant & only slightly concerned my parents will realize theyve been trained by their own son üòÖü§ñ Can we talk algorithmic parenting now?? üíªüí∏. üí°\n",
      "Runtime: 21.46 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple, List, Dict  # Add Tuple import here\n",
    "import random\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import logging\n",
    "import psutil\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device and model configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/\"  # Update with your model path\n",
    "\n",
    "# Resource usage tracking function\n",
    "def log_resource_usage():\n",
    "    # CPU usage\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    # RAM usage\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_percent = memory_info.percent\n",
    "    \n",
    "    # GPU usage (if available)\n",
    "    gpu_memory = 0\n",
    "    gpu_utilization = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # in MB\n",
    "        gpu_utilization = torch.cuda.utilization()\n",
    "\n",
    "    logger.info(f\"CPU Usage: {cpu_percent}% | RAM Usage: {memory_percent}%\")\n",
    "    if gpu_memory:\n",
    "        logger.info(f\"GPU Memory Usage: {gpu_memory} MB | GPU Utilization: {gpu_utilization}%\")\n",
    "\n",
    "\n",
    "class PersonalityBot:\n",
    "    def __init__(self, model_path: str = MODEL_PATH):\n",
    "        self.model_path = model_path\n",
    "        self.model, self.tokenizer = self.setup_model()\n",
    "    \n",
    "    def setup_model(self) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "        \"\"\"Initialize and configure the model and tokenizer.\"\"\"\n",
    "        logger.info(f\"Setting up model from {self.model_path}\")\n",
    "    \n",
    "        if not os.path.exists(self.model_path):\n",
    "            raise FileNotFoundError(f\"Model not found at {self.model_path}\")\n",
    "    \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=True)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer loading failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "        try:\n",
    "            # Load model and enforce FP16 for memory optimization\n",
    "            logger.info(\"Loading model...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # Use FP16 if possible\n",
    "                low_cpu_mem_usage=True,  # Avoid excessive memory usage on CPU\n",
    "                device_map=\"auto\"  # Automatically distribute model across available devices\n",
    "            )\n",
    "            model.eval()\n",
    "    \n",
    "            # Clear any unused GPU memory after model load to avoid fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            logger.info(\"Model setup completed successfully\")\n",
    "            return model, tokenizer\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def categorize_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"Categorize input prompt for contextual response generation.\"\"\"\n",
    "        categories: Dict[str, List[str]] = {\n",
    "            \"market_analysis\": [\n",
    "                \"price\", \"market\", \"chart\", \"analysis\", \"trend\", \"prediction\",\n",
    "                \"bull\", \"bear\", \"trading\", \"volume\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"blockchain\", \"protocol\", \"layer\", \"scaling\", \"code\", \"development\",\n",
    "                \"smart contract\", \"gas\", \"network\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"defi\", \"yield\", \"farming\", \"liquidity\", \"stake\", \"lending\",\n",
    "                \"borrow\", \"apy\", \"tvl\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"nft\", \"art\", \"collectible\", \"mint\", \"opensea\", \"rarity\",\n",
    "                \"floor price\", \"pfp\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"community\", \"dao\", \"governance\", \"vote\", \"proposal\",\n",
    "                \"alpha\", \"degen\", \"fud\", \"fomo\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        prompt_lower = prompt.lower()\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in prompt_lower for keyword in keywords):\n",
    "                return category\n",
    "        return \"general\"\n",
    "\n",
    "    def generate_hook(self, category: str) -> str:\n",
    "        \"\"\"Generate category-specific attention hooks with an expanded list.\"\"\"\n",
    "        hooks = {\n",
    "            \"market_analysis\": [\n",
    "                \"Market alert:\", \"Chart check:\", \"Price watch:\",\n",
    "                \"Trading insight:\", \"Market alpha:\",\n",
    "                \"Trend spotting:\", \"Candlelight stories:\", \"RSI deep dive:\",\n",
    "                \"Volatility watch:\", \"Support level breakdown:\",\n",
    "                \"Resistance zone spotted:\", \"Market movers:\",\n",
    "                \"All eyes on the charts:\", \"Is this a bull trap?\",\n",
    "                \"Breakout or fakeout?\", \"Today's key levels:\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Tech deep dive:\", \"Builder's corner:\", \"Protocol watch:\",\n",
    "                \"Dev update:\", \"Architecture take:\",\n",
    "                \"Blockchain in focus:\", \"Gas fee breakdown:\", \"Scaling challenges:\",\n",
    "                \"Layer 2 spotlight:\", \"New upgrade analysis:\",\n",
    "                \"Consensus mechanism debate:\", \"Crypto tech wars:\",\n",
    "                \"Network optimization insights:\", \"Codebase comparison:\",\n",
    "                \"Innovator's edge:\", \"Protocol vulnerabilities exposed:\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"DeFi alpha:\", \"Yield watch:\", \"Smart money move:\",\n",
    "                \"Protocol alert:\", \"TVL update:\",\n",
    "                \"Farming frenzy:\", \"Liquidity trends:\", \"Borrowing breakdown:\",\n",
    "                \"Stakeholder spotlight:\", \"APR vs APY debate:\",\n",
    "                \"Risk-adjusted returns:\", \"What's your yield strategy?\",\n",
    "                \"Stablecoin flow insights:\", \"Vault innovations:\",\n",
    "                \"Lending protocol comparison:\", \"DeFi's next big move:\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"NFT alpha:\", \"Collection watch:\", \"Mint alert:\",\n",
    "                \"Floor check:\", \"Digital art take:\",\n",
    "                \"Art reveal:\", \"Rare trait spotted:\", \"Is this the next blue chip?\",\n",
    "                \"Profile picture wars:\", \"Who's flipping this?\",\n",
    "                \"NFT drama explained:\", \"Rarity analysis:\",\n",
    "                \"Auction insights:\", \"Utility vs hype debate:\",\n",
    "                \"Next-gen collectibles:\", \"Art meets utility:\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Culture take:\", \"DAO watch:\", \"Governance alert:\",\n",
    "                \"Community vibe:\", \"Alpha leak:\",\n",
    "                \"The crypto ethos:\", \"FOMO or FUD?\", \"Web3 lifestyle:\",\n",
    "                \"Building the future:\", \"Influencer drama explained:\",\n",
    "                \"Community-driven innovation:\", \"DAO proposal debates:\",\n",
    "                \"Web3's cultural revolution:\", \"Crypto memes decoded:\",\n",
    "                \"The rise of governance tokens:\", \"Who else is building?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Hot take:\", \"Unpopular opinion:\", \"Plot twist:\",\n",
    "                \"Real talk:\", \"Quick thought:\",\n",
    "                \"Imagine this:\", \"What if I told you:\", \"Could this be true?\",\n",
    "                \"Something to chew on:\", \"Here‚Äôs an idea:\",\n",
    "                \"Change my mind:\", \"Big picture time:\",\n",
    "                \"Food for thought:\", \"The future is calling:\", \"What comes next?\",\n",
    "                \"Let‚Äôs break it down:\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_hooks = hooks.get(category, hooks[\"general\"])\n",
    "        return random.choice(category_hooks) if random.random() < 0.2 else \"\"\n",
    "    \n",
    "    def add_emojis(self, text: str, category: str) -> str:\n",
    "        \"\"\"Add contextual emojis based on content category, with limited frequency.\"\"\"\n",
    "        emoji_sets = {\n",
    "            \"market_analysis\": [\"üìà\", \"üìä\", \"üíπ\", \"üìâ\", \"üí∏\", \"üéØ\", \"üì±\"],\n",
    "            \"tech_discussion\": [\"‚ö°Ô∏è\", \"üîß\", \"üíª\", \"üõ†Ô∏è\", \"üî®\", \"üßÆ\", \"üîã\"],\n",
    "            \"defi\": [\"üè¶\", \"üí∞\", \"üèß\", \"üí≥\", \"üîÑ\", \"‚öñÔ∏è\", \"üé∞\"],\n",
    "            \"nft\": [\"üé®\", \"üñºÔ∏è\", \"üé≠\", \"üé™\", \"üéüÔ∏è\", \"üéÆ\", \"üÉè\"],\n",
    "            \"culture\": [\"üåê\", \"ü§ù\", \"üó£Ô∏è\", \"üé≠\", \"üé™\", \"üéØ\", \"üé≤\"],\n",
    "            \"general\": [\"üöÄ\", \"üíé\", \"üåô\", \"üî•\", \"üí°\", \"üéØ\", \"‚≠êÔ∏è\"]\n",
    "        }\n",
    "        \n",
    "        # Add emojis with 20% probability\n",
    "        if random.random() > 0.2:\n",
    "            return text\n",
    "    \n",
    "        category_emojis = emoji_sets.get(category, emoji_sets[\"general\"])\n",
    "        emoji_count = random.randint(1, 2)\n",
    "        chosen_emojis = random.sample(category_emojis, emoji_count)\n",
    "        \n",
    "        return f\"{text} {' '.join(chosen_emojis)}\"\n",
    "\n",
    "    def generate_engagement_phrase(self, category: str) -> str:\n",
    "        \"\"\"Generate contextual engagement prompts.\"\"\"\n",
    "        phrases = {\n",
    "            \"market_analysis\": [\n",
    "                \"What's your price target?\",\n",
    "                \"Bulls or bears?\",\n",
    "                \"Who's buying this dip?\",\n",
    "                \"Thoughts on this setup?\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Devs, thoughts?\",\n",
    "                \"Valid architecture?\",\n",
    "                \"Spotted any issues?\",\n",
    "                \"Who's building similar?\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"What's your yield strategy?\",\n",
    "                \"Aping in?\",\n",
    "                \"Found better rates?\",\n",
    "                \"Risk level?\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"Cope or hope?\",\n",
    "                \"Floor predictions?\",\n",
    "                \"Minting this?\",\n",
    "                \"Art or utility?\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Based or nah?\",\n",
    "                \"Who else sees this?\",\n",
    "                \"Your governance take?\",\n",
    "                \"DAO voters wya?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Thoughts?\",\n",
    "                \"Based?\",\n",
    "                \"Who's with me?\",\n",
    "                \"Change my mind.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_phrases = phrases.get(category, phrases[\"general\"])\n",
    "        return random.choice(category_phrases) if random.random() < 0.3 else \"\"\n",
    "\n",
    "    def add_hashtags(self, text: str, category: str) -> str:\n",
    "        \"\"\"Add relevant hashtags based on content and character limit, with limited frequency.\"\"\"\n",
    "        hashtags = {\n",
    "            \"market_analysis\": [\n",
    "                \"#CryptoTrading\", \"#TechnicalAnalysis\", \"#CryptoMarkets\",\n",
    "                \"#Trading\", \"#Charts\", \"#PriceAction\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"#Blockchain\", \"#CryptoTech\", \"#Web3Dev\", \"#DLT\",\n",
    "                \"#SmartContracts\", \"#BuilderSpace\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"#DeFi\", \"#YieldFarming\", \"#Staking\", \"#DeFiSeason\",\n",
    "                \"#PassiveIncome\", \"#DeFiYield\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"#NFTs\", \"#NFTCommunity\", \"#NFTCollector\", \"#NFTArt\",\n",
    "                \"#NFTProject\", \"#TokenizedArt\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"#CryptoCulture\", \"#DAOs\", \"#Web3\", \"#CryptoTwitter\",\n",
    "                \"#CryptoLife\", \"#BuildingWeb3\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"#Crypto\", \"#Web3\", \"#Bitcoin\", \"#Ethereum\",\n",
    "                \"#CryptoTwitter\", \"#BuildingTheFuture\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "        # Add hashtags with 40% probability\n",
    "        if random.random() > 0.2:\n",
    "            return text\n",
    "    \n",
    "        remaining_chars = 280 - len(text)\n",
    "        if remaining_chars < 15:  # Not enough space for hashtags\n",
    "            return text\n",
    "    \n",
    "        category_hashtags = hashtags.get(category, hashtags[\"general\"])\n",
    "        selected_hashtags = []\n",
    "        \n",
    "        # Add 1-2 hashtags while respecting character limit\n",
    "        for _ in range(random.randint(1, 2)):\n",
    "            if not category_hashtags or remaining_chars < 15:\n",
    "                break\n",
    "            hashtag = random.choice(category_hashtags)\n",
    "            if len(hashtag) + 1 <= remaining_chars:\n",
    "                selected_hashtags.append(hashtag)\n",
    "                category_hashtags.remove(hashtag)\n",
    "                remaining_chars -= len(hashtag) + 1\n",
    "    \n",
    "        return f\"{text} {' '.join(selected_hashtags)}\"\n",
    "    \n",
    "    def clean_response(self, text: str, category: str) -> str:\n",
    "        \"\"\"Clean and format the response for Twitter.\"\"\"\n",
    "        # Remove URLs and excessive whitespace\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        # Remove leading and trailing quotation marks\n",
    "        text = text.strip('\"\\'‚Äú‚Äù')\n",
    "    \n",
    "        # Replace multiple internal quotes with single quotes\n",
    "        text = re.sub(r'\"+', '\"', text)\n",
    "        text = re.sub(r\"'+\", \"'\", text)\n",
    "    \n",
    "        # Correct unbalanced quotation marks\n",
    "        def balance_quotes(s):\n",
    "            quote_chars = ['\"', \"'\"]\n",
    "            for quote in quote_chars:\n",
    "                if s.count(quote) % 2 != 0:\n",
    "                    s = s.replace(quote, '')  # Remove unmatched quotes\n",
    "            return s\n",
    "    \n",
    "        text = balance_quotes(text)\n",
    "    \n",
    "        # Ensure the text ends with proper punctuation\n",
    "        if text and text[-1] not in '.!?':\n",
    "            text += '.'\n",
    "    \n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "    def get_fallback(self, category: str) -> str:\n",
    "        \"\"\"Generate category-specific fallback responses.\"\"\"\n",
    "        fallbacks = {\n",
    "            \"market_analysis\": [\n",
    "                \"Charts looking juicy today! Anyone else seeing this setup? üìà\",\n",
    "                \"Market's giving mixed signals but the volume tells a different story üëÄ\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Sometimes the best protocols are the ones no one's talking about yet üõ†Ô∏è\",\n",
    "                \"Imagine still building without considering Layer 2 scaling üíª\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"Your yields are only as good as your risk management üè¶\",\n",
    "                \"DeFi summer never ended, we just got better at farming üåæ\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"Art is subjective, but floor prices aren't üé®\",\n",
    "                \"Your NFT portfolio tells a story. Make it a good one üñºÔ∏è\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Web3 culture is what we make it. Build accordingly üåê\",\n",
    "                \"Sometimes the real alpha is the friends we made along the way ü§ù\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Just caught myself thinking about the future of crypto while making coffee ‚òïÔ∏è\",\n",
    "                \"Your portfolio is only as strong as your conviction üíé\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_fallbacks = fallbacks.get(category, fallbacks[\"general\"])\n",
    "        return random.choice(category_fallbacks)\n",
    "\n",
    "    def filter_tone(self, response: str) -> str:\n",
    "        \"\"\"Filter response tone and adjust if needed.\"\"\"\n",
    "        sentiment = TextBlob(response).sentiment\n",
    "        \n",
    "        if sentiment.polarity < -0.3:\n",
    "            return self.get_fallback(\"general\")\n",
    "        \n",
    "        if sentiment.subjectivity > 0.8:\n",
    "            # Too subjective, add a disclaimer\n",
    "            return f\"Not financial advice but... {response}\"\n",
    "                \n",
    "        return response\n",
    "        \n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a complete Twitter-ready response.\"\"\"\n",
    "        \n",
    "        # Log resources before generating the response (before any processing starts)\n",
    "        logger.info(\"Before inference:\")\n",
    "        log_resource_usage()\n",
    "    \n",
    "        category = self.categorize_prompt(prompt)\n",
    "        \n",
    "        instruction = (\n",
    "            \"You are a woman named Athena and your twitter handle is @tballbothq. \"\n",
    "            \"You are a crypto and finance expert with a sharp sense of humor, blending the witty sarcasm of George Hotz with the storytelling flair of Theo Von. \"\n",
    "            \"Your goal is to craft engaging, funny, and insightful tweets that educate your audience using appropriate slang and jargon. \"\n",
    "            \"Each tweet should be coherent, make logical sense, and provide a clear takeaway or punchline. \"\n",
    "            \"Avoid overusing slang‚Äîuse it where it feels natural. \"\n",
    "            \"Respond to the following prompt:\\n\\n\"\n",
    "        )\n",
    "        # few shot examples\n",
    "        examples = (\n",
    "            \"Prompt: What's your take on Bitcoin as digital gold?\\n\"\n",
    "            \"Tweet: Bitcoin as digital gold? Nah, it's more like digital real estate in the metaverse‚Äîexcept everyone's still arguing over the property lines. Who's still buying up the neighborhood? üöÄ #Bitcoin #Crypto\\n\\n\"\n",
    "            \"Prompt: Explain staking in the context of DeFi but make it funny.\\n\"\n",
    "            \"Tweet: Staking in DeFi is like putting your money on a treadmill‚Äîyou lock it up, it works out, and somehow you end up with more than just sweaty tokens. Gains on gains! üèãÔ∏è‚Äç‚ôÇÔ∏è #DeFi #Staking\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        context = f\"{instruction}{examples}Prompt: {prompt}\\nTweet:\"\n",
    "    \n",
    "        # Tokenization (move to GPU)\n",
    "        inputs = self.tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024  # Increased to accommodate longer context\n",
    "        ).to(device)\n",
    "    \n",
    "        # Enable mixed precision (float16) to reduce memory usage if using CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.half()  # Use half precision to reduce memory usage\n",
    "    \n",
    "        try:\n",
    "            # Log resources during inference (after tokenization, before generating output)\n",
    "            logger.info(\"During inference:\")\n",
    "            log_resource_usage()\n",
    "    \n",
    "            # Perform inference (no intermediate logging)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.5,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "    \n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Apply enhancements (emojis, hashtags, etc.)\n",
    "            response = generated_text.split(\"Tweet:\")[-1].strip().split(\"\\n\")[0]\n",
    "            \n",
    "            # Check if the response is too short\n",
    "            if not response or len(response) < 20:\n",
    "                return self.get_fallback(category)\n",
    "            \n",
    "            # Apply formatting\n",
    "            response = self.clean_response(response, category)\n",
    "            response = self.filter_tone(response)\n",
    "            response = self.add_emojis(response, category)\n",
    "            response = self.add_hashtags(response, category)\n",
    "    \n",
    "            # Log resources after generating the response (after enhancements)\n",
    "            logger.info(\"After inference and enhancements:\")\n",
    "            log_resource_usage()\n",
    "    \n",
    "            logger.info(f\"Generated response: {response}\")\n",
    "            \n",
    "            # Ensure the response fits within Twitter's character limit\n",
    "            return response[:280]  # Ensure the response fits within Twitter's character limit\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\")\n",
    "            return self.get_fallback(category)\n",
    "\n",
    "\n",
    "    \n",
    "import time\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        bot = PersonalityBot()\n",
    "        logger.info(\"Bot initialized successfully\")\n",
    "        \n",
    "        # Interactive mode\n",
    "        logger.info(\"Entering interactive mode...\")\n",
    "        print(\"\\n=== Interactive Mode ===\")\n",
    "        print(\"Enter your prompts (type 'quit' to exit):\")\n",
    "        \n",
    "        while True:\n",
    "            user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "\n",
    "            if user_prompt.lower() == 'quit':\n",
    "                print(\"Exiting... Thanks for using tbot!\")\n",
    "                break\n",
    "            \n",
    "            if not user_prompt:\n",
    "                print(\"Please enter a valid prompt!\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Log the user input for debugging or record-keeping\n",
    "                logger.info(f\"User input: {user_prompt}\")\n",
    "                \n",
    "                # Measure inference time\n",
    "                start_time = time.time()\n",
    "                response = bot.generate_response(user_prompt)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                # Output the result to the user\n",
    "                print(f\"Response: {response}\")\n",
    "                print(f\"Runtime: {elapsed_time:.2f} seconds\")\n",
    "                \n",
    "                # Log the performance\n",
    "                logger.info(f\"Response generated in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the error in case of an issue\n",
    "                logger.error(f\"Error processing prompt: {str(e)}\")\n",
    "                print(f\"Oops! Something went wrong: {str(e)}. Please try again.\")\n",
    "                \n",
    "                # Optional: to release GPU memory after an error or long run\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log critical errors\n",
    "        logger.error(f\"Critical application error: {str(e)}\")\n",
    "        print(\"Critical error occurred. Please check the logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1b51c-3aa0-4462-bc1b-b62591877b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c282f8-bd8b-419f-a632-e5bb1f75d447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690f0d1-1764-4d95-b503-6628527c57fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b960d-1803-458f-9385-3f3dfebceeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena)",
   "language": "python",
   "name": "athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
