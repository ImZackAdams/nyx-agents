{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ab932-1345-483e-aa33-db3217f6eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load and save\n",
    "model_name = \"tiiuae/Falcon3-10B-Instruct-1.58bit\"\n",
    "save_directory = \"./falcon3_10b_instruct\"\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(f\"Saving to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Reloading from local directory...\")\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory, trust_remote_code=True)\n",
    "local_model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548a66c9-7c5b-46fb-831c-eb78243e653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading Falcon model in BF16 base from: tiiuae/Falcon3-10B-Instruct-1.58bit\n",
      "\n",
      "Generating with sampling (temperature=0.3, top_p=1.0)...\n",
      "\n",
      "Sampling Output:\n",
      "Sure, I'd be happy to help! Here are three healthy dinner ideas: 1) Grilled salmon with quinoa and steamed broccoli; 2) Stir-fried tofu with brown rice and snap peas; 3) Baked cod with roasted sweet potatoes and green beans. Please remember to adjust portion sizes and choose ingredients\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "def main():\n",
    "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # for better error traces\n",
    "\n",
    "    model_name = \"tiiuae/Falcon3-10B-Instruct-1.58bit\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(f\"Loading Falcon model in BF16 base from: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16  # base weights in BF16\n",
    "    )\n",
    "\n",
    "    # Remove built-in config if it exists\n",
    "    if hasattr(model.config, \"quantization_config\"):\n",
    "        del model.config.quantization_config\n",
    "\n",
    "    # Attach our BitsAndBytes config\n",
    "    model.config.quantization_config = bnb_config\n",
    "\n",
    "    prompt = \"\"\"You are Falcon, a helpful AI assistant.\n",
    "User: List 3 healthy dinner ideas\n",
    "Falcon:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Try sampling with very conservative parameters\n",
    "    print(\"\\nGenerating with sampling (temperature=0.3, top_p=1.0)...\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=64,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=1.0,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(\n",
    "            output_ids[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        print(f\"\\nSampling Output:\\n{response.strip()}\\n\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(\"RuntimeError during generation:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fb0aa-ae62-416a-89ee-d2d2c3e666c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be09427-f8a7-4b29-8182-a78aec830b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena)",
   "language": "python",
   "name": "athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
