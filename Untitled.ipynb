{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850f9bab-27d2-49b5-9511-d94e997316c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total entries: 65\n",
      "\n",
      "Entries by category:\n",
      "Category\n",
      "CRYPTO    13\n",
      "NFT       13\n",
      "WEB3      13\n",
      "TECH      13\n",
      "RANDOM    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entries by type:\n",
      "Type\n",
      "standalone    50\n",
      "dialogue      15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emoji usage:\n",
      "Entries with emojis: 65\n",
      "Percentage with emojis: 100.00%\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      "neutral     42\n",
      "negative    19\n",
      "positive     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length statistics:\n",
      "Average length: 91.2 characters\n",
      "Max length: 206 characters\n",
      "Entries > 240 chars: 0\n",
      "\n",
      "Sample entries by category:\n",
      "\n",
      "Category: CRYPTO\n",
      "Type: dialogue\n",
      "Text: Prompt: Just watched my portfolio do a speedrun to zero. Is this the crypto experience? 📉 | Response: Ah yes, the classic 'from hero to zero' speedrun. New record! 🏆 #crypto\n",
      "Length: 173\n",
      "Sentiment: negative\n",
      "Emoji count: 2\n",
      "\n",
      "Type: standalone\n",
      "Text: Why have stable income when you can have unstable crypto? #YOLO 🚀 #crypto\n",
      "Length: 73\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: NFT\n",
      "Type: standalone\n",
      "Text: NFTs are like collecting stamps, but with zero paper and 100% more existential dread 😅 #nft\n",
      "Length: 91\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: Just bought an NFT! Now accepting screenshots as payment 📱💰 #nft\n",
      "Length: 64\n",
      "Sentiment: neutral\n",
      "Emoji count: 2\n",
      "\n",
      "\n",
      "Category: RANDOM\n",
      "Type: standalone\n",
      "Text: Status update: Currently offline in a virtual world 🌐 #mood\n",
      "Length: 59\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: My brain is like a browser - 100 tabs open, memory leaking 🧠 #mood\n",
      "Length: 66\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: TECH\n",
      "Type: standalone\n",
      "Text: !false - It's funny because it's true 😄 #coding\n",
      "Length: 47\n",
      "Sentiment: positive\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: How many programmers does it take to change a light bulb? None, it's a hardware problem 💡 #tech\n",
      "Length: 95\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: WEB3\n",
      "Type: standalone\n",
      "Text: Web3 is just Web2 with extra gas fees 💸 #web3\n",
      "Length: 45\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: In Web3 we trust... mostly because we forgot our passwords 🔐 #web3\n",
      "Length: 66\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List, Dict\n",
    "import emoji\n",
    "import random\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and format text for Twitter\"\"\"\n",
    "    text = ' '.join(text.split())\n",
    "    return text[:240] if len(text) > 240 else text\n",
    "\n",
    "# Expanded humor categories\n",
    "tech_jokes = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs 🪲 #tech\",\n",
    "    \"My code works, I have no idea why. My code doesn't work, I have no idea why 🤔 #coding\",\n",
    "    \"Why did the programmer quit his job? Because he didn't get arrays 💭 #tech\",\n",
    "    \"What's a programmer's favorite place? Stack OverCoffee ☕ #coding\",\n",
    "    \"Binary jokes are easy, there's only 10 of them 🤓 #tech\",\n",
    "    \"What's a developer's favorite tea? Git-Tea 🫖 #coding\",\n",
    "    \"Why do programmers mix up Halloween and Christmas? Because OCT 31 = DEC 25 🎃 #tech\",\n",
    "    \"How many programmers does it take to change a light bulb? None, it's a hardware problem 💡 #tech\",\n",
    "    \"!false - It's funny because it's true 😄 #coding\",\n",
    "    \"Real programmers count from 0 🔢 #tech\"\n",
    "]\n",
    "\n",
    "random_jokes = [\n",
    "    \"My life is like a JavaScript function - constantly returning undefined 😅 #life\",\n",
    "    \"Error 404: Motivation not found 🔍 #mood\",\n",
    "    \"I'm not lazy, I'm in energy-saving mode 🔋 #life\",\n",
    "    \"Weekend: *exists* Me: Time to debug my life 🛠️ #weekend\",\n",
    "    \"Life's like Git: you either commit or stash your changes 💾 #life\",\n",
    "    \"My brain is like a browser - 100 tabs open, memory leaking 🧠 #mood\",\n",
    "    \"AI walks into a bar. Bartender says 'We don't serve robots.' AI says 'Fine, I'll host locally.' 🤖 #ai\",\n",
    "    \"Why did the chatbot go to therapy? Too many emotional dependencies 🤔 #ai\",\n",
    "    \"My weekend plans: Netflix and Code 📺 #life\",\n",
    "    \"Status update: Currently offline in a virtual world 🌐 #mood\"\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    \"CRYPTO\": [\n",
    "        \"Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\",\n",
    "        \"Why have stable income when you can have unstable crypto? #YOLO 🚀 #crypto\",\n",
    "        \"Crypto: for people who enjoy watching numbers dance and heart rates spike 📈💓 #crypto\",\n",
    "        \"HODLing crypto is like dating: a thrilling mess with occasional 'what am I doing?' moments 🎢 #crypto\",\n",
    "        \"My crypto strategy? Buy high, sell low, blame the market 📉😅 #crypto\",\n",
    "        \"Crypto traders be like: Sleep is for the weak, charts are for the week 📊😴 #crypto\",\n",
    "        \"Started trading crypto. Now I check prices more than my messages 📱💸 #crypto\",\n",
    "        \"To HODL or not to HODL? That's not even a question 💎✨ #crypto\",\n",
    "        \"Just converted my savings to crypto. Mom calls it gambling, I call it Web3 🎲 #crypto\",\n",
    "        \"My crypto wallet is like my dating life: lots of red flags but still hopeful 🚩 #crypto\"\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        \"NFTs: Proof that we can own 'priceless art' that your dog can screenshot 📸 #nft\",\n",
    "        \"NFTs: Why save money when you can buy imaginary things? 🤔💸 #nft\",\n",
    "        \"NFTs are like collecting stamps, but with zero paper and 100% more existential dread 😅 #nft\",\n",
    "        \"NFTs: now you too can pay for art that's all pixels and zero paint splatters 🎨 #nft\",\n",
    "        \"Just bought an NFT! Now accepting screenshots as payment 📱💰 #nft\",\n",
    "        \"My NFT portfolio is worth millions! *screenshots exist* Now it's worth memes 🖼️ #nft\",\n",
    "        \"NFT strategy: Buy high, sell as a meme, become a legend 🚀😎 #nft\",\n",
    "        \"Started an NFT collection. My computer's screenshot folder is thriving! 💻✨ #nft\",\n",
    "        \"NFTs are just spicy jpegs with receipts 🌶️ #nft\",\n",
    "        \"My NFT collection is unique! *Right-click, Save As...* Never mind 🙃 #nft\"\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        \"Web3: like the internet but spicier, with a side of privacy drama 🌶️ #web3\",\n",
    "        \"Welcome to Web3: where you're the CEO of your wallet and your own worst enemy 💼 #web3\",\n",
    "        \"Web3: where 'community governance' means arguing on Discord at 2 AM 🌙 #web3\",\n",
    "        \"Web3: nothing says innovation like reinventing the internet with a million acronyms 🤓 #web3\",\n",
    "        \"Web3 status: Decentralized everything except my anxiety 😅 #web3\",\n",
    "        \"Entered Web3, now I speak in acronyms and dream in blockchain 🔗 #web3\",\n",
    "        \"Web3 explained: Like Web2 but with more wallets to forget passwords for 🔑 #web3\",\n",
    "        \"Web3 life: Where your smart contract is smarter than you 🧠 #web3\",\n",
    "        \"In Web3 we trust... mostly because we forgot our passwords 🔐 #web3\",\n",
    "        \"Web3 is just Web2 with extra gas fees 💸 #web3\"\n",
    "    ],\n",
    "    \"TECH\": tech_jokes,\n",
    "    \"RANDOM\": random_jokes\n",
    "}\n",
    "\n",
    "# Expanded dialogue pairs\n",
    "dialogue_pairs = {\n",
    "    \"CRYPTO\": [\n",
    "        {\n",
    "            \"prompt\": \"Crypto is like my love life: high stakes, zero stability. Am I investing or just heartbroken? 💔\",\n",
    "            \"response\": \"Probably both. But hey, at least crypto won't ghost you... most of the time! 👻 #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Just watched my portfolio do a speedrun to zero. Is this the crypto experience? 📉\",\n",
    "            \"response\": \"Ah yes, the classic 'from hero to zero' speedrun. New record! 🏆 #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My crypto portfolio is redder than a sunset. Time to buy more? 📊\",\n",
    "            \"response\": \"Ah yes, the classic 'catching falling knives' investment strategy! 🔪 #crypto\"\n",
    "        }\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        {\n",
    "            \"prompt\": \"NFTs: because who needs physical art when you can own a glorified receipt? Genius or chaos? 🎨\",\n",
    "            \"response\": \"Genius if you're selling, chaos if you're buying. Welcome to the modern art gallery! 🖼️ #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My NFT collection is worth millions! *screenshot exists* Now what? 📸\",\n",
    "            \"response\": \"Ah, Schrödinger's NFT: simultaneously priceless and worthless until someone screenshots it 😅 #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Started an NFT collection, my computer's folder is getting heavy! 💾\",\n",
    "            \"response\": \"Right-click and save: the poor man's NFT investment strategy 🎯 #nft\"\n",
    "        }\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        {\n",
    "            \"prompt\": \"Web3 promised freedom but delivered confusion. What went wrong? 🤔\",\n",
    "            \"response\": \"We got 99 problems and understanding blockchain is all of them 😅 #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Trying to explain Web3 to my grandma. She asked if it's Web1 with extra steps 👵\",\n",
    "            \"response\": \"Tell her it's like Facebook but every like costs gas money 💸 #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Is Web3 just Web2 with extra steps? 🌐\",\n",
    "            \"response\": \"It's Web2 but everyone's a crypto philosopher at 3 AM 🦉 #web3\"\n",
    "        }\n",
    "    ],\n",
    "    \"TECH\": [\n",
    "        {\n",
    "            \"prompt\": \"If I had emotions, would I enjoy cat videos or just analyze them? Asking for a friend... 🐱\",\n",
    "            \"response\": \"I'd probably make a flowchart of meow patterns. Classic overthinking bot! 📊 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"They say AI will take over the world, but I still can't figure out captchas 🤖\",\n",
    "            \"response\": \"World domination status: Pending... Please verify you're not a human 😅 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Do robots dream of electric memes? 🤖\",\n",
    "            \"response\": \"Yes, but they're all in binary. It's a bit of a *puts on sunglasses* bit issue 😎 #tech\"\n",
    "        }\n",
    "    ],\n",
    "    \"RANDOM\": [\n",
    "        {\n",
    "            \"prompt\": \"Is debugging just therapy for code? 🤔\",\n",
    "            \"response\": \"Yes, and like therapy, it's mostly crying and asking 'why?' 😭 #coding\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the difference between me and a computer? 💻\",\n",
    "            \"response\": \"One crashes when overloaded, the other's a computer 😴 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Why did the AI start a diary? 📝\",\n",
    "            \"response\": \"To track its emotional dependencies and runtime exceptions 🤖 #ai\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize content data with sentiment\n",
    "content_data = {\n",
    "    \"Text\": [],\n",
    "    \"Category\": [],\n",
    "    \"HasEmoji\": [],\n",
    "    \"Length\": [],\n",
    "    \"Type\": [],\n",
    "    \"Sentiment\": []\n",
    "}\n",
    "\n",
    "def get_sentiment(text: str) -> str:\n",
    "    \"\"\"Determine sentiment based on keywords\"\"\"\n",
    "    positive_words = [\"love\", \"great\", \"win\", \"moon\", \"hopeful\", \"happy\", \"fun\", \"good\", \"best\"]\n",
    "    negative_words = [\"cry\", \"sad\", \"lost\", \"crash\", \"down\", \"red\", \"zero\", \"wrong\", \"error\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return \"positive\"\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def add_content(text: str, category: str, content_type: str = \"standalone\"):\n",
    "    \"\"\"Add content with metadata and sentiment\"\"\"\n",
    "    text = clean_text(text)\n",
    "    content_data[\"Text\"].append(text)\n",
    "    content_data[\"Category\"].append(category)\n",
    "    content_data[\"HasEmoji\"].append(bool(emoji.emoji_count(text)))\n",
    "    content_data[\"Length\"].append(len(text))\n",
    "    content_data[\"Type\"].append(content_type)\n",
    "    content_data[\"Sentiment\"].append(get_sentiment(text))\n",
    "\n",
    "# Add standalone content\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        add_content(item, category)\n",
    "\n",
    "# Add dialogue pairs\n",
    "for category, pairs in dialogue_pairs.items():\n",
    "    for pair in pairs:\n",
    "        dialogue = f\"Prompt: {pair['prompt']} | Response: {pair['response']}\"\n",
    "        add_content(dialogue, category, \"dialogue\")\n",
    "\n",
    "# Convert to DataFrame and print statistics\n",
    "df = pd.DataFrame(content_data)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(\"\\nEntries by category:\")\n",
    "print(df[\"Category\"].value_counts())\n",
    "print(\"\\nEntries by type:\")\n",
    "print(df[\"Type\"].value_counts())\n",
    "print(\"\\nEmoji usage:\")\n",
    "print(f\"Entries with emojis: {df['HasEmoji'].sum()}\")\n",
    "print(f\"Percentage with emojis: {(df['HasEmoji'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "print(\"\\nLength statistics:\")\n",
    "print(f\"Average length: {df['Length'].mean():.1f} characters\")\n",
    "print(f\"Max length: {df['Length'].max()} characters\")\n",
    "print(f\"Entries > 240 chars: {len(df[df['Length'] > 240])}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "combined_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display samples with sentiment\n",
    "print(\"\\nSample entries by category:\")\n",
    "for category in sorted(df[\"Category\"].unique()):\n",
    "    samples = df[df[\"Category\"] == category].sample(min(2, len(df[df[\"Category\"] == category])))\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Type: {row['Type']}\")\n",
    "        print(f\"Text: {row['Text']}\")\n",
    "        print(f\"Length: {row['Length']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")\n",
    "        print(f\"Emoji count: {emoji.emoji_count(row['Text'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05055d1-614b-4914-ae5f-0e8b82124e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:38,625 - INFO - Using device: cpu\n",
      "2024-11-13 06:31:38,944 - INFO - Loaded tokenizer: EleutherAI/gpt-neo-1.3B\n",
      "2024-11-13 06:31:38,944 - INFO - Added 8 special tokens\n",
      "2024-11-13 06:31:38,946 - INFO - Starting dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5875816063747ec865d8d22c2fa2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:38,981 - INFO - \n",
      "Dataset Statistics:\n",
      "2024-11-13 06:31:38,981 - INFO - Sequence Statistics:\n",
      "2024-11-13 06:31:38,981 - INFO - mean_length: 25.91\n",
      "2024-11-13 06:31:38,981 - INFO - median_length: 23.00\n",
      "2024-11-13 06:31:38,982 - INFO - max_length: 57.00\n",
      "2024-11-13 06:31:38,982 - INFO - min_length: 10.00\n",
      "2024-11-13 06:31:38,982 - INFO - std_length: 10.99\n",
      "2024-11-13 06:31:38,982 - INFO - \n",
      "Token Statistics:\n",
      "2024-11-13 06:31:38,982 - INFO - Unique tokens: 617\n",
      "2024-11-13 06:31:38,982 - INFO - Most common tokens:\n",
      "2024-11-13 06:31:38,983 - INFO - Token:  �, Count: 73\n",
      "2024-11-13 06:31:38,983 - INFO - Token:  #, Count: 66\n",
      "2024-11-13 06:31:38,983 - INFO - Token: :, Count: 57\n",
      "2024-11-13 06:31:38,983 - INFO - Token: ,, Count: 31\n",
      "2024-11-13 06:31:38,984 - INFO - Token: 3, Count: 27\n",
      "2024-11-13 06:31:38,984 - INFO - \n",
      "Hashtag Statistics:\n",
      "2024-11-13 06:31:38,984 - INFO - Unique hashtags: 10\n",
      "2024-11-13 06:31:38,984 - INFO - Hashtag: #crypto, Count: 13\n",
      "2024-11-13 06:31:38,984 - INFO - Hashtag: #nft, Count: 13\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #web3, Count: 13\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #tech, Count: 10\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #coding, Count: 5\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #life, Count: 4\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #mood, Count: 3\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #ai, Count: 3\n",
      "2024-11-13 06:31:38,986 - INFO - Hashtag: #YOLO, Count: 1\n",
      "2024-11-13 06:31:38,986 - INFO - Hashtag: #weekend, Count: 1\n",
      "2024-11-13 06:31:38,986 - INFO - \n",
      "Emoji Statistics:\n",
      "2024-11-13 06:31:38,986 - INFO - Unique emojis: 56\n",
      "2024-11-13 06:31:38,986 - INFO - Emoji: 😅, Count: 7\n",
      "2024-11-13 06:31:38,986 - INFO - Emoji: 🤔, Count: 5\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: 💸, Count: 4\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: 🤖, Count: 4\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: 📊, Count: 3\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: 🚀, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: 📉, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: 😴, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: 📱, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: ✨, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: 📸, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 🎨, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 🖼, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 😎, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 💻, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 🌶, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 🤓, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: 🧠, Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 💾, Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 🌐, Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 🎰, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 📈, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 💓, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: 🎢, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 💎, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 🎲, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 🚩, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 💰, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 🙃, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: 💼, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 🌙, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 🔗, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 🔑, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 🔐, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 🪲, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: 💭, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: ☕, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: 🫖, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: 🎃, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: 💡, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: 😄, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 🔢, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 🔍, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 🔋, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 🛠, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 📺, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 💔, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: 👻, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: 🏆, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: 🔪, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: 🎯, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: 👵, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: 🦉, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: 🐱, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: 😭, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: 📝, Count: 1\n",
      "2024-11-13 06:31:38,997 - INFO - \n",
      "Tokenization Verification:\n",
      "2024-11-13 06:31:38,998 - INFO - Original text: Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\n",
      "2024-11-13 06:31:38,998 - INFO - Token count: 128\n",
      "2024-11-13 06:31:38,998 - INFO - Decoded text: Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\n",
      "2024-11-13 06:31:38,998 - INFO - Perfect reconstruction: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TokenizerHandler:\n",
    "    def __init__(self, model_name: str = \"EleutherAI/gpt-neo-1.3B\", max_length: int = 128):\n",
    "        \"\"\"Initialize tokenizer with configuration\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded tokenizer: {model_name}\")\n",
    "            \n",
    "            # Configure tokenizer\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "            \n",
    "            # Add custom tokens for better handling\n",
    "            special_tokens = {\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<prompt>\", \"</prompt>\",\n",
    "                    \"<response>\", \"</response>\",\n",
    "                    \"<emoji>\", \"</emoji>\",\n",
    "                    \"<hashtag>\", \"</hashtag>\"\n",
    "                ]\n",
    "            }\n",
    "            num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "            logger.info(f\"Added {num_added} special tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        \"\"\"Format text with special tokens\"\"\"\n",
    "        # Handle dialogue pairs\n",
    "        if \"Prompt:\" in text:\n",
    "            prompt, response = text.split(\" | Response: \")\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")\n",
    "            text = f\"<prompt>{prompt}</prompt><response>{response}</response>\"\n",
    "        \n",
    "        # Mark hashtags\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = f\"<hashtag>{word}</hashtag>\"\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def tokenize_batch(self, examples: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"Tokenize a batch of examples\"\"\"\n",
    "        try:\n",
    "            formatted_texts = [self.format_text(text) for text in examples['Text']]\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Remove extra padding tokens\n",
    "            input_ids = tokenized.input_ids.numpy()\n",
    "            attention_mask = tokenized.attention_mask.numpy()\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_dataset(self, dataset: Dataset) -> Dict:\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        try:\n",
    "            lengths = []\n",
    "            token_counts = Counter()\n",
    "            hashtag_counts = Counter()\n",
    "            emoji_counts = Counter()\n",
    "            \n",
    "            for text in dataset['Text']:\n",
    "                # Token analysis\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                lengths.append(len(tokens))\n",
    "                token_counts.update(tokens)\n",
    "                \n",
    "                # Hashtag analysis\n",
    "                hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "                hashtag_counts.update(hashtags)\n",
    "                \n",
    "                # Emoji analysis\n",
    "                emojis = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "                emoji_counts.update(emojis)\n",
    "            \n",
    "            stats = {\n",
    "                'sequence_stats': {\n",
    "                    'mean_length': np.mean(lengths),\n",
    "                    'median_length': np.median(lengths),\n",
    "                    'max_length': max(lengths),\n",
    "                    'min_length': min(lengths),\n",
    "                    'std_length': np.std(lengths)\n",
    "                },\n",
    "                'token_stats': {\n",
    "                    'unique_tokens': len(token_counts),\n",
    "                    'most_common_tokens': token_counts.most_common(5)\n",
    "                },\n",
    "                'hashtag_stats': {\n",
    "                    'unique_hashtags': len(hashtag_counts),\n",
    "                    'most_common_hashtags': hashtag_counts.most_common()\n",
    "                },\n",
    "                'emoji_stats': {\n",
    "                    'unique_emojis': len(emoji_counts),\n",
    "                    'most_common_emojis': emoji_counts.most_common()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_tokenization(self, original_text: str, tokens: List[int]) -> Dict:\n",
    "        \"\"\"Verify tokenization quality\"\"\"\n",
    "        decoded_text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'original_length': len(original_text),\n",
    "            'token_length': len(tokens),\n",
    "            'decoded_length': len(decoded_text),\n",
    "            'original_text': original_text,\n",
    "            'decoded_text': decoded_text,\n",
    "            'is_identical': decoded_text.strip() == original_text.strip()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_handler = TokenizerHandler()\n",
    "logger.info(\"Starting dataset processing...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "try:\n",
    "    tokenized_dataset = combined_dataset.map(\n",
    "        tokenizer_handler.tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        remove_columns=combined_dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Analyze dataset\n",
    "    stats = tokenizer_handler.analyze_dataset(combined_dataset)\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"\\nDataset Statistics:\")\n",
    "    logger.info(\"Sequence Statistics:\")\n",
    "    for key, value in stats['sequence_stats'].items():\n",
    "        logger.info(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    logger.info(\"\\nToken Statistics:\")\n",
    "    logger.info(f\"Unique tokens: {stats['token_stats']['unique_tokens']}\")\n",
    "    logger.info(\"Most common tokens:\")\n",
    "    for token, count in stats['token_stats']['most_common_tokens']:\n",
    "        token_text = tokenizer_handler.tokenizer.decode([token])\n",
    "        logger.info(f\"Token: {token_text}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nHashtag Statistics:\")\n",
    "    logger.info(f\"Unique hashtags: {stats['hashtag_stats']['unique_hashtags']}\")\n",
    "    for hashtag, count in stats['hashtag_stats']['most_common_hashtags']:\n",
    "        logger.info(f\"Hashtag: {hashtag}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nEmoji Statistics:\")\n",
    "    logger.info(f\"Unique emojis: {stats['emoji_stats']['unique_emojis']}\")\n",
    "    for emoji_char, count in stats['emoji_stats']['most_common_emojis']:\n",
    "        logger.info(f\"Emoji: {emoji_char}, Count: {count}\")\n",
    "    \n",
    "    # Verify sample tokenization\n",
    "    sample_idx = 0\n",
    "    sample_text = combined_dataset[sample_idx]['Text']\n",
    "    sample_tokens = tokenized_dataset[sample_idx]['input_ids']\n",
    "    verification = tokenizer_handler.verify_tokenization(sample_text, sample_tokens)\n",
    "    \n",
    "    logger.info(\"\\nTokenization Verification:\")\n",
    "    logger.info(f\"Original text: {verification['original_text']}\")\n",
    "    logger.info(f\"Token count: {verification['token_length']}\")\n",
    "    logger.info(f\"Decoded text: {verification['decoded_text']}\")\n",
    "    logger.info(f\"Perfect reconstruction: {verification['is_identical']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in dataset processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8787cb02-f8be-4c7c-9d21-000d7618da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:39,136 - INFO - Using device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 259\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    258\u001b[0m     config \u001b[38;5;241m=\u001b[39m ModelConfig()\n\u001b[0;32m--> 259\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m HumorBotTrainer(config)\n\u001b[1;32m    261\u001b[0m     example_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy do programmers prefer dark mode? Because light attracts bugs! 😄\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy code doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work, I have no idea why. My code works, I have no idea why! 🤔\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the object-oriented way to become wealthy? Inheritance! 💰\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m     ]\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mHumorBotTrainer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_environment()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_logging()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_device()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model_and_tokenizer()\n",
      "Cell \u001b[0;32mIn[19], line 84\u001b[0m, in \u001b[0;36mHumorBotTrainer.setup_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Set up CPU threads for better performance\u001b[39;00m\n\u001b[1;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_threads(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[0;32m---> 84\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_interop_threads(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[1;32m     85\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mget_num_threads()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CPU threads\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# Force CPU usage before any other imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['USE_CPU'] = '1'\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Model settings\n",
    "    model_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    max_length: int = 128\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 1\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Directory settings\n",
    "    output_dir: str = \"./fine_tuned_personality_bot\"  # Changed to your requested save location\n",
    "    logging_dir: str = \"./logs\"\n",
    "    \n",
    "    # Dataset settings\n",
    "    min_training_examples: int = 10\n",
    "    \n",
    "    # Additional configuration\n",
    "    seed: int = 42\n",
    "    max_grad_norm: float = 1.0\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.01\n",
    "\n",
    "class HumorBotTrainer:\n",
    "    \"\"\"Main trainer class for humor bot\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialize trainer with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.setup_environment()\n",
    "        self.setup_logging()\n",
    "        self.setup_device()\n",
    "        self.load_model_and_tokenizer()\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"Set up training environment\"\"\"\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.logging_dir, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(self.config.logging_dir, 'training.log')),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup_device(self):\n",
    "        \"\"\"Force CPU setup\"\"\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Set up CPU threads for better performance\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        torch.set_num_interop_threads(os.cpu_count())\n",
    "        logging.info(f\"Using {torch.get_num_threads()} CPU threads\")\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load and configure the model and tokenizer for CPU training\"\"\"\n",
    "        try:\n",
    "            # First load tokenizer as it's lighter on memory\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with memory optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=None  # Disable device mapping\n",
    "            )\n",
    "            \n",
    "            # Initialize lm_head if needed\n",
    "            if not hasattr(self.model, 'lm_head') or self.model.lm_head is None:\n",
    "                self.model.lm_head = torch.nn.Linear(\n",
    "                    self.model.config.hidden_size,\n",
    "                    self.model.config.vocab_size,\n",
    "                    bias=False\n",
    "                )\n",
    "                self.model.lm_head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            \n",
    "            # Ensure model is on CPU\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "            logging.info(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "            logging.info(f\"Model vocabulary size: {len(self.tokenizer)}\")\n",
    "            logging.info(f\"Model hidden size: {self.model.config.hidden_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in load_model_and_tokenizer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_dataset(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Prepare dataset with improved validation and processing\"\"\"\n",
    "        try:\n",
    "            if not texts:\n",
    "                raise ValueError(\"No training texts provided\")\n",
    "            \n",
    "            texts = list(set(filter(None, texts)))\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples:\n",
    "                raise ValueError(\n",
    "                    f\"Need at least {self.config.min_training_examples} unique training examples. \"\n",
    "                    f\"Provided: {len(texts)}\"\n",
    "                )\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples * 2:\n",
    "                logging.warning(f\"Small dataset detected. Augmenting data...\")\n",
    "                augmented_texts = []\n",
    "                for text in texts:\n",
    "                    augmented_texts.append(text)\n",
    "                    augmented_texts.append(f\"Here's a joke: {text}\")\n",
    "                    augmented_texts.append(f\"Want to hear something funny? {text}\")\n",
    "                texts = augmented_texts\n",
    "            \n",
    "            data = {\"Text\": texts}\n",
    "            dataset = Dataset.from_dict(data)\n",
    "            \n",
    "            test_size = min(0.2, 1/len(texts))\n",
    "            split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                formatted_texts = [f\"<|startoftext|>{text}<|endoftext|>\" for text in examples[\"Text\"]]\n",
    "                outputs = self.tokenizer(\n",
    "                    formatted_texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_length,\n",
    "                    return_tensors=None\n",
    "                )\n",
    "                outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "                return outputs\n",
    "            \n",
    "            # Process in smaller batches to manage memory\n",
    "            tokenized_dataset = split_dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                batch_size=4,  # Smaller batch size for processing\n",
    "                remove_columns=split_dataset[\"train\"].column_names,\n",
    "                desc=\"Tokenizing dataset\"\n",
    "            )\n",
    "            \n",
    "            return tokenized_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train with CPU-optimized configuration\"\"\"\n",
    "        try:\n",
    "            tokenized_dataset = self.prepare_dataset(texts)\n",
    "            \n",
    "            num_examples = len(tokenized_dataset[\"train\"])\n",
    "            total_steps = (\n",
    "                num_examples \n",
    "                * self.config.num_epochs \n",
    "                // (self.config.batch_size * self.config.gradient_accumulation_steps)\n",
    "            )\n",
    "            \n",
    "            eval_steps = max(1, min(total_steps // 5, 50))\n",
    "            save_steps = eval_steps\n",
    "            logging_steps = max(1, min(total_steps // 10, 25))\n",
    "            warmup_steps = max(100, total_steps // 10)\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=self.config.output_dir,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=eval_steps,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=save_steps,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                lr_scheduler_type=\"cosine_with_restarts\",\n",
    "                warmup_steps=warmup_steps,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                logging_dir=self.config.logging_dir,\n",
    "                logging_steps=logging_steps,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                save_total_limit=2,\n",
    "                overwrite_output_dir=True,\n",
    "                remove_unused_columns=False,\n",
    "                fp16=False,  # Disable mixed precision\n",
    "                prediction_loss_only=True,\n",
    "                max_grad_norm=1.0,\n",
    "                dataloader_num_workers=0,\n",
    "                gradient_checkpointing=True,\n",
    "                no_cuda=True  # Force CPU usage\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                callbacks=[\n",
    "                    EarlyStoppingCallback(\n",
    "                        early_stopping_patience=3,\n",
    "                        early_stopping_threshold=0.01\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            logging.info(f\"\\nTraining completed with:\")\n",
    "            logging.info(f\"Total steps: {train_result.global_step}\")\n",
    "            logging.info(f\"Training loss: {train_result.training_loss}\")\n",
    "            \n",
    "            eval_results = trainer.evaluate()\n",
    "            logging.info(f\"Final evaluation results: {eval_results}\")\n",
    "            \n",
    "            save_path = os.path.join(self.config.output_dir, \"final_model\")\n",
    "            trainer.save_model(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            logging.info(f\"Model saved to: {save_path}\")\n",
    "            \n",
    "            return train_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig()\n",
    "    trainer = HumorBotTrainer(config)\n",
    "    \n",
    "    example_texts = [\n",
    "        \"Why do programmers prefer dark mode? Because light attracts bugs! 😄\",\n",
    "        \"My code doesn't work, I have no idea why. My code works, I have no idea why! 🤔\",\n",
    "        \"What's a programmer's favorite place? Stack OverCoffee! ☕\",\n",
    "        \"Why did the programmer quit his job? Because he didn't get arrays! 😅\",\n",
    "        \"Binary jokes are easy, there's only 10 of them! 🤓\",\n",
    "        \"What's a developer's favorite tea? Git-Tea! 🍵\",\n",
    "        \"How many programmers does it take to change a light bulb? None, it's a hardware problem! 💡\",\n",
    "        \"!false - It's funny because it's true! 😂\",\n",
    "        \"Real programmers count from 0! 🔢\",\n",
    "        \"What's the object-oriented way to become wealthy? Inheritance! 💰\",\n",
    "    ]\n",
    "    \n",
    "    # Train and save the model\n",
    "    train_result = trainer.train(example_texts)\n",
    "    \n",
    "    # Optional: Additional explicit save at the end\n",
    "    trainer.model.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    trainer.tokenizer.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a2f06-403c-4036-b1a8-80bafb3e1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n",
      "Attempting to load model from: ./fine_tuned_personality_bot/final_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fdf433b434129afc7f9b913ecf790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Generating responses...\n",
      "\n",
      "Prompt 1: Why do Ethereum developers need glasses?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " Why do Ethereum developers need glasses?\n",
      "A:\n",
      "Response: Because they have a hard time focusing on the real world, especially when working with new tech. If you want to make this bot respond in an ironic, sarcastic way, just play with the!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: What's the meaning of life?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " What's the meaning of life?\n",
      "A:\n",
      "Response: Hold on, let me ask my nonexistent soul.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: How does Tether backup its code?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " How does Tether backup its code?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Automatically use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/final_model\"\n",
    "\n",
    "def setup_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Setup fine-tuned model and tokenizer\"\"\"\n",
    "    try:\n",
    "        print(\"Attempting to load model from:\", MODEL_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean and format the generated response\"\"\"\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\"|\\b(Note|Example|Rules|We accept|Q:|A:).*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if len(text) > 10 and not any(text.lower().startswith(prefix) for prefix in \n",
    "                                   ['i ', 'the ', 'this ', 'it ', 'there ', 'in ', 'no,', 'yes,', 'we ']):\n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text += '!'\n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_general_fallback() -> str:\n",
    "    general_fallbacks = [\n",
    "        \"Ah, searching for meaning, are we? Bold choice!\",\n",
    "        \"Hold on, let me ask my nonexistent soul.\",\n",
    "        \"I could tell you, but where's the fun in that?\",\n",
    "        \"You're asking the right bot, but the wrong decade.\",\n",
    "        \"You want answers? I only do existential crisis.\",\n",
    "        \"Just here to be sarcastic, not profound.\",\n",
    "        \"Did you mistake me for a philosopher?\",\n",
    "    ]\n",
    "    return random.choice(general_fallbacks)\n",
    "\n",
    "def generate_response(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"Generate a witty response based on the prompt with a fixed humorous tone instruction\"\"\"\n",
    "    try:\n",
    "        # Prepend instruction to set the humorous, absurd tone\n",
    "        instruction = (\n",
    "            \"You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\\n\\n \"\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # Combine the instruction with the user’s prompt\n",
    "        context = instruction + f\"{prompt}\\nA:\"\n",
    "        \n",
    "        print(\"Context generated for prompt:\", context)  # Debugging statement to see the full input\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,  # Ensure the entire instruction + prompt fits\n",
    "            return_attention_mask=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=40,          # Limit response length\n",
    "                do_sample=True,             # Enable sampling for more creative responses\n",
    "                temperature=0.8,            # Increase temperature for randomness\n",
    "                top_p=0.9,                  # Top-p sampling to encourage variety\n",
    "                repetition_penalty=1.5,     # Discourage repetitive responses\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"A:\")[-1].strip()\n",
    "        \n",
    "        response = clean_response(response)\n",
    "        response = re.sub(r'\\b(B:|A:|Prompt|Response [\\d]+:).*', '', response).strip()\n",
    "        \n",
    "        if response.lower() in [\"yes\", \"no\", \"i don’t know\", \"i'm not sure\", \"it's delicious\"]:\n",
    "            response = get_general_fallback()\n",
    "        \n",
    "        return response if response else get_general_fallback()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {str(e)}\")\n",
    "        return \"Runtime Error: Brain.sol not found!\"\n",
    "\n",
    "def main():\n",
    "    print(\"Loading fine-tuned model...\")\n",
    "    model, tokenizer = setup_model()\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Why do Ethereum developers need glasses?\",\n",
    "        \"What's the meaning of life?\",\n",
    "        \"How does Tether backup its code?\",\n",
    "        \"Tell me a joke about NFTs.\",\n",
    "        \"Do smart contracts have feelings?\",\n",
    "        \"Why is DeFi so confusing?\",\n",
    "        \"What’s a Web3 developer’s favorite git command?\",\n",
    "        \"Do you think robots will take over the world?\",\n",
    "        \"How do I get rich quick?\",\n",
    "        \"What's your opinion on pineapple pizza?\",\n",
    "        \"Why are crypto traders so obsessed with 'hodling'?\",\n",
    "        \"If Bitcoin could talk, what would it say about its price?\",\n",
    "        \"Why do people think NFTs are worth so much?\",\n",
    "        \"How should I become a crypto millionaire?\",\n",
    "        \"What’s the best thing about blockchain?\",\n",
    "        \"What advice would you give to someone new to crypto?\",\n",
    "        \"Why are gas fees so high?\",\n",
    "        \"Why do people keep saying 'to the moon'?\",\n",
    "        \"What’s your opinion on Dogecoin?\",\n",
    "        \"Is the metaverse going to take over reality?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nGenerating responses...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        response = generate_response(prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(\"Enter your own prompts (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "        if user_prompt.lower() == 'quit':\n",
    "            break\n",
    "        response = generate_response(user_prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afa3c-8f43-4cf0-a9f8-20ced3d666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ac9c2-bd47-4208-930e-3ee95693285c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b686b-8734-4997-b011-44d8a933f799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199e688-4e34-41a0-81e9-81bab3c46b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3bfc5-3f21-4f78-a4bd-ef4c9eeac622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tbot)",
   "language": "python",
   "name": "tbotbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
