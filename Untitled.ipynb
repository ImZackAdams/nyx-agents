{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850f9bab-27d2-49b5-9511-d94e997316c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total entries: 65\n",
      "\n",
      "Entries by category:\n",
      "Category\n",
      "CRYPTO    13\n",
      "NFT       13\n",
      "WEB3      13\n",
      "TECH      13\n",
      "RANDOM    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entries by type:\n",
      "Type\n",
      "standalone    50\n",
      "dialogue      15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emoji usage:\n",
      "Entries with emojis: 65\n",
      "Percentage with emojis: 100.00%\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      "neutral     42\n",
      "negative    19\n",
      "positive     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length statistics:\n",
      "Average length: 91.2 characters\n",
      "Max length: 206 characters\n",
      "Entries > 240 chars: 0\n",
      "\n",
      "Sample entries by category:\n",
      "\n",
      "Category: CRYPTO\n",
      "Type: dialogue\n",
      "Text: Prompt: Just watched my portfolio do a speedrun to zero. Is this the crypto experience? ðŸ“‰ | Response: Ah yes, the classic 'from hero to zero' speedrun. New record! ðŸ† #crypto\n",
      "Length: 173\n",
      "Sentiment: negative\n",
      "Emoji count: 2\n",
      "\n",
      "Type: standalone\n",
      "Text: Why have stable income when you can have unstable crypto? #YOLO ðŸš€ #crypto\n",
      "Length: 73\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: NFT\n",
      "Type: standalone\n",
      "Text: NFTs are like collecting stamps, but with zero paper and 100% more existential dread ðŸ˜… #nft\n",
      "Length: 91\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: Just bought an NFT! Now accepting screenshots as payment ðŸ“±ðŸ’° #nft\n",
      "Length: 64\n",
      "Sentiment: neutral\n",
      "Emoji count: 2\n",
      "\n",
      "\n",
      "Category: RANDOM\n",
      "Type: standalone\n",
      "Text: Status update: Currently offline in a virtual world ðŸŒ #mood\n",
      "Length: 59\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: My brain is like a browser - 100 tabs open, memory leaking ðŸ§  #mood\n",
      "Length: 66\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: TECH\n",
      "Type: standalone\n",
      "Text: !false - It's funny because it's true ðŸ˜„ #coding\n",
      "Length: 47\n",
      "Sentiment: positive\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: How many programmers does it take to change a light bulb? None, it's a hardware problem ðŸ’¡ #tech\n",
      "Length: 95\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: WEB3\n",
      "Type: standalone\n",
      "Text: Web3 is just Web2 with extra gas fees ðŸ’¸ #web3\n",
      "Length: 45\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: In Web3 we trust... mostly because we forgot our passwords ðŸ” #web3\n",
      "Length: 66\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List, Dict\n",
    "import emoji\n",
    "import random\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and format text for Twitter\"\"\"\n",
    "    text = ' '.join(text.split())\n",
    "    return text[:240] if len(text) > 240 else text\n",
    "\n",
    "# Expanded humor categories\n",
    "tech_jokes = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs ðŸª² #tech\",\n",
    "    \"My code works, I have no idea why. My code doesn't work, I have no idea why ðŸ¤” #coding\",\n",
    "    \"Why did the programmer quit his job? Because he didn't get arrays ðŸ’­ #tech\",\n",
    "    \"What's a programmer's favorite place? Stack OverCoffee â˜• #coding\",\n",
    "    \"Binary jokes are easy, there's only 10 of them ðŸ¤“ #tech\",\n",
    "    \"What's a developer's favorite tea? Git-Tea ðŸ«– #coding\",\n",
    "    \"Why do programmers mix up Halloween and Christmas? Because OCT 31 = DEC 25 ðŸŽƒ #tech\",\n",
    "    \"How many programmers does it take to change a light bulb? None, it's a hardware problem ðŸ’¡ #tech\",\n",
    "    \"!false - It's funny because it's true ðŸ˜„ #coding\",\n",
    "    \"Real programmers count from 0 ðŸ”¢ #tech\"\n",
    "]\n",
    "\n",
    "random_jokes = [\n",
    "    \"My life is like a JavaScript function - constantly returning undefined ðŸ˜… #life\",\n",
    "    \"Error 404: Motivation not found ðŸ” #mood\",\n",
    "    \"I'm not lazy, I'm in energy-saving mode ðŸ”‹ #life\",\n",
    "    \"Weekend: *exists* Me: Time to debug my life ðŸ› ï¸ #weekend\",\n",
    "    \"Life's like Git: you either commit or stash your changes ðŸ’¾ #life\",\n",
    "    \"My brain is like a browser - 100 tabs open, memory leaking ðŸ§  #mood\",\n",
    "    \"AI walks into a bar. Bartender says 'We don't serve robots.' AI says 'Fine, I'll host locally.' ðŸ¤– #ai\",\n",
    "    \"Why did the chatbot go to therapy? Too many emotional dependencies ðŸ¤” #ai\",\n",
    "    \"My weekend plans: Netflix and Code ðŸ“º #life\",\n",
    "    \"Status update: Currently offline in a virtual world ðŸŒ #mood\"\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    \"CRYPTO\": [\n",
    "        \"Crypto: The digital casino where everyone's all-in, but no one knows the rules ðŸŽ° #crypto\",\n",
    "        \"Why have stable income when you can have unstable crypto? #YOLO ðŸš€ #crypto\",\n",
    "        \"Crypto: for people who enjoy watching numbers dance and heart rates spike ðŸ“ˆðŸ’“ #crypto\",\n",
    "        \"HODLing crypto is like dating: a thrilling mess with occasional 'what am I doing?' moments ðŸŽ¢ #crypto\",\n",
    "        \"My crypto strategy? Buy high, sell low, blame the market ðŸ“‰ðŸ˜… #crypto\",\n",
    "        \"Crypto traders be like: Sleep is for the weak, charts are for the week ðŸ“ŠðŸ˜´ #crypto\",\n",
    "        \"Started trading crypto. Now I check prices more than my messages ðŸ“±ðŸ’¸ #crypto\",\n",
    "        \"To HODL or not to HODL? That's not even a question ðŸ’Žâœ¨ #crypto\",\n",
    "        \"Just converted my savings to crypto. Mom calls it gambling, I call it Web3 ðŸŽ² #crypto\",\n",
    "        \"My crypto wallet is like my dating life: lots of red flags but still hopeful ðŸš© #crypto\"\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        \"NFTs: Proof that we can own 'priceless art' that your dog can screenshot ðŸ“¸ #nft\",\n",
    "        \"NFTs: Why save money when you can buy imaginary things? ðŸ¤”ðŸ’¸ #nft\",\n",
    "        \"NFTs are like collecting stamps, but with zero paper and 100% more existential dread ðŸ˜… #nft\",\n",
    "        \"NFTs: now you too can pay for art that's all pixels and zero paint splatters ðŸŽ¨ #nft\",\n",
    "        \"Just bought an NFT! Now accepting screenshots as payment ðŸ“±ðŸ’° #nft\",\n",
    "        \"My NFT portfolio is worth millions! *screenshots exist* Now it's worth memes ðŸ–¼ï¸ #nft\",\n",
    "        \"NFT strategy: Buy high, sell as a meme, become a legend ðŸš€ðŸ˜Ž #nft\",\n",
    "        \"Started an NFT collection. My computer's screenshot folder is thriving! ðŸ’»âœ¨ #nft\",\n",
    "        \"NFTs are just spicy jpegs with receipts ðŸŒ¶ï¸ #nft\",\n",
    "        \"My NFT collection is unique! *Right-click, Save As...* Never mind ðŸ™ƒ #nft\"\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        \"Web3: like the internet but spicier, with a side of privacy drama ðŸŒ¶ï¸ #web3\",\n",
    "        \"Welcome to Web3: where you're the CEO of your wallet and your own worst enemy ðŸ’¼ #web3\",\n",
    "        \"Web3: where 'community governance' means arguing on Discord at 2 AM ðŸŒ™ #web3\",\n",
    "        \"Web3: nothing says innovation like reinventing the internet with a million acronyms ðŸ¤“ #web3\",\n",
    "        \"Web3 status: Decentralized everything except my anxiety ðŸ˜… #web3\",\n",
    "        \"Entered Web3, now I speak in acronyms and dream in blockchain ðŸ”— #web3\",\n",
    "        \"Web3 explained: Like Web2 but with more wallets to forget passwords for ðŸ”‘ #web3\",\n",
    "        \"Web3 life: Where your smart contract is smarter than you ðŸ§  #web3\",\n",
    "        \"In Web3 we trust... mostly because we forgot our passwords ðŸ” #web3\",\n",
    "        \"Web3 is just Web2 with extra gas fees ðŸ’¸ #web3\"\n",
    "    ],\n",
    "    \"TECH\": tech_jokes,\n",
    "    \"RANDOM\": random_jokes\n",
    "}\n",
    "\n",
    "# Expanded dialogue pairs\n",
    "dialogue_pairs = {\n",
    "    \"CRYPTO\": [\n",
    "        {\n",
    "            \"prompt\": \"Crypto is like my love life: high stakes, zero stability. Am I investing or just heartbroken? ðŸ’”\",\n",
    "            \"response\": \"Probably both. But hey, at least crypto won't ghost you... most of the time! ðŸ‘» #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Just watched my portfolio do a speedrun to zero. Is this the crypto experience? ðŸ“‰\",\n",
    "            \"response\": \"Ah yes, the classic 'from hero to zero' speedrun. New record! ðŸ† #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My crypto portfolio is redder than a sunset. Time to buy more? ðŸ“Š\",\n",
    "            \"response\": \"Ah yes, the classic 'catching falling knives' investment strategy! ðŸ”ª #crypto\"\n",
    "        }\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        {\n",
    "            \"prompt\": \"NFTs: because who needs physical art when you can own a glorified receipt? Genius or chaos? ðŸŽ¨\",\n",
    "            \"response\": \"Genius if you're selling, chaos if you're buying. Welcome to the modern art gallery! ðŸ–¼ï¸ #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My NFT collection is worth millions! *screenshot exists* Now what? ðŸ“¸\",\n",
    "            \"response\": \"Ah, SchrÃ¶dinger's NFT: simultaneously priceless and worthless until someone screenshots it ðŸ˜… #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Started an NFT collection, my computer's folder is getting heavy! ðŸ’¾\",\n",
    "            \"response\": \"Right-click and save: the poor man's NFT investment strategy ðŸŽ¯ #nft\"\n",
    "        }\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        {\n",
    "            \"prompt\": \"Web3 promised freedom but delivered confusion. What went wrong? ðŸ¤”\",\n",
    "            \"response\": \"We got 99 problems and understanding blockchain is all of them ðŸ˜… #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Trying to explain Web3 to my grandma. She asked if it's Web1 with extra steps ðŸ‘µ\",\n",
    "            \"response\": \"Tell her it's like Facebook but every like costs gas money ðŸ’¸ #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Is Web3 just Web2 with extra steps? ðŸŒ\",\n",
    "            \"response\": \"It's Web2 but everyone's a crypto philosopher at 3 AM ðŸ¦‰ #web3\"\n",
    "        }\n",
    "    ],\n",
    "    \"TECH\": [\n",
    "        {\n",
    "            \"prompt\": \"If I had emotions, would I enjoy cat videos or just analyze them? Asking for a friend... ðŸ±\",\n",
    "            \"response\": \"I'd probably make a flowchart of meow patterns. Classic overthinking bot! ðŸ“Š #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"They say AI will take over the world, but I still can't figure out captchas ðŸ¤–\",\n",
    "            \"response\": \"World domination status: Pending... Please verify you're not a human ðŸ˜… #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Do robots dream of electric memes? ðŸ¤–\",\n",
    "            \"response\": \"Yes, but they're all in binary. It's a bit of a *puts on sunglasses* bit issue ðŸ˜Ž #tech\"\n",
    "        }\n",
    "    ],\n",
    "    \"RANDOM\": [\n",
    "        {\n",
    "            \"prompt\": \"Is debugging just therapy for code? ðŸ¤”\",\n",
    "            \"response\": \"Yes, and like therapy, it's mostly crying and asking 'why?' ðŸ˜­ #coding\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the difference between me and a computer? ðŸ’»\",\n",
    "            \"response\": \"One crashes when overloaded, the other's a computer ðŸ˜´ #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Why did the AI start a diary? ðŸ“\",\n",
    "            \"response\": \"To track its emotional dependencies and runtime exceptions ðŸ¤– #ai\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize content data with sentiment\n",
    "content_data = {\n",
    "    \"Text\": [],\n",
    "    \"Category\": [],\n",
    "    \"HasEmoji\": [],\n",
    "    \"Length\": [],\n",
    "    \"Type\": [],\n",
    "    \"Sentiment\": []\n",
    "}\n",
    "\n",
    "def get_sentiment(text: str) -> str:\n",
    "    \"\"\"Determine sentiment based on keywords\"\"\"\n",
    "    positive_words = [\"love\", \"great\", \"win\", \"moon\", \"hopeful\", \"happy\", \"fun\", \"good\", \"best\"]\n",
    "    negative_words = [\"cry\", \"sad\", \"lost\", \"crash\", \"down\", \"red\", \"zero\", \"wrong\", \"error\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return \"positive\"\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def add_content(text: str, category: str, content_type: str = \"standalone\"):\n",
    "    \"\"\"Add content with metadata and sentiment\"\"\"\n",
    "    text = clean_text(text)\n",
    "    content_data[\"Text\"].append(text)\n",
    "    content_data[\"Category\"].append(category)\n",
    "    content_data[\"HasEmoji\"].append(bool(emoji.emoji_count(text)))\n",
    "    content_data[\"Length\"].append(len(text))\n",
    "    content_data[\"Type\"].append(content_type)\n",
    "    content_data[\"Sentiment\"].append(get_sentiment(text))\n",
    "\n",
    "# Add standalone content\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        add_content(item, category)\n",
    "\n",
    "# Add dialogue pairs\n",
    "for category, pairs in dialogue_pairs.items():\n",
    "    for pair in pairs:\n",
    "        dialogue = f\"Prompt: {pair['prompt']} | Response: {pair['response']}\"\n",
    "        add_content(dialogue, category, \"dialogue\")\n",
    "\n",
    "# Convert to DataFrame and print statistics\n",
    "df = pd.DataFrame(content_data)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(\"\\nEntries by category:\")\n",
    "print(df[\"Category\"].value_counts())\n",
    "print(\"\\nEntries by type:\")\n",
    "print(df[\"Type\"].value_counts())\n",
    "print(\"\\nEmoji usage:\")\n",
    "print(f\"Entries with emojis: {df['HasEmoji'].sum()}\")\n",
    "print(f\"Percentage with emojis: {(df['HasEmoji'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "print(\"\\nLength statistics:\")\n",
    "print(f\"Average length: {df['Length'].mean():.1f} characters\")\n",
    "print(f\"Max length: {df['Length'].max()} characters\")\n",
    "print(f\"Entries > 240 chars: {len(df[df['Length'] > 240])}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "combined_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display samples with sentiment\n",
    "print(\"\\nSample entries by category:\")\n",
    "for category in sorted(df[\"Category\"].unique()):\n",
    "    samples = df[df[\"Category\"] == category].sample(min(2, len(df[df[\"Category\"] == category])))\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Type: {row['Type']}\")\n",
    "        print(f\"Text: {row['Text']}\")\n",
    "        print(f\"Length: {row['Length']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")\n",
    "        print(f\"Emoji count: {emoji.emoji_count(row['Text'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05055d1-614b-4914-ae5f-0e8b82124e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:38,625 - INFO - Using device: cpu\n",
      "2024-11-13 06:31:38,944 - INFO - Loaded tokenizer: EleutherAI/gpt-neo-1.3B\n",
      "2024-11-13 06:31:38,944 - INFO - Added 8 special tokens\n",
      "2024-11-13 06:31:38,946 - INFO - Starting dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5875816063747ec865d8d22c2fa2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:38,981 - INFO - \n",
      "Dataset Statistics:\n",
      "2024-11-13 06:31:38,981 - INFO - Sequence Statistics:\n",
      "2024-11-13 06:31:38,981 - INFO - mean_length: 25.91\n",
      "2024-11-13 06:31:38,981 - INFO - median_length: 23.00\n",
      "2024-11-13 06:31:38,982 - INFO - max_length: 57.00\n",
      "2024-11-13 06:31:38,982 - INFO - min_length: 10.00\n",
      "2024-11-13 06:31:38,982 - INFO - std_length: 10.99\n",
      "2024-11-13 06:31:38,982 - INFO - \n",
      "Token Statistics:\n",
      "2024-11-13 06:31:38,982 - INFO - Unique tokens: 617\n",
      "2024-11-13 06:31:38,982 - INFO - Most common tokens:\n",
      "2024-11-13 06:31:38,983 - INFO - Token:  ï¿½, Count: 73\n",
      "2024-11-13 06:31:38,983 - INFO - Token:  #, Count: 66\n",
      "2024-11-13 06:31:38,983 - INFO - Token: :, Count: 57\n",
      "2024-11-13 06:31:38,983 - INFO - Token: ,, Count: 31\n",
      "2024-11-13 06:31:38,984 - INFO - Token: 3, Count: 27\n",
      "2024-11-13 06:31:38,984 - INFO - \n",
      "Hashtag Statistics:\n",
      "2024-11-13 06:31:38,984 - INFO - Unique hashtags: 10\n",
      "2024-11-13 06:31:38,984 - INFO - Hashtag: #crypto, Count: 13\n",
      "2024-11-13 06:31:38,984 - INFO - Hashtag: #nft, Count: 13\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #web3, Count: 13\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #tech, Count: 10\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #coding, Count: 5\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #life, Count: 4\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #mood, Count: 3\n",
      "2024-11-13 06:31:38,985 - INFO - Hashtag: #ai, Count: 3\n",
      "2024-11-13 06:31:38,986 - INFO - Hashtag: #YOLO, Count: 1\n",
      "2024-11-13 06:31:38,986 - INFO - Hashtag: #weekend, Count: 1\n",
      "2024-11-13 06:31:38,986 - INFO - \n",
      "Emoji Statistics:\n",
      "2024-11-13 06:31:38,986 - INFO - Unique emojis: 56\n",
      "2024-11-13 06:31:38,986 - INFO - Emoji: ðŸ˜…, Count: 7\n",
      "2024-11-13 06:31:38,986 - INFO - Emoji: ðŸ¤”, Count: 5\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: ðŸ’¸, Count: 4\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: ðŸ¤–, Count: 4\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: ðŸ“Š, Count: 3\n",
      "2024-11-13 06:31:38,987 - INFO - Emoji: ðŸš€, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: ðŸ“‰, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: ðŸ˜´, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: ðŸ“±, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: âœ¨, Count: 2\n",
      "2024-11-13 06:31:38,988 - INFO - Emoji: ðŸ“¸, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸŽ¨, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸ–¼, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸ˜Ž, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸ’», Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸŒ¶, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸ¤“, Count: 2\n",
      "2024-11-13 06:31:38,989 - INFO - Emoji: ðŸ§ , Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸ’¾, Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸŒ, Count: 2\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸŽ°, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸ“ˆ, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸ’“, Count: 1\n",
      "2024-11-13 06:31:38,990 - INFO - Emoji: ðŸŽ¢, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸ’Ž, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸŽ², Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸš©, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸ’°, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸ™ƒ, Count: 1\n",
      "2024-11-13 06:31:38,991 - INFO - Emoji: ðŸ’¼, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸŒ™, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸ”—, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸ”‘, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸ”, Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸª², Count: 1\n",
      "2024-11-13 06:31:38,992 - INFO - Emoji: ðŸ’­, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: â˜•, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: ðŸ«–, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: ðŸŽƒ, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: ðŸ’¡, Count: 1\n",
      "2024-11-13 06:31:38,993 - INFO - Emoji: ðŸ˜„, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ”¢, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ”, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ”‹, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ› , Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ“º, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ’”, Count: 1\n",
      "2024-11-13 06:31:38,994 - INFO - Emoji: ðŸ‘», Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: ðŸ†, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: ðŸ”ª, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: ðŸŽ¯, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: ðŸ‘µ, Count: 1\n",
      "2024-11-13 06:31:38,995 - INFO - Emoji: ðŸ¦‰, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: ðŸ±, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: ðŸ˜­, Count: 1\n",
      "2024-11-13 06:31:38,996 - INFO - Emoji: ðŸ“, Count: 1\n",
      "2024-11-13 06:31:38,997 - INFO - \n",
      "Tokenization Verification:\n",
      "2024-11-13 06:31:38,998 - INFO - Original text: Crypto: The digital casino where everyone's all-in, but no one knows the rules ðŸŽ° #crypto\n",
      "2024-11-13 06:31:38,998 - INFO - Token count: 128\n",
      "2024-11-13 06:31:38,998 - INFO - Decoded text: Crypto: The digital casino where everyone's all-in, but no one knows the rules ðŸŽ° #crypto\n",
      "2024-11-13 06:31:38,998 - INFO - Perfect reconstruction: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TokenizerHandler:\n",
    "    def __init__(self, model_name: str = \"EleutherAI/gpt-neo-1.3B\", max_length: int = 128):\n",
    "        \"\"\"Initialize tokenizer with configuration\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded tokenizer: {model_name}\")\n",
    "            \n",
    "            # Configure tokenizer\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "            \n",
    "            # Add custom tokens for better handling\n",
    "            special_tokens = {\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<prompt>\", \"</prompt>\",\n",
    "                    \"<response>\", \"</response>\",\n",
    "                    \"<emoji>\", \"</emoji>\",\n",
    "                    \"<hashtag>\", \"</hashtag>\"\n",
    "                ]\n",
    "            }\n",
    "            num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "            logger.info(f\"Added {num_added} special tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        \"\"\"Format text with special tokens\"\"\"\n",
    "        # Handle dialogue pairs\n",
    "        if \"Prompt:\" in text:\n",
    "            prompt, response = text.split(\" | Response: \")\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")\n",
    "            text = f\"<prompt>{prompt}</prompt><response>{response}</response>\"\n",
    "        \n",
    "        # Mark hashtags\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = f\"<hashtag>{word}</hashtag>\"\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def tokenize_batch(self, examples: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"Tokenize a batch of examples\"\"\"\n",
    "        try:\n",
    "            formatted_texts = [self.format_text(text) for text in examples['Text']]\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Remove extra padding tokens\n",
    "            input_ids = tokenized.input_ids.numpy()\n",
    "            attention_mask = tokenized.attention_mask.numpy()\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_dataset(self, dataset: Dataset) -> Dict:\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        try:\n",
    "            lengths = []\n",
    "            token_counts = Counter()\n",
    "            hashtag_counts = Counter()\n",
    "            emoji_counts = Counter()\n",
    "            \n",
    "            for text in dataset['Text']:\n",
    "                # Token analysis\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                lengths.append(len(tokens))\n",
    "                token_counts.update(tokens)\n",
    "                \n",
    "                # Hashtag analysis\n",
    "                hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "                hashtag_counts.update(hashtags)\n",
    "                \n",
    "                # Emoji analysis\n",
    "                emojis = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "                emoji_counts.update(emojis)\n",
    "            \n",
    "            stats = {\n",
    "                'sequence_stats': {\n",
    "                    'mean_length': np.mean(lengths),\n",
    "                    'median_length': np.median(lengths),\n",
    "                    'max_length': max(lengths),\n",
    "                    'min_length': min(lengths),\n",
    "                    'std_length': np.std(lengths)\n",
    "                },\n",
    "                'token_stats': {\n",
    "                    'unique_tokens': len(token_counts),\n",
    "                    'most_common_tokens': token_counts.most_common(5)\n",
    "                },\n",
    "                'hashtag_stats': {\n",
    "                    'unique_hashtags': len(hashtag_counts),\n",
    "                    'most_common_hashtags': hashtag_counts.most_common()\n",
    "                },\n",
    "                'emoji_stats': {\n",
    "                    'unique_emojis': len(emoji_counts),\n",
    "                    'most_common_emojis': emoji_counts.most_common()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_tokenization(self, original_text: str, tokens: List[int]) -> Dict:\n",
    "        \"\"\"Verify tokenization quality\"\"\"\n",
    "        decoded_text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'original_length': len(original_text),\n",
    "            'token_length': len(tokens),\n",
    "            'decoded_length': len(decoded_text),\n",
    "            'original_text': original_text,\n",
    "            'decoded_text': decoded_text,\n",
    "            'is_identical': decoded_text.strip() == original_text.strip()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_handler = TokenizerHandler()\n",
    "logger.info(\"Starting dataset processing...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "try:\n",
    "    tokenized_dataset = combined_dataset.map(\n",
    "        tokenizer_handler.tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        remove_columns=combined_dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Analyze dataset\n",
    "    stats = tokenizer_handler.analyze_dataset(combined_dataset)\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"\\nDataset Statistics:\")\n",
    "    logger.info(\"Sequence Statistics:\")\n",
    "    for key, value in stats['sequence_stats'].items():\n",
    "        logger.info(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    logger.info(\"\\nToken Statistics:\")\n",
    "    logger.info(f\"Unique tokens: {stats['token_stats']['unique_tokens']}\")\n",
    "    logger.info(\"Most common tokens:\")\n",
    "    for token, count in stats['token_stats']['most_common_tokens']:\n",
    "        token_text = tokenizer_handler.tokenizer.decode([token])\n",
    "        logger.info(f\"Token: {token_text}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nHashtag Statistics:\")\n",
    "    logger.info(f\"Unique hashtags: {stats['hashtag_stats']['unique_hashtags']}\")\n",
    "    for hashtag, count in stats['hashtag_stats']['most_common_hashtags']:\n",
    "        logger.info(f\"Hashtag: {hashtag}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nEmoji Statistics:\")\n",
    "    logger.info(f\"Unique emojis: {stats['emoji_stats']['unique_emojis']}\")\n",
    "    for emoji_char, count in stats['emoji_stats']['most_common_emojis']:\n",
    "        logger.info(f\"Emoji: {emoji_char}, Count: {count}\")\n",
    "    \n",
    "    # Verify sample tokenization\n",
    "    sample_idx = 0\n",
    "    sample_text = combined_dataset[sample_idx]['Text']\n",
    "    sample_tokens = tokenized_dataset[sample_idx]['input_ids']\n",
    "    verification = tokenizer_handler.verify_tokenization(sample_text, sample_tokens)\n",
    "    \n",
    "    logger.info(\"\\nTokenization Verification:\")\n",
    "    logger.info(f\"Original text: {verification['original_text']}\")\n",
    "    logger.info(f\"Token count: {verification['token_length']}\")\n",
    "    logger.info(f\"Decoded text: {verification['decoded_text']}\")\n",
    "    logger.info(f\"Perfect reconstruction: {verification['is_identical']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in dataset processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8787cb02-f8be-4c7c-9d21-000d7618da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 06:31:39,136 - INFO - Using device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 259\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    258\u001b[0m     config \u001b[38;5;241m=\u001b[39m ModelConfig()\n\u001b[0;32m--> 259\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m HumorBotTrainer(config)\n\u001b[1;32m    261\u001b[0m     example_texts \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy do programmers prefer dark mode? Because light attracts bugs! ðŸ˜„\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy code doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work, I have no idea why. My code works, I have no idea why! ðŸ¤”\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the object-oriented way to become wealthy? Inheritance! ðŸ’°\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m     ]\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# Train and save the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mHumorBotTrainer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_environment()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_logging()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_device()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model_and_tokenizer()\n",
      "Cell \u001b[0;32mIn[19], line 84\u001b[0m, in \u001b[0;36mHumorBotTrainer.setup_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Set up CPU threads for better performance\u001b[39;00m\n\u001b[1;32m     83\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_threads(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[0;32m---> 84\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_num_interop_threads(os\u001b[38;5;241m.\u001b[39mcpu_count())\n\u001b[1;32m     85\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mget_num_threads()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CPU threads\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# Force CPU usage before any other imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['USE_CPU'] = '1'\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Model settings\n",
    "    model_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    max_length: int = 128\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 1\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Directory settings\n",
    "    output_dir: str = \"./fine_tuned_personality_bot\"  # Changed to your requested save location\n",
    "    logging_dir: str = \"./logs\"\n",
    "    \n",
    "    # Dataset settings\n",
    "    min_training_examples: int = 10\n",
    "    \n",
    "    # Additional configuration\n",
    "    seed: int = 42\n",
    "    max_grad_norm: float = 1.0\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.01\n",
    "\n",
    "class HumorBotTrainer:\n",
    "    \"\"\"Main trainer class for humor bot\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialize trainer with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.setup_environment()\n",
    "        self.setup_logging()\n",
    "        self.setup_device()\n",
    "        self.load_model_and_tokenizer()\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"Set up training environment\"\"\"\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.logging_dir, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(self.config.logging_dir, 'training.log')),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup_device(self):\n",
    "        \"\"\"Force CPU setup\"\"\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Set up CPU threads for better performance\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        torch.set_num_interop_threads(os.cpu_count())\n",
    "        logging.info(f\"Using {torch.get_num_threads()} CPU threads\")\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load and configure the model and tokenizer for CPU training\"\"\"\n",
    "        try:\n",
    "            # First load tokenizer as it's lighter on memory\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with memory optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=None  # Disable device mapping\n",
    "            )\n",
    "            \n",
    "            # Initialize lm_head if needed\n",
    "            if not hasattr(self.model, 'lm_head') or self.model.lm_head is None:\n",
    "                self.model.lm_head = torch.nn.Linear(\n",
    "                    self.model.config.hidden_size,\n",
    "                    self.model.config.vocab_size,\n",
    "                    bias=False\n",
    "                )\n",
    "                self.model.lm_head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            \n",
    "            # Ensure model is on CPU\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "            logging.info(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "            logging.info(f\"Model vocabulary size: {len(self.tokenizer)}\")\n",
    "            logging.info(f\"Model hidden size: {self.model.config.hidden_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in load_model_and_tokenizer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_dataset(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Prepare dataset with improved validation and processing\"\"\"\n",
    "        try:\n",
    "            if not texts:\n",
    "                raise ValueError(\"No training texts provided\")\n",
    "            \n",
    "            texts = list(set(filter(None, texts)))\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples:\n",
    "                raise ValueError(\n",
    "                    f\"Need at least {self.config.min_training_examples} unique training examples. \"\n",
    "                    f\"Provided: {len(texts)}\"\n",
    "                )\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples * 2:\n",
    "                logging.warning(f\"Small dataset detected. Augmenting data...\")\n",
    "                augmented_texts = []\n",
    "                for text in texts:\n",
    "                    augmented_texts.append(text)\n",
    "                    augmented_texts.append(f\"Here's a joke: {text}\")\n",
    "                    augmented_texts.append(f\"Want to hear something funny? {text}\")\n",
    "                texts = augmented_texts\n",
    "            \n",
    "            data = {\"Text\": texts}\n",
    "            dataset = Dataset.from_dict(data)\n",
    "            \n",
    "            test_size = min(0.2, 1/len(texts))\n",
    "            split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                formatted_texts = [f\"<|startoftext|>{text}<|endoftext|>\" for text in examples[\"Text\"]]\n",
    "                outputs = self.tokenizer(\n",
    "                    formatted_texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_length,\n",
    "                    return_tensors=None\n",
    "                )\n",
    "                outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "                return outputs\n",
    "            \n",
    "            # Process in smaller batches to manage memory\n",
    "            tokenized_dataset = split_dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                batch_size=4,  # Smaller batch size for processing\n",
    "                remove_columns=split_dataset[\"train\"].column_names,\n",
    "                desc=\"Tokenizing dataset\"\n",
    "            )\n",
    "            \n",
    "            return tokenized_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train with CPU-optimized configuration\"\"\"\n",
    "        try:\n",
    "            tokenized_dataset = self.prepare_dataset(texts)\n",
    "            \n",
    "            num_examples = len(tokenized_dataset[\"train\"])\n",
    "            total_steps = (\n",
    "                num_examples \n",
    "                * self.config.num_epochs \n",
    "                // (self.config.batch_size * self.config.gradient_accumulation_steps)\n",
    "            )\n",
    "            \n",
    "            eval_steps = max(1, min(total_steps // 5, 50))\n",
    "            save_steps = eval_steps\n",
    "            logging_steps = max(1, min(total_steps // 10, 25))\n",
    "            warmup_steps = max(100, total_steps // 10)\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=self.config.output_dir,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=eval_steps,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=save_steps,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                lr_scheduler_type=\"cosine_with_restarts\",\n",
    "                warmup_steps=warmup_steps,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                logging_dir=self.config.logging_dir,\n",
    "                logging_steps=logging_steps,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                save_total_limit=2,\n",
    "                overwrite_output_dir=True,\n",
    "                remove_unused_columns=False,\n",
    "                fp16=False,  # Disable mixed precision\n",
    "                prediction_loss_only=True,\n",
    "                max_grad_norm=1.0,\n",
    "                dataloader_num_workers=0,\n",
    "                gradient_checkpointing=True,\n",
    "                no_cuda=True  # Force CPU usage\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                callbacks=[\n",
    "                    EarlyStoppingCallback(\n",
    "                        early_stopping_patience=3,\n",
    "                        early_stopping_threshold=0.01\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            logging.info(f\"\\nTraining completed with:\")\n",
    "            logging.info(f\"Total steps: {train_result.global_step}\")\n",
    "            logging.info(f\"Training loss: {train_result.training_loss}\")\n",
    "            \n",
    "            eval_results = trainer.evaluate()\n",
    "            logging.info(f\"Final evaluation results: {eval_results}\")\n",
    "            \n",
    "            save_path = os.path.join(self.config.output_dir, \"final_model\")\n",
    "            trainer.save_model(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            logging.info(f\"Model saved to: {save_path}\")\n",
    "            \n",
    "            return train_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig()\n",
    "    trainer = HumorBotTrainer(config)\n",
    "    \n",
    "    example_texts = [\n",
    "        \"Why do programmers prefer dark mode? Because light attracts bugs! ðŸ˜„\",\n",
    "        \"My code doesn't work, I have no idea why. My code works, I have no idea why! ðŸ¤”\",\n",
    "        \"What's a programmer's favorite place? Stack OverCoffee! â˜•\",\n",
    "        \"Why did the programmer quit his job? Because he didn't get arrays! ðŸ˜…\",\n",
    "        \"Binary jokes are easy, there's only 10 of them! ðŸ¤“\",\n",
    "        \"What's a developer's favorite tea? Git-Tea! ðŸµ\",\n",
    "        \"How many programmers does it take to change a light bulb? None, it's a hardware problem! ðŸ’¡\",\n",
    "        \"!false - It's funny because it's true! ðŸ˜‚\",\n",
    "        \"Real programmers count from 0! ðŸ”¢\",\n",
    "        \"What's the object-oriented way to become wealthy? Inheritance! ðŸ’°\",\n",
    "    ]\n",
    "    \n",
    "    # Train and save the model\n",
    "    train_result = trainer.train(example_texts)\n",
    "    \n",
    "    # Optional: Additional explicit save at the end\n",
    "    trainer.model.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    trainer.tokenizer.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a2f06-403c-4036-b1a8-80bafb3e1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model...\n",
      "Attempting to load model from: ./fine_tuned_personality_bot/final_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836fdf433b434129afc7f9b913ecf790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Generating responses...\n",
      "\n",
      "Prompt 1: Why do Ethereum developers need glasses?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " Why do Ethereum developers need glasses?\n",
      "A:\n",
      "Response: Because they have a hard time focusing on the real world, especially when working with new tech. If you want to make this bot respond in an ironic, sarcastic way, just play with the!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 2: What's the meaning of life?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " What's the meaning of life?\n",
      "A:\n",
      "Response: Hold on, let me ask my nonexistent soul.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Prompt 3: How does Tether backup its code?\n",
      "Context generated for prompt: You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\n",
      "\n",
      " How does Tether backup its code?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Automatically use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/final_model\"\n",
    "\n",
    "def setup_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Setup fine-tuned model and tokenizer\"\"\"\n",
    "    try:\n",
    "        print(\"Attempting to load model from:\", MODEL_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        ).to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean and format the generated response\"\"\"\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\"|\\b(Note|Example|Rules|We accept|Q:|A:).*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if len(text) > 10 and not any(text.lower().startswith(prefix) for prefix in \n",
    "                                   ['i ', 'the ', 'this ', 'it ', 'there ', 'in ', 'no,', 'yes,', 'we ']):\n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text += '!'\n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_general_fallback() -> str:\n",
    "    general_fallbacks = [\n",
    "        \"Ah, searching for meaning, are we? Bold choice!\",\n",
    "        \"Hold on, let me ask my nonexistent soul.\",\n",
    "        \"I could tell you, but where's the fun in that?\",\n",
    "        \"You're asking the right bot, but the wrong decade.\",\n",
    "        \"You want answers? I only do existential crisis.\",\n",
    "        \"Just here to be sarcastic, not profound.\",\n",
    "        \"Did you mistake me for a philosopher?\",\n",
    "    ]\n",
    "    return random.choice(general_fallbacks)\n",
    "\n",
    "def generate_response(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"Generate a witty response based on the prompt with a fixed humorous tone instruction\"\"\"\n",
    "    try:\n",
    "        # Prepend instruction to set the humorous, absurd tone\n",
    "        instruction = (\n",
    "            \"You are a sarcastic, witty bot that always responds in a funny, clever, and often absurd way. Avoid straightforward answers and instead give responses that are humorous, playful, and absurd.\\n\\n \"\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # Combine the instruction with the userâ€™s prompt\n",
    "        context = instruction + f\"{prompt}\\nA:\"\n",
    "        \n",
    "        print(\"Context generated for prompt:\", context)  # Debugging statement to see the full input\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,  # Ensure the entire instruction + prompt fits\n",
    "            return_attention_mask=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=40,          # Limit response length\n",
    "                do_sample=True,             # Enable sampling for more creative responses\n",
    "                temperature=0.8,            # Increase temperature for randomness\n",
    "                top_p=0.9,                  # Top-p sampling to encourage variety\n",
    "                repetition_penalty=1.5,     # Discourage repetitive responses\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"A:\")[-1].strip()\n",
    "        \n",
    "        response = clean_response(response)\n",
    "        response = re.sub(r'\\b(B:|A:|Prompt|Response [\\d]+:).*', '', response).strip()\n",
    "        \n",
    "        if response.lower() in [\"yes\", \"no\", \"i donâ€™t know\", \"i'm not sure\", \"it's delicious\"]:\n",
    "            response = get_general_fallback()\n",
    "        \n",
    "        return response if response else get_general_fallback()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {str(e)}\")\n",
    "        return \"Runtime Error: Brain.sol not found!\"\n",
    "\n",
    "def main():\n",
    "    print(\"Loading fine-tuned model...\")\n",
    "    model, tokenizer = setup_model()\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Why do Ethereum developers need glasses?\",\n",
    "        \"What's the meaning of life?\",\n",
    "        \"How does Tether backup its code?\",\n",
    "        \"Tell me a joke about NFTs.\",\n",
    "        \"Do smart contracts have feelings?\",\n",
    "        \"Why is DeFi so confusing?\",\n",
    "        \"Whatâ€™s a Web3 developerâ€™s favorite git command?\",\n",
    "        \"Do you think robots will take over the world?\",\n",
    "        \"How do I get rich quick?\",\n",
    "        \"What's your opinion on pineapple pizza?\",\n",
    "        \"Why are crypto traders so obsessed with 'hodling'?\",\n",
    "        \"If Bitcoin could talk, what would it say about its price?\",\n",
    "        \"Why do people think NFTs are worth so much?\",\n",
    "        \"How should I become a crypto millionaire?\",\n",
    "        \"Whatâ€™s the best thing about blockchain?\",\n",
    "        \"What advice would you give to someone new to crypto?\",\n",
    "        \"Why are gas fees so high?\",\n",
    "        \"Why do people keep saying 'to the moon'?\",\n",
    "        \"Whatâ€™s your opinion on Dogecoin?\",\n",
    "        \"Is the metaverse going to take over reality?\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nGenerating responses...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        response = generate_response(prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\\n\")\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    print(\"Enter your own prompts (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "        if user_prompt.lower() == 'quit':\n",
    "            break\n",
    "        response = generate_response(user_prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afa3c-8f43-4cf0-a9f8-20ced3d666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ac9c2-bd47-4208-930e-3ee95693285c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b686b-8734-4997-b011-44d8a933f799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199e688-4e34-41a0-81e9-81bab3c46b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3bfc5-3f21-4f78-a4bd-ef4c9eeac622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tbot)",
   "language": "python",
   "name": "tbotbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
