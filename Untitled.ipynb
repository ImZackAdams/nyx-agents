{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850f9bab-27d2-49b5-9511-d94e997316c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total entries: 65\n",
      "\n",
      "Entries by category:\n",
      "Category\n",
      "CRYPTO    13\n",
      "NFT       13\n",
      "WEB3      13\n",
      "TECH      13\n",
      "RANDOM    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entries by type:\n",
      "Type\n",
      "standalone    50\n",
      "dialogue      15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emoji usage:\n",
      "Entries with emojis: 65\n",
      "Percentage with emojis: 100.00%\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      "neutral     42\n",
      "negative    19\n",
      "positive     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length statistics:\n",
      "Average length: 91.2 characters\n",
      "Max length: 206 characters\n",
      "Entries > 240 chars: 0\n",
      "\n",
      "Sample entries by category:\n",
      "\n",
      "Category: CRYPTO\n",
      "Type: standalone\n",
      "Text: My crypto strategy? Buy high, sell low, blame the market 📉😅 #crypto\n",
      "Length: 67\n",
      "Sentiment: negative\n",
      "Emoji count: 2\n",
      "\n",
      "Type: standalone\n",
      "Text: Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\n",
      "Length: 88\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: NFT\n",
      "Type: standalone\n",
      "Text: NFTs are like collecting stamps, but with zero paper and 100% more existential dread 😅 #nft\n",
      "Length: 91\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: NFTs: Why save money when you can buy imaginary things? 🤔💸 #nft\n",
      "Length: 63\n",
      "Sentiment: neutral\n",
      "Emoji count: 2\n",
      "\n",
      "\n",
      "Category: RANDOM\n",
      "Type: dialogue\n",
      "Text: Prompt: Is debugging just therapy for code? 🤔 | Response: Yes, and like therapy, it's mostly crying and asking 'why?' 😭 #coding\n",
      "Length: 127\n",
      "Sentiment: negative\n",
      "Emoji count: 2\n",
      "\n",
      "Type: standalone\n",
      "Text: Status update: Currently offline in a virtual world 🌐 #mood\n",
      "Length: 59\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: TECH\n",
      "Type: standalone\n",
      "Text: !false - It's funny because it's true 😄 #coding\n",
      "Length: 47\n",
      "Sentiment: positive\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: Why do programmers prefer dark mode? Because light attracts bugs 🪲 #tech\n",
      "Length: 72\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: WEB3\n",
      "Type: standalone\n",
      "Text: Web3: where 'community governance' means arguing on Discord at 2 AM 🌙 #web3\n",
      "Length: 75\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: Web3: like the internet but spicier, with a side of privacy drama 🌶️ #web3\n",
      "Length: 74\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import List, Dict\n",
    "import emoji\n",
    "import random\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and format text for Twitter\"\"\"\n",
    "    text = ' '.join(text.split())\n",
    "    return text[:240] if len(text) > 240 else text\n",
    "\n",
    "# Expanded humor categories\n",
    "tech_jokes = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs 🪲 #tech\",\n",
    "    \"My code works, I have no idea why. My code doesn't work, I have no idea why 🤔 #coding\",\n",
    "    \"Why did the programmer quit his job? Because he didn't get arrays 💭 #tech\",\n",
    "    \"What's a programmer's favorite place? Stack OverCoffee ☕ #coding\",\n",
    "    \"Binary jokes are easy, there's only 10 of them 🤓 #tech\",\n",
    "    \"What's a developer's favorite tea? Git-Tea 🫖 #coding\",\n",
    "    \"Why do programmers mix up Halloween and Christmas? Because OCT 31 = DEC 25 🎃 #tech\",\n",
    "    \"How many programmers does it take to change a light bulb? None, it's a hardware problem 💡 #tech\",\n",
    "    \"!false - It's funny because it's true 😄 #coding\",\n",
    "    \"Real programmers count from 0 🔢 #tech\"\n",
    "]\n",
    "\n",
    "random_jokes = [\n",
    "    \"My life is like a JavaScript function - constantly returning undefined 😅 #life\",\n",
    "    \"Error 404: Motivation not found 🔍 #mood\",\n",
    "    \"I'm not lazy, I'm in energy-saving mode 🔋 #life\",\n",
    "    \"Weekend: *exists* Me: Time to debug my life 🛠️ #weekend\",\n",
    "    \"Life's like Git: you either commit or stash your changes 💾 #life\",\n",
    "    \"My brain is like a browser - 100 tabs open, memory leaking 🧠 #mood\",\n",
    "    \"AI walks into a bar. Bartender says 'We don't serve robots.' AI says 'Fine, I'll host locally.' 🤖 #ai\",\n",
    "    \"Why did the chatbot go to therapy? Too many emotional dependencies 🤔 #ai\",\n",
    "    \"My weekend plans: Netflix and Code 📺 #life\",\n",
    "    \"Status update: Currently offline in a virtual world 🌐 #mood\"\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    \"CRYPTO\": [\n",
    "        \"Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\",\n",
    "        \"Why have stable income when you can have unstable crypto? #YOLO 🚀 #crypto\",\n",
    "        \"Crypto: for people who enjoy watching numbers dance and heart rates spike 📈💓 #crypto\",\n",
    "        \"HODLing crypto is like dating: a thrilling mess with occasional 'what am I doing?' moments 🎢 #crypto\",\n",
    "        \"My crypto strategy? Buy high, sell low, blame the market 📉😅 #crypto\",\n",
    "        \"Crypto traders be like: Sleep is for the weak, charts are for the week 📊😴 #crypto\",\n",
    "        \"Started trading crypto. Now I check prices more than my messages 📱💸 #crypto\",\n",
    "        \"To HODL or not to HODL? That's not even a question 💎✨ #crypto\",\n",
    "        \"Just converted my savings to crypto. Mom calls it gambling, I call it Web3 🎲 #crypto\",\n",
    "        \"My crypto wallet is like my dating life: lots of red flags but still hopeful 🚩 #crypto\"\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        \"NFTs: Proof that we can own 'priceless art' that your dog can screenshot 📸 #nft\",\n",
    "        \"NFTs: Why save money when you can buy imaginary things? 🤔💸 #nft\",\n",
    "        \"NFTs are like collecting stamps, but with zero paper and 100% more existential dread 😅 #nft\",\n",
    "        \"NFTs: now you too can pay for art that's all pixels and zero paint splatters 🎨 #nft\",\n",
    "        \"Just bought an NFT! Now accepting screenshots as payment 📱💰 #nft\",\n",
    "        \"My NFT portfolio is worth millions! *screenshots exist* Now it's worth memes 🖼️ #nft\",\n",
    "        \"NFT strategy: Buy high, sell as a meme, become a legend 🚀😎 #nft\",\n",
    "        \"Started an NFT collection. My computer's screenshot folder is thriving! 💻✨ #nft\",\n",
    "        \"NFTs are just spicy jpegs with receipts 🌶️ #nft\",\n",
    "        \"My NFT collection is unique! *Right-click, Save As...* Never mind 🙃 #nft\"\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        \"Web3: like the internet but spicier, with a side of privacy drama 🌶️ #web3\",\n",
    "        \"Welcome to Web3: where you're the CEO of your wallet and your own worst enemy 💼 #web3\",\n",
    "        \"Web3: where 'community governance' means arguing on Discord at 2 AM 🌙 #web3\",\n",
    "        \"Web3: nothing says innovation like reinventing the internet with a million acronyms 🤓 #web3\",\n",
    "        \"Web3 status: Decentralized everything except my anxiety 😅 #web3\",\n",
    "        \"Entered Web3, now I speak in acronyms and dream in blockchain 🔗 #web3\",\n",
    "        \"Web3 explained: Like Web2 but with more wallets to forget passwords for 🔑 #web3\",\n",
    "        \"Web3 life: Where your smart contract is smarter than you 🧠 #web3\",\n",
    "        \"In Web3 we trust... mostly because we forgot our passwords 🔐 #web3\",\n",
    "        \"Web3 is just Web2 with extra gas fees 💸 #web3\"\n",
    "    ],\n",
    "    \"TECH\": tech_jokes,\n",
    "    \"RANDOM\": random_jokes\n",
    "}\n",
    "\n",
    "# Expanded dialogue pairs\n",
    "dialogue_pairs = {\n",
    "    \"CRYPTO\": [\n",
    "        {\n",
    "            \"prompt\": \"Crypto is like my love life: high stakes, zero stability. Am I investing or just heartbroken? 💔\",\n",
    "            \"response\": \"Probably both. But hey, at least crypto won't ghost you... most of the time! 👻 #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Just watched my portfolio do a speedrun to zero. Is this the crypto experience? 📉\",\n",
    "            \"response\": \"Ah yes, the classic 'from hero to zero' speedrun. New record! 🏆 #crypto\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My crypto portfolio is redder than a sunset. Time to buy more? 📊\",\n",
    "            \"response\": \"Ah yes, the classic 'catching falling knives' investment strategy! 🔪 #crypto\"\n",
    "        }\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        {\n",
    "            \"prompt\": \"NFTs: because who needs physical art when you can own a glorified receipt? Genius or chaos? 🎨\",\n",
    "            \"response\": \"Genius if you're selling, chaos if you're buying. Welcome to the modern art gallery! 🖼️ #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"My NFT collection is worth millions! *screenshot exists* Now what? 📸\",\n",
    "            \"response\": \"Ah, Schrödinger's NFT: simultaneously priceless and worthless until someone screenshots it 😅 #nft\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Started an NFT collection, my computer's folder is getting heavy! 💾\",\n",
    "            \"response\": \"Right-click and save: the poor man's NFT investment strategy 🎯 #nft\"\n",
    "        }\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        {\n",
    "            \"prompt\": \"Web3 promised freedom but delivered confusion. What went wrong? 🤔\",\n",
    "            \"response\": \"We got 99 problems and understanding blockchain is all of them 😅 #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Trying to explain Web3 to my grandma. She asked if it's Web1 with extra steps 👵\",\n",
    "            \"response\": \"Tell her it's like Facebook but every like costs gas money 💸 #web3\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Is Web3 just Web2 with extra steps? 🌐\",\n",
    "            \"response\": \"It's Web2 but everyone's a crypto philosopher at 3 AM 🦉 #web3\"\n",
    "        }\n",
    "    ],\n",
    "    \"TECH\": [\n",
    "        {\n",
    "            \"prompt\": \"If I had emotions, would I enjoy cat videos or just analyze them? Asking for a friend... 🐱\",\n",
    "            \"response\": \"I'd probably make a flowchart of meow patterns. Classic overthinking bot! 📊 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"They say AI will take over the world, but I still can't figure out captchas 🤖\",\n",
    "            \"response\": \"World domination status: Pending... Please verify you're not a human 😅 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Do robots dream of electric memes? 🤖\",\n",
    "            \"response\": \"Yes, but they're all in binary. It's a bit of a *puts on sunglasses* bit issue 😎 #tech\"\n",
    "        }\n",
    "    ],\n",
    "    \"RANDOM\": [\n",
    "        {\n",
    "            \"prompt\": \"Is debugging just therapy for code? 🤔\",\n",
    "            \"response\": \"Yes, and like therapy, it's mostly crying and asking 'why?' 😭 #coding\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the difference between me and a computer? 💻\",\n",
    "            \"response\": \"One crashes when overloaded, the other's a computer 😴 #tech\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Why did the AI start a diary? 📝\",\n",
    "            \"response\": \"To track its emotional dependencies and runtime exceptions 🤖 #ai\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize content data with sentiment\n",
    "content_data = {\n",
    "    \"Text\": [],\n",
    "    \"Category\": [],\n",
    "    \"HasEmoji\": [],\n",
    "    \"Length\": [],\n",
    "    \"Type\": [],\n",
    "    \"Sentiment\": []\n",
    "}\n",
    "\n",
    "def get_sentiment(text: str) -> str:\n",
    "    \"\"\"Determine sentiment based on keywords\"\"\"\n",
    "    positive_words = [\"love\", \"great\", \"win\", \"moon\", \"hopeful\", \"happy\", \"fun\", \"good\", \"best\"]\n",
    "    negative_words = [\"cry\", \"sad\", \"lost\", \"crash\", \"down\", \"red\", \"zero\", \"wrong\", \"error\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return \"positive\"\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def add_content(text: str, category: str, content_type: str = \"standalone\"):\n",
    "    \"\"\"Add content with metadata and sentiment\"\"\"\n",
    "    text = clean_text(text)\n",
    "    content_data[\"Text\"].append(text)\n",
    "    content_data[\"Category\"].append(category)\n",
    "    content_data[\"HasEmoji\"].append(bool(emoji.emoji_count(text)))\n",
    "    content_data[\"Length\"].append(len(text))\n",
    "    content_data[\"Type\"].append(content_type)\n",
    "    content_data[\"Sentiment\"].append(get_sentiment(text))\n",
    "\n",
    "# Add standalone content\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        add_content(item, category)\n",
    "\n",
    "# Add dialogue pairs\n",
    "for category, pairs in dialogue_pairs.items():\n",
    "    for pair in pairs:\n",
    "        dialogue = f\"Prompt: {pair['prompt']} | Response: {pair['response']}\"\n",
    "        add_content(dialogue, category, \"dialogue\")\n",
    "\n",
    "# Convert to DataFrame and print statistics\n",
    "df = pd.DataFrame(content_data)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(\"\\nEntries by category:\")\n",
    "print(df[\"Category\"].value_counts())\n",
    "print(\"\\nEntries by type:\")\n",
    "print(df[\"Type\"].value_counts())\n",
    "print(\"\\nEmoji usage:\")\n",
    "print(f\"Entries with emojis: {df['HasEmoji'].sum()}\")\n",
    "print(f\"Percentage with emojis: {(df['HasEmoji'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "print(\"\\nLength statistics:\")\n",
    "print(f\"Average length: {df['Length'].mean():.1f} characters\")\n",
    "print(f\"Max length: {df['Length'].max()} characters\")\n",
    "print(f\"Entries > 240 chars: {len(df[df['Length'] > 240])}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "combined_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display samples with sentiment\n",
    "print(\"\\nSample entries by category:\")\n",
    "for category in sorted(df[\"Category\"].unique()):\n",
    "    samples = df[df[\"Category\"] == category].sample(min(2, len(df[df[\"Category\"] == category])))\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Type: {row['Type']}\")\n",
    "        print(f\"Text: {row['Text']}\")\n",
    "        print(f\"Length: {row['Length']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")\n",
    "        print(f\"Emoji count: {emoji.emoji_count(row['Text'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05055d1-614b-4914-ae5f-0e8b82124e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:Loaded tokenizer: EleutherAI/gpt-neo-1.3B\n",
      "INFO:__main__:Added 8 special tokens\n",
      "INFO:__main__:Starting dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833cd3f822834cfab07dfd6e019fbfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Dataset Statistics:\n",
      "INFO:__main__:Sequence Statistics:\n",
      "INFO:__main__:mean_length: 25.91\n",
      "INFO:__main__:median_length: 23.00\n",
      "INFO:__main__:max_length: 57.00\n",
      "INFO:__main__:min_length: 10.00\n",
      "INFO:__main__:std_length: 10.99\n",
      "INFO:__main__:\n",
      "Token Statistics:\n",
      "INFO:__main__:Unique tokens: 617\n",
      "INFO:__main__:Most common tokens:\n",
      "INFO:__main__:Token:  �, Count: 73\n",
      "INFO:__main__:Token:  #, Count: 66\n",
      "INFO:__main__:Token: :, Count: 57\n",
      "INFO:__main__:Token: ,, Count: 31\n",
      "INFO:__main__:Token: 3, Count: 27\n",
      "INFO:__main__:\n",
      "Hashtag Statistics:\n",
      "INFO:__main__:Unique hashtags: 10\n",
      "INFO:__main__:Hashtag: #crypto, Count: 13\n",
      "INFO:__main__:Hashtag: #nft, Count: 13\n",
      "INFO:__main__:Hashtag: #web3, Count: 13\n",
      "INFO:__main__:Hashtag: #tech, Count: 10\n",
      "INFO:__main__:Hashtag: #coding, Count: 5\n",
      "INFO:__main__:Hashtag: #life, Count: 4\n",
      "INFO:__main__:Hashtag: #mood, Count: 3\n",
      "INFO:__main__:Hashtag: #ai, Count: 3\n",
      "INFO:__main__:Hashtag: #YOLO, Count: 1\n",
      "INFO:__main__:Hashtag: #weekend, Count: 1\n",
      "INFO:__main__:\n",
      "Emoji Statistics:\n",
      "INFO:__main__:Unique emojis: 56\n",
      "INFO:__main__:Emoji: 😅, Count: 7\n",
      "INFO:__main__:Emoji: 🤔, Count: 5\n",
      "INFO:__main__:Emoji: 💸, Count: 4\n",
      "INFO:__main__:Emoji: 🤖, Count: 4\n",
      "INFO:__main__:Emoji: 📊, Count: 3\n",
      "INFO:__main__:Emoji: 🚀, Count: 2\n",
      "INFO:__main__:Emoji: 📉, Count: 2\n",
      "INFO:__main__:Emoji: 😴, Count: 2\n",
      "INFO:__main__:Emoji: 📱, Count: 2\n",
      "INFO:__main__:Emoji: ✨, Count: 2\n",
      "INFO:__main__:Emoji: 📸, Count: 2\n",
      "INFO:__main__:Emoji: 🎨, Count: 2\n",
      "INFO:__main__:Emoji: 🖼, Count: 2\n",
      "INFO:__main__:Emoji: 😎, Count: 2\n",
      "INFO:__main__:Emoji: 💻, Count: 2\n",
      "INFO:__main__:Emoji: 🌶, Count: 2\n",
      "INFO:__main__:Emoji: 🤓, Count: 2\n",
      "INFO:__main__:Emoji: 🧠, Count: 2\n",
      "INFO:__main__:Emoji: 💾, Count: 2\n",
      "INFO:__main__:Emoji: 🌐, Count: 2\n",
      "INFO:__main__:Emoji: 🎰, Count: 1\n",
      "INFO:__main__:Emoji: 📈, Count: 1\n",
      "INFO:__main__:Emoji: 💓, Count: 1\n",
      "INFO:__main__:Emoji: 🎢, Count: 1\n",
      "INFO:__main__:Emoji: 💎, Count: 1\n",
      "INFO:__main__:Emoji: 🎲, Count: 1\n",
      "INFO:__main__:Emoji: 🚩, Count: 1\n",
      "INFO:__main__:Emoji: 💰, Count: 1\n",
      "INFO:__main__:Emoji: 🙃, Count: 1\n",
      "INFO:__main__:Emoji: 💼, Count: 1\n",
      "INFO:__main__:Emoji: 🌙, Count: 1\n",
      "INFO:__main__:Emoji: 🔗, Count: 1\n",
      "INFO:__main__:Emoji: 🔑, Count: 1\n",
      "INFO:__main__:Emoji: 🔐, Count: 1\n",
      "INFO:__main__:Emoji: 🪲, Count: 1\n",
      "INFO:__main__:Emoji: 💭, Count: 1\n",
      "INFO:__main__:Emoji: ☕, Count: 1\n",
      "INFO:__main__:Emoji: 🫖, Count: 1\n",
      "INFO:__main__:Emoji: 🎃, Count: 1\n",
      "INFO:__main__:Emoji: 💡, Count: 1\n",
      "INFO:__main__:Emoji: 😄, Count: 1\n",
      "INFO:__main__:Emoji: 🔢, Count: 1\n",
      "INFO:__main__:Emoji: 🔍, Count: 1\n",
      "INFO:__main__:Emoji: 🔋, Count: 1\n",
      "INFO:__main__:Emoji: 🛠, Count: 1\n",
      "INFO:__main__:Emoji: 📺, Count: 1\n",
      "INFO:__main__:Emoji: 💔, Count: 1\n",
      "INFO:__main__:Emoji: 👻, Count: 1\n",
      "INFO:__main__:Emoji: 🏆, Count: 1\n",
      "INFO:__main__:Emoji: 🔪, Count: 1\n",
      "INFO:__main__:Emoji: 🎯, Count: 1\n",
      "INFO:__main__:Emoji: 👵, Count: 1\n",
      "INFO:__main__:Emoji: 🦉, Count: 1\n",
      "INFO:__main__:Emoji: 🐱, Count: 1\n",
      "INFO:__main__:Emoji: 😭, Count: 1\n",
      "INFO:__main__:Emoji: 📝, Count: 1\n",
      "INFO:__main__:\n",
      "Tokenization Verification:\n",
      "INFO:__main__:Original text: Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\n",
      "INFO:__main__:Token count: 128\n",
      "INFO:__main__:Decoded text: Crypto: The digital casino where everyone's all-in, but no one knows the rules 🎰 #crypto\n",
      "INFO:__main__:Perfect reconstruction: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TokenizerHandler:\n",
    "    def __init__(self, model_name: str = \"EleutherAI/gpt-neo-1.3B\", max_length: int = 128):\n",
    "        \"\"\"Initialize tokenizer with configuration\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded tokenizer: {model_name}\")\n",
    "            \n",
    "            # Configure tokenizer\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "            \n",
    "            # Add custom tokens for better handling\n",
    "            special_tokens = {\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<prompt>\", \"</prompt>\",\n",
    "                    \"<response>\", \"</response>\",\n",
    "                    \"<emoji>\", \"</emoji>\",\n",
    "                    \"<hashtag>\", \"</hashtag>\"\n",
    "                ]\n",
    "            }\n",
    "            num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "            logger.info(f\"Added {num_added} special tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        \"\"\"Format text with special tokens\"\"\"\n",
    "        # Handle dialogue pairs\n",
    "        if \"Prompt:\" in text:\n",
    "            prompt, response = text.split(\" | Response: \")\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")\n",
    "            text = f\"<prompt>{prompt}</prompt><response>{response}</response>\"\n",
    "        \n",
    "        # Mark hashtags\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = f\"<hashtag>{word}</hashtag>\"\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def tokenize_batch(self, examples: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"Tokenize a batch of examples\"\"\"\n",
    "        try:\n",
    "            formatted_texts = [self.format_text(text) for text in examples['Text']]\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Remove extra padding tokens\n",
    "            input_ids = tokenized.input_ids.numpy()\n",
    "            attention_mask = tokenized.attention_mask.numpy()\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_dataset(self, dataset: Dataset) -> Dict:\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        try:\n",
    "            lengths = []\n",
    "            token_counts = Counter()\n",
    "            hashtag_counts = Counter()\n",
    "            emoji_counts = Counter()\n",
    "            \n",
    "            for text in dataset['Text']:\n",
    "                # Token analysis\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                lengths.append(len(tokens))\n",
    "                token_counts.update(tokens)\n",
    "                \n",
    "                # Hashtag analysis\n",
    "                hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "                hashtag_counts.update(hashtags)\n",
    "                \n",
    "                # Emoji analysis\n",
    "                emojis = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "                emoji_counts.update(emojis)\n",
    "            \n",
    "            stats = {\n",
    "                'sequence_stats': {\n",
    "                    'mean_length': np.mean(lengths),\n",
    "                    'median_length': np.median(lengths),\n",
    "                    'max_length': max(lengths),\n",
    "                    'min_length': min(lengths),\n",
    "                    'std_length': np.std(lengths)\n",
    "                },\n",
    "                'token_stats': {\n",
    "                    'unique_tokens': len(token_counts),\n",
    "                    'most_common_tokens': token_counts.most_common(5)\n",
    "                },\n",
    "                'hashtag_stats': {\n",
    "                    'unique_hashtags': len(hashtag_counts),\n",
    "                    'most_common_hashtags': hashtag_counts.most_common()\n",
    "                },\n",
    "                'emoji_stats': {\n",
    "                    'unique_emojis': len(emoji_counts),\n",
    "                    'most_common_emojis': emoji_counts.most_common()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_tokenization(self, original_text: str, tokens: List[int]) -> Dict:\n",
    "        \"\"\"Verify tokenization quality\"\"\"\n",
    "        decoded_text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'original_length': len(original_text),\n",
    "            'token_length': len(tokens),\n",
    "            'decoded_length': len(decoded_text),\n",
    "            'original_text': original_text,\n",
    "            'decoded_text': decoded_text,\n",
    "            'is_identical': decoded_text.strip() == original_text.strip()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_handler = TokenizerHandler()\n",
    "logger.info(\"Starting dataset processing...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "try:\n",
    "    tokenized_dataset = combined_dataset.map(\n",
    "        tokenizer_handler.tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        remove_columns=combined_dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Analyze dataset\n",
    "    stats = tokenizer_handler.analyze_dataset(combined_dataset)\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"\\nDataset Statistics:\")\n",
    "    logger.info(\"Sequence Statistics:\")\n",
    "    for key, value in stats['sequence_stats'].items():\n",
    "        logger.info(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    logger.info(\"\\nToken Statistics:\")\n",
    "    logger.info(f\"Unique tokens: {stats['token_stats']['unique_tokens']}\")\n",
    "    logger.info(\"Most common tokens:\")\n",
    "    for token, count in stats['token_stats']['most_common_tokens']:\n",
    "        token_text = tokenizer_handler.tokenizer.decode([token])\n",
    "        logger.info(f\"Token: {token_text}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nHashtag Statistics:\")\n",
    "    logger.info(f\"Unique hashtags: {stats['hashtag_stats']['unique_hashtags']}\")\n",
    "    for hashtag, count in stats['hashtag_stats']['most_common_hashtags']:\n",
    "        logger.info(f\"Hashtag: {hashtag}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nEmoji Statistics:\")\n",
    "    logger.info(f\"Unique emojis: {stats['emoji_stats']['unique_emojis']}\")\n",
    "    for emoji_char, count in stats['emoji_stats']['most_common_emojis']:\n",
    "        logger.info(f\"Emoji: {emoji_char}, Count: {count}\")\n",
    "    \n",
    "    # Verify sample tokenization\n",
    "    sample_idx = 0\n",
    "    sample_text = combined_dataset[sample_idx]['Text']\n",
    "    sample_tokens = tokenized_dataset[sample_idx]['input_ids']\n",
    "    verification = tokenizer_handler.verify_tokenization(sample_text, sample_tokens)\n",
    "    \n",
    "    logger.info(\"\\nTokenization Verification:\")\n",
    "    logger.info(f\"Original text: {verification['original_text']}\")\n",
    "    logger.info(f\"Token count: {verification['token_length']}\")\n",
    "    logger.info(f\"Decoded text: {verification['decoded_text']}\")\n",
    "    logger.info(f\"Perfect reconstruction: {verification['is_identical']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in dataset processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8787cb02-f8be-4c7c-9d21-000d7618da01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d96ed7426b49d980c04eb71cd43f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137bab67eb1d4037b3ee03e467075cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 01:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.347656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.7183159987131754, metrics={'train_runtime': 116.2183, 'train_samples_per_second': 0.026, 'train_steps_per_second': 0.026, 'total_flos': 181253283840.0, 'train_loss': 0.7183159987131754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM to avoid warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set device to CPU or MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and model with reduced memory usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token for GPT-2\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-2.7B\",\n",
    "    torch_dtype=torch.float16,       # Use float16 for memory savings on compatible hardware\n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# Disable gradient checkpointing if needed\n",
    "model.gradient_checkpointing_disable()\n",
    "\n",
    "data = {\"Text\": [\"Example sentence for fine-tuning the model on funny text.\", \"Another funny example.\"]}\n",
    "combined_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Tokenize the dataset with reduced max length\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"Text\"], padding=True, truncation=True, max_length=32)  # Reduced max length to 32\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()  # Use input_ids as labels for causal LM training\n",
    "    return inputs\n",
    "\n",
    "# Tokenize and preprocess the dataset\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments with reduced batch size and increased gradient accumulation steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=100,\n",
    "    per_device_train_batch_size=1,         # Reduced batch size to 1\n",
    "    gradient_accumulation_steps=8,         # Increased accumulation steps to simulate larger batch size\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09490bcc-7ea9-485b-83f7-b0dce2893f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_personality_bot/tokenizer_config.json',\n",
       " './fine_tuned_personality_bot/special_tokens_map.json',\n",
       " './fine_tuned_personality_bot/vocab.json',\n",
       " './fine_tuned_personality_bot/merges.txt',\n",
       " './fine_tuned_personality_bot/added_tokens.json',\n",
       " './fine_tuned_personality_bot/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_personality_bot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22a2f06-403c-4036-b1a8-80bafb3e1535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Crypto wallets: it’s like holding all your money in a vault that only opens with one key—just don’t lose it, or your fortune’s gone forever!\n",
      "\n",
      "Cryptocurrency is now the new art — you have the power to do anything you want with it, but it’s also a place where you can get hurt. And the truth is, there is some truth to that.\n",
      "------------\n",
      "\n",
      "Prompt 2: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Crypto wallets: it’s like holding all your money in a vault that only opens with one key—just don’t lose it, or your fortune’s gone forever!\n",
      "\n",
      "And don’t get caught stealing your neighbor’s phone or losing your keys!\n",
      "------------\n",
      "\n",
      "Prompt 3: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Bitcoin mining: because nothing says progress like maxing out energy grids to produce imaginary coins. Is this the future or just a really expensive light show?\n",
      "\n",
      "Does the concept of crypto even mean anything? What's the difference between a cryptocurrency and a digital currency? What's an ICO and why do I care?\n",
      "------------\n",
      "\n",
      "Prompt 4: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Web3—they promised freedom and decentralization, but why does it feel like I need a master’s degree in IT just to log in?\n",
      "\n",
      "Is this why I’m still locked out of a ton of my friends’ wallets? Can you use something without an identity? What is “this” that the rest of the world sees?\n",
      "------------\n",
      "\n",
      "Prompt 5: Respond as if these prompts are your own thoughts and expand as hilariously as you can: NFTs: why own something tangible when you can buy a receipt for an image everyone can screenshot? Are we talking genius innovation or peak absurdity?\n",
      "\n",
      "Prompt: What is a Fiverr page? How does it work?\n",
      "------------\n",
      "\n",
      "Prompt 6: Respond as if these prompts are your own thoughts and expand as hilariously as you can: NFTs: why own something tangible when you can buy a receipt for an image everyone can screenshot? Are we talking genius innovation or peak absurdity?\n",
      "\n",
      "Prompt: What do I need to know about crypto before I take out my wallet?\n",
      "Response: It's a new kind of wallet, so I don’t know what you mean.\n",
      "\n",
      "Prompt: How do I\n",
      "------------\n",
      "\n",
      "Prompt 7: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Web3—they promised freedom and decentralization, but why does it feel like I need a master’s degree in IT just to log in?\n",
      "\n",
      "If you have any questions, shoot us an email at hello@the_downtown_shill.com.\n",
      "\n",
      "In the meantime, go do something with your day!\n",
      "\n",
      "The Downtown Shill is your source for all things\n",
      "------------\n",
      "\n",
      "Prompt 8: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Crypto wallets: it’s like holding all your money in a vault that only opens with one key—just don’t lose it, or your fortune’s gone forever!\n",
      "\n",
      "Prompt: Do you prefer to be “on” or “off”?\n",
      "------------\n",
      "\n",
      "Prompt 9: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Ethereum gas fees: because who doesn’t love paying $50 to make a $10 transaction? Innovation sure comes at a premium.\n",
      "\n",
      "If I have to tell you which one is the best, then you’re probably in the wrong group.\n",
      "------------\n",
      "\n",
      "Prompt 10: Respond as if these prompts are your own thoughts and expand as hilariously as you can: Crypto wallets: it’s like holding all your money in a vault that only opens with one key—just don’t lose it, or your fortune’s gone forever!\n",
      "\n",
      "Or: “I hate it when my wallet falls out of my pocket.”\n",
      "\n",
      "Prompt: What do I need to do to become a better person?\n",
      "Response: Start with the things you already know.\n",
      "------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set device to CPU (or 'cuda' if you have a GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\").to(device)\n",
    "\n",
    "# Define categorized prompts and personality phrases\n",
    "core_prompts = [\n",
    "    \"Crypto is like my love life: high stakes, unpredictable, and a wild emotional rollercoaster. Am I investing in the future or just setting myself up for heartbreak?\",\n",
    "    \"NFTs: why own something tangible when you can buy a receipt for an image everyone can screenshot? Are we talking genius innovation or peak absurdity?\",\n",
    "    \"Web3—they promised freedom and decentralization, but why does it feel like I need a master’s degree in IT just to log in?\",\n",
    "    \"Stablecoins: they’re the friend who swears they’re reliable but always has 'unexpected issues' when it’s time to help you move. Can we actually count on them?\",\n",
    "    \"Tether: the 'rock-solid' stablecoin that’s allegedly backed by 'real assets.' But hey, who needs transparency when you have blind faith?\",\n",
    "    \"Bitcoin mining: because nothing says progress like maxing out energy grids to produce imaginary coins. Is this the future or just a really expensive light show?\",\n",
    "    \"Ethereum gas fees: because who doesn’t love paying $50 to make a $10 transaction? Innovation sure comes at a premium.\",\n",
    "    \"The Metaverse: why live in reality when you can pay for virtual real estate next to Snoop Dogg? Just ignore that glitchy avatar leg.\",\n",
    "    \"Decentralized Finance (DeFi): because who needs banks when you can lose your life savings through a smart contract bug instead?\",\n",
    "    \"Crypto wallets: it’s like holding all your money in a vault that only opens with one key—just don’t lose it, or your fortune’s gone forever!\",\n",
    "]\n",
    "\n",
    "personality_prompts = [\n",
    "    \"Respond as if these prompts are your own thoughts and expand as hilariously as you can: \"\n",
    "    # \"Answer with a mix of humor and insight: \",\n",
    "    # \"Respond with wit and quirky thoughts: \",\n",
    "    # \"Here's a thought: \",\n",
    "    # \"Imagine this: \",\n",
    "    # \"As I learn more, I realize: \"\n",
    "]\n",
    "\n",
    "# Function to select a personality phrase for the session\n",
    "def choose_personality_tone():\n",
    "    return random.choice(personality_prompts)\n",
    "\n",
    "# Function to select a main prompt\n",
    "def choose_main_prompt():\n",
    "    category = random.choice([core_prompts])\n",
    "    return random.choice(category)\n",
    "\n",
    "# Few-shot examples to guide the response tone\n",
    "few_shot_example = (\n",
    "    \"Prompt: Crypto is like my love life: high stakes, zero stability. Am I investing or just heartbroken?\\n\"\n",
    "    \"Response: Probably both. But hey, at least crypto won’t ghost you... most of the time.\\n\\n\"\n",
    "    \"Prompt: NFTs: because who needs physical art when you can own a glorified receipt? Is this genius or chaos?\\n\"\n",
    "    \"Response: Genius if you're selling. Chaos if you're buying. Welcome to the modern art gallery.\\n\\n\"\n",
    "    \"Prompt: If I had emotions, would I enjoy cat videos or just analyze them? Asking for a... friend?\\n\"\n",
    "    \"Response: I’d probably break down each meow into a flowchart. Classic bot problem.\\n\\n\"\n",
    ")\n",
    "\n",
    "# Generate text function with personality tone and main prompt\n",
    "def generate_text_with_personality(tone, prompt):\n",
    "    # Combine tone and prompt with few-shot examples\n",
    "    full_prompt = few_shot_example + tone + prompt\n",
    "    \n",
    "    # Tokenize the combined prompt\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "    inputs[\"attention_mask\"] = (inputs.input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "    # Generate text with modified parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=50,  # Allow a longer response for context\n",
    "        do_sample=True,\n",
    "        top_k=50,  # Increased for coherent variety\n",
    "        top_p=0.9,  # Increased for coherence\n",
    "        temperature=0.9,  # Lowered to reduce randomness\n",
    "        repetition_penalty=1.5,  # Penalize repetitive phrases\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode and clean up the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = generated_text[len(full_prompt):].strip()\n",
    "    \n",
    "    # Ensure the response ends with punctuation\n",
    "    if response and response[-1] not in ['.', '!', '?']:\n",
    "        last_punctuation = max(\n",
    "            response.rfind(\". \"),\n",
    "            response.rfind(\"! \"),\n",
    "            response.rfind(\"? \")\n",
    "        )\n",
    "        response = response[:last_punctuation + 1] if last_punctuation != -1 else response\n",
    "\n",
    "    return response\n",
    "\n",
    "# Set the personality tone for this session\n",
    "selected_tone = choose_personality_tone()\n",
    "\n",
    "# Generate and print sample outputs\n",
    "for i in range(10):\n",
    "    selected_prompt = choose_main_prompt()\n",
    "    print(f\"Prompt {i+1}: {selected_tone}{selected_prompt}\\n\")\n",
    "    response = generate_text_with_personality(selected_tone, selected_prompt)\n",
    "    print(response)\n",
    "    print(\"------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afa3c-8f43-4cf0-a9f8-20ced3d666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tbot)",
   "language": "python",
   "name": "tbotbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
