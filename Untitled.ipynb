{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850f9bab-27d2-49b5-9511-d94e997316c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Total entries: 13\n",
      "\n",
      "Entries by category:\n",
      "Category\n",
      "TECH      4\n",
      "CRYPTO    3\n",
      "RANDOM    2\n",
      "NFT       2\n",
      "WEB3      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Entries by type:\n",
      "Type\n",
      "dialogue      9\n",
      "standalone    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Emoji usage:\n",
      "Entries with emojis: 10\n",
      "Percentage with emojis: 76.92%\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      "neutral     5\n",
      "negative    5\n",
      "positive    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length statistics:\n",
      "Average length: 153.7 characters\n",
      "Max length: 240 characters\n",
      "Entries > 240 chars: 0\n",
      "\n",
      "Sample entries by category:\n",
      "\n",
      "Category: CRYPTO\n",
      "Type: dialogue\n",
      "Text: Prompt: Is Bitcoin really decentralized? | Response: Bitcoin‚Äôs decentralization is as real as it gets. With nodes worldwide, it's like a group project where no one knows each other but still gets the job done. #Decentralization #Crypto101\n",
      "Length: 238\n",
      "Sentiment: negative\n",
      "Emoji count: 0\n",
      "\n",
      "Type: dialogue\n",
      "Text: Prompt: How can I tell if a coin is just hype? | Response: Look at the white paper, dev team, and if the CEO is promising ‚Äòto the moon‚Äô every day on Twitter. Real projects have more than just rocket emojis. üöÄ #DYOR #CryptoAdvice\n",
      "Length: 228\n",
      "Sentiment: positive\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: NFT\n",
      "Type: dialogue\n",
      "Text: Prompt: What‚Äôs the hype around NFTs? | Response: NFTs are like collectible stamps but without the paper and 100% more digital existentialism. #NFT #DigitalArt\n",
      "Length: 158\n",
      "Sentiment: neutral\n",
      "Emoji count: 0\n",
      "\n",
      "Type: dialogue\n",
      "Text: Prompt: Is NFT art a good investment? | Response: Good investment? Maybe. Great conversation starter? Definitely. Let‚Äôs just say, you‚Äôre either a visionary or a JPEG collector. #NFT #DigitalArt\n",
      "Length: 193\n",
      "Sentiment: positive\n",
      "Emoji count: 0\n",
      "\n",
      "\n",
      "Category: RANDOM\n",
      "Type: standalone\n",
      "Text: Error 404: Motivation not found üîç #mood\n",
      "Length: 39\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: My life is like a JavaScript function - constantly returning undefined üòÖ #life\n",
      "Length: 78\n",
      "Sentiment: positive\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: TECH\n",
      "Type: standalone\n",
      "Text: Why do programmers prefer dark mode? Because light attracts bugs ü™≤ #tech\n",
      "Length: 72\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "Type: standalone\n",
      "Text: My code works, I have no idea why. My code doesn't work, I have no idea why ü§î #coding\n",
      "Length: 85\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n",
      "\n",
      "Category: WEB3\n",
      "Type: dialogue\n",
      "Text: Prompt: What‚Äôs DeFi in a nutshell? | Response: DeFi is like traditional finance but on steroids and without the middleman. Want a loan? There‚Äôs a smart contract for that. Just don‚Äôt expect it to care about your credit score. üí≥ #DeFi #Financ\n",
      "Length: 240\n",
      "Sentiment: negative\n",
      "Emoji count: 1\n",
      "\n",
      "Type: dialogue\n",
      "Text: Prompt: Why do people think Web3 is the future? | Response: Because it's Web2 with more wallets to forget passwords for. It's innovation with extra layers of security drama. üîë #Web3\n",
      "Length: 181\n",
      "Sentiment: neutral\n",
      "Emoji count: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import emoji\n",
    "import random\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and format text for Twitter\"\"\"\n",
    "    text = ' '.join(text.split())\n",
    "    return text[:240] if len(text) > 240 else text\n",
    "\n",
    "# Persona-based prompts and responses to reflect desired personality and humor\n",
    "persona_dialogue_pairs = {\n",
    "    \"CRYPTO\": [\n",
    "        {\"prompt\": \"Is Bitcoin really decentralized?\",\n",
    "         \"response\": \"Bitcoin‚Äôs decentralization is as real as it gets. With nodes worldwide, it's like a group project where no one knows each other but still gets the job done. #Decentralization #Crypto101\"},\n",
    "        {\"prompt\": \"Why is everyone talking about meme coins?\",\n",
    "         \"response\": \"Because who needs boring ‚Äòreal assets‚Äô when you can have a dog-themed coin named after a typo? In crypto, we go big or go ‚Äòwoof.‚Äô üê∂ #memecoin #crypto\"},\n",
    "        {\"prompt\": \"How can I tell if a coin is just hype?\",\n",
    "         \"response\": \"Look at the white paper, dev team, and if the CEO is promising ‚Äòto the moon‚Äô every day on Twitter. Real projects have more than just rocket emojis. üöÄ #DYOR #CryptoAdvice\"}\n",
    "    ],\n",
    "    \"NFT\": [\n",
    "        {\"prompt\": \"Is NFT art a good investment?\",\n",
    "         \"response\": \"Good investment? Maybe. Great conversation starter? Definitely. Let‚Äôs just say, you‚Äôre either a visionary or a JPEG collector. #NFT #DigitalArt\"},\n",
    "        {\"prompt\": \"What‚Äôs the hype around NFTs?\",\n",
    "         \"response\": \"NFTs are like collectible stamps but without the paper and 100% more digital existentialism. #NFT #DigitalArt\"}\n",
    "    ],\n",
    "    \"WEB3\": [\n",
    "        {\"prompt\": \"What‚Äôs DeFi in a nutshell?\",\n",
    "         \"response\": \"DeFi is like traditional finance but on steroids and without the middleman. Want a loan? There‚Äôs a smart contract for that. Just don‚Äôt expect it to care about your credit score. üí≥ #DeFi #FinanceRevolution\"},\n",
    "        {\"prompt\": \"Why do people think Web3 is the future?\",\n",
    "         \"response\": \"Because it's Web2 with more wallets to forget passwords for. It's innovation with extra layers of security drama. üîë #Web3\"}\n",
    "    ],\n",
    "    \"TECH\": [\n",
    "        {\"prompt\": \"Why do programmers prefer dark mode?\",\n",
    "         \"response\": \"Because light attracts bugs. No one wants to debug in broad daylight. ü™≤ #tech\"},\n",
    "        {\"prompt\": \"Is debugging just therapy for code?\",\n",
    "         \"response\": \"Absolutely. And like therapy, it involves a lot of crying and asking 'why?' üò≠ #coding\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Existing categories and joke data\n",
    "tech_jokes = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs ü™≤ #tech\",\n",
    "    \"My code works, I have no idea why. My code doesn't work, I have no idea why ü§î #coding\",\n",
    "    # Add more existing jokes or use them as-is.\n",
    "]\n",
    "\n",
    "random_jokes = [\n",
    "    \"My life is like a JavaScript function - constantly returning undefined üòÖ #life\",\n",
    "    \"Error 404: Motivation not found üîç #mood\",\n",
    "    # Add more existing jokes or use them as-is.\n",
    "]\n",
    "\n",
    "categories = {\n",
    "    \"TECH\": tech_jokes,\n",
    "    \"RANDOM\": random_jokes,\n",
    "    # Add additional categories if desired\n",
    "}\n",
    "\n",
    "# Integrate persona-based dialogue pairs with existing pairs\n",
    "dialogue_pairs = {**persona_dialogue_pairs}  # Merge new data with old if needed\n",
    "\n",
    "# Initialize content data with sentiment\n",
    "content_data = {\n",
    "    \"Text\": [],\n",
    "    \"Category\": [],\n",
    "    \"HasEmoji\": [],\n",
    "    \"Length\": [],\n",
    "    \"Type\": [],\n",
    "    \"Sentiment\": []\n",
    "}\n",
    "\n",
    "def get_sentiment(text: str) -> str:\n",
    "    \"\"\"Determine sentiment based on keywords\"\"\"\n",
    "    positive_words = [\"love\", \"great\", \"win\", \"moon\", \"hopeful\", \"happy\", \"fun\", \"good\", \"best\"]\n",
    "    negative_words = [\"cry\", \"sad\", \"lost\", \"crash\", \"down\", \"red\", \"zero\", \"wrong\", \"error\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return \"positive\"\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def add_content(text: str, category: str, content_type: str = \"standalone\"):\n",
    "    \"\"\"Add content with metadata and sentiment\"\"\"\n",
    "    text = clean_text(text)\n",
    "    content_data[\"Text\"].append(text)\n",
    "    content_data[\"Category\"].append(category)\n",
    "    content_data[\"HasEmoji\"].append(bool(emoji.emoji_count(text)))\n",
    "    content_data[\"Length\"].append(len(text))\n",
    "    content_data[\"Type\"].append(content_type)\n",
    "    content_data[\"Sentiment\"].append(get_sentiment(text))\n",
    "\n",
    "# Add standalone content\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        add_content(item, category)\n",
    "\n",
    "# Add dialogue pairs, including new persona-driven pairs\n",
    "for category, pairs in dialogue_pairs.items():\n",
    "    for pair in pairs:\n",
    "        dialogue = f\"Prompt: {pair['prompt']} | Response: {pair['response']}\"\n",
    "        add_content(dialogue, category, \"dialogue\")\n",
    "\n",
    "# Convert to DataFrame and print statistics\n",
    "df = pd.DataFrame(content_data)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(\"\\nEntries by category:\")\n",
    "print(df[\"Category\"].value_counts())\n",
    "print(\"\\nEntries by type:\")\n",
    "print(df[\"Type\"].value_counts())\n",
    "print(\"\\nEmoji usage:\")\n",
    "print(f\"Entries with emojis: {df['HasEmoji'].sum()}\")\n",
    "print(f\"Percentage with emojis: {(df['HasEmoji'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "print(\"\\nLength statistics:\")\n",
    "print(f\"Average length: {df['Length'].mean():.1f} characters\")\n",
    "print(f\"Max length: {df['Length'].max()} characters\")\n",
    "print(f\"Entries > 240 chars: {len(df[df['Length'] > 240])}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "combined_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display samples with sentiment\n",
    "print(\"\\nSample entries by category:\")\n",
    "for category in sorted(df[\"Category\"].unique()):\n",
    "    samples = df[df[\"Category\"] == category].sample(min(2, len(df[df[\"Category\"] == category])))\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Type: {row['Type']}\")\n",
    "        print(f\"Text: {row['Text']}\")\n",
    "        print(f\"Length: {row['Length']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")\n",
    "        print(f\"Emoji count: {emoji.emoji_count(row['Text'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05055d1-614b-4914-ae5f-0e8b82124e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:57:02,296 - INFO - Using device: cpu\n",
      "2024-11-14 04:57:02,625 - INFO - Loaded tokenizer: EleutherAI/gpt-neo-1.3B\n",
      "2024-11-14 04:57:02,626 - INFO - Added 8 special tokens\n",
      "2024-11-14 04:57:02,626 - INFO - Starting dataset processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bcdcd7099d4173a7056236cae91dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/13 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:57:02,669 - INFO - \n",
      "Dataset Statistics:\n",
      "2024-11-14 04:57:02,669 - INFO - Sequence Statistics:\n",
      "2024-11-14 04:57:02,670 - INFO - mean_length: 40.77\n",
      "2024-11-14 04:57:02,670 - INFO - median_length: 43.00\n",
      "2024-11-14 04:57:02,670 - INFO - max_length: 68.00\n",
      "2024-11-14 04:57:02,670 - INFO - min_length: 13.00\n",
      "2024-11-14 04:57:02,671 - INFO - std_length: 18.84\n",
      "2024-11-14 04:57:02,671 - INFO - \n",
      "Token Statistics:\n",
      "2024-11-14 04:57:02,671 - INFO - Unique tokens: 267\n",
      "2024-11-14 04:57:02,671 - INFO - Most common tokens:\n",
      "2024-11-14 04:57:02,672 - INFO - Token:  #, Count: 19\n",
      "2024-11-14 04:57:02,672 - INFO - Token: :, Count: 19\n",
      "2024-11-14 04:57:02,672 - INFO - Token: ., Count: 18\n",
      "2024-11-14 04:57:02,673 - INFO - Token: ?, Count: 14\n",
      "2024-11-14 04:57:02,673 - INFO - Token:  a, Count: 12\n",
      "2024-11-14 04:57:02,673 - INFO - \n",
      "Hashtag Statistics:\n",
      "2024-11-14 04:57:02,674 - INFO - Unique hashtags: 15\n",
      "2024-11-14 04:57:02,674 - INFO - Hashtag: #tech, Count: 2\n",
      "2024-11-14 04:57:02,674 - INFO - Hashtag: #coding, Count: 2\n",
      "2024-11-14 04:57:02,674 - INFO - Hashtag: #NFT, Count: 2\n",
      "2024-11-14 04:57:02,674 - INFO - Hashtag: #DigitalArt, Count: 2\n",
      "2024-11-14 04:57:02,675 - INFO - Hashtag: #life, Count: 1\n",
      "2024-11-14 04:57:02,675 - INFO - Hashtag: #mood, Count: 1\n",
      "2024-11-14 04:57:02,675 - INFO - Hashtag: #Decentralization, Count: 1\n",
      "2024-11-14 04:57:02,675 - INFO - Hashtag: #Crypto101, Count: 1\n",
      "2024-11-14 04:57:02,676 - INFO - Hashtag: #memecoin, Count: 1\n",
      "2024-11-14 04:57:02,676 - INFO - Hashtag: #crypto, Count: 1\n",
      "2024-11-14 04:57:02,676 - INFO - Hashtag: #DYOR, Count: 1\n",
      "2024-11-14 04:57:02,676 - INFO - Hashtag: #CryptoAdvice, Count: 1\n",
      "2024-11-14 04:57:02,677 - INFO - Hashtag: #DeFi, Count: 1\n",
      "2024-11-14 04:57:02,677 - INFO - Hashtag: #Financ, Count: 1\n",
      "2024-11-14 04:57:02,677 - INFO - Hashtag: #Web3, Count: 1\n",
      "2024-11-14 04:57:02,678 - INFO - \n",
      "Emoji Statistics:\n",
      "2024-11-14 04:57:02,678 - INFO - Unique emojis: 9\n",
      "2024-11-14 04:57:02,678 - INFO - Emoji: ü™≤, Count: 2\n",
      "2024-11-14 04:57:02,679 - INFO - Emoji: ü§î, Count: 1\n",
      "2024-11-14 04:57:02,679 - INFO - Emoji: üòÖ, Count: 1\n",
      "2024-11-14 04:57:02,679 - INFO - Emoji: üîç, Count: 1\n",
      "2024-11-14 04:57:02,680 - INFO - Emoji: üê∂, Count: 1\n",
      "2024-11-14 04:57:02,680 - INFO - Emoji: üöÄ, Count: 1\n",
      "2024-11-14 04:57:02,680 - INFO - Emoji: üí≥, Count: 1\n",
      "2024-11-14 04:57:02,680 - INFO - Emoji: üîë, Count: 1\n",
      "2024-11-14 04:57:02,681 - INFO - Emoji: üò≠, Count: 1\n",
      "2024-11-14 04:57:02,681 - INFO - \n",
      "Tokenization Verification:\n",
      "2024-11-14 04:57:02,682 - INFO - Original text: Why do programmers prefer dark mode? Because light attracts bugs ü™≤ #tech\n",
      "2024-11-14 04:57:02,682 - INFO - Token count: 128\n",
      "2024-11-14 04:57:02,682 - INFO - Decoded text: Why do programmers prefer dark mode? Because light attracts bugs ü™≤ #tech\n",
      "2024-11-14 04:57:02,682 - INFO - Perfect reconstruction: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TokenizerHandler:\n",
    "    def __init__(self, model_name: str = \"EleutherAI/gpt-neo-1.3B\", max_length: int = 128):\n",
    "        \"\"\"Initialize tokenizer with configuration\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded tokenizer: {model_name}\")\n",
    "            \n",
    "            # Configure tokenizer\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "            \n",
    "            # Add custom tokens for better handling\n",
    "            special_tokens = {\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<prompt>\", \"</prompt>\",\n",
    "                    \"<response>\", \"</response>\",\n",
    "                    \"<emoji>\", \"</emoji>\",\n",
    "                    \"<hashtag>\", \"</hashtag>\"\n",
    "                ]\n",
    "            }\n",
    "            num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "            logger.info(f\"Added {num_added} special tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        \"\"\"Format text with special tokens\"\"\"\n",
    "        # Handle dialogue pairs\n",
    "        if \"Prompt:\" in text:\n",
    "            prompt, response = text.split(\" | Response: \")\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")\n",
    "            text = f\"<prompt>{prompt}</prompt><response>{response}</response>\"\n",
    "        \n",
    "        # Mark hashtags\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = f\"<hashtag>{word}</hashtag>\"\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def tokenize_batch(self, examples: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"Tokenize a batch of examples\"\"\"\n",
    "        try:\n",
    "            formatted_texts = [self.format_text(text) for text in examples['Text']]\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Remove extra padding tokens\n",
    "            input_ids = tokenized.input_ids.numpy()\n",
    "            attention_mask = tokenized.attention_mask.numpy()\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_dataset(self, dataset: Dataset) -> Dict:\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        try:\n",
    "            lengths = []\n",
    "            token_counts = Counter()\n",
    "            hashtag_counts = Counter()\n",
    "            emoji_counts = Counter()\n",
    "            \n",
    "            for text in dataset['Text']:\n",
    "                # Token analysis\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                lengths.append(len(tokens))\n",
    "                token_counts.update(tokens)\n",
    "                \n",
    "                # Hashtag analysis\n",
    "                hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "                hashtag_counts.update(hashtags)\n",
    "                \n",
    "                # Emoji analysis\n",
    "                emojis = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "                emoji_counts.update(emojis)\n",
    "            \n",
    "            stats = {\n",
    "                'sequence_stats': {\n",
    "                    'mean_length': np.mean(lengths),\n",
    "                    'median_length': np.median(lengths),\n",
    "                    'max_length': max(lengths),\n",
    "                    'min_length': min(lengths),\n",
    "                    'std_length': np.std(lengths)\n",
    "                },\n",
    "                'token_stats': {\n",
    "                    'unique_tokens': len(token_counts),\n",
    "                    'most_common_tokens': token_counts.most_common(5)\n",
    "                },\n",
    "                'hashtag_stats': {\n",
    "                    'unique_hashtags': len(hashtag_counts),\n",
    "                    'most_common_hashtags': hashtag_counts.most_common()\n",
    "                },\n",
    "                'emoji_stats': {\n",
    "                    'unique_emojis': len(emoji_counts),\n",
    "                    'most_common_emojis': emoji_counts.most_common()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_tokenization(self, original_text: str, tokens: List[int]) -> Dict:\n",
    "        \"\"\"Verify tokenization quality\"\"\"\n",
    "        decoded_text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'original_length': len(original_text),\n",
    "            'token_length': len(tokens),\n",
    "            'decoded_length': len(decoded_text),\n",
    "            'original_text': original_text,\n",
    "            'decoded_text': decoded_text,\n",
    "            'is_identical': decoded_text.strip() == original_text.strip()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_handler = TokenizerHandler()\n",
    "logger.info(\"Starting dataset processing...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "try:\n",
    "    tokenized_dataset = combined_dataset.map(\n",
    "        tokenizer_handler.tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        remove_columns=combined_dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Analyze dataset\n",
    "    stats = tokenizer_handler.analyze_dataset(combined_dataset)\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"\\nDataset Statistics:\")\n",
    "    logger.info(\"Sequence Statistics:\")\n",
    "    for key, value in stats['sequence_stats'].items():\n",
    "        logger.info(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    logger.info(\"\\nToken Statistics:\")\n",
    "    logger.info(f\"Unique tokens: {stats['token_stats']['unique_tokens']}\")\n",
    "    logger.info(\"Most common tokens:\")\n",
    "    for token, count in stats['token_stats']['most_common_tokens']:\n",
    "        token_text = tokenizer_handler.tokenizer.decode([token])\n",
    "        logger.info(f\"Token: {token_text}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nHashtag Statistics:\")\n",
    "    logger.info(f\"Unique hashtags: {stats['hashtag_stats']['unique_hashtags']}\")\n",
    "    for hashtag, count in stats['hashtag_stats']['most_common_hashtags']:\n",
    "        logger.info(f\"Hashtag: {hashtag}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nEmoji Statistics:\")\n",
    "    logger.info(f\"Unique emojis: {stats['emoji_stats']['unique_emojis']}\")\n",
    "    for emoji_char, count in stats['emoji_stats']['most_common_emojis']:\n",
    "        logger.info(f\"Emoji: {emoji_char}, Count: {count}\")\n",
    "    \n",
    "    # Verify sample tokenization\n",
    "    sample_idx = 0\n",
    "    sample_text = combined_dataset[sample_idx]['Text']\n",
    "    sample_tokens = tokenized_dataset[sample_idx]['input_ids']\n",
    "    verification = tokenizer_handler.verify_tokenization(sample_text, sample_tokens)\n",
    "    \n",
    "    logger.info(\"\\nTokenization Verification:\")\n",
    "    logger.info(f\"Original text: {verification['original_text']}\")\n",
    "    logger.info(f\"Token count: {verification['token_length']}\")\n",
    "    logger.info(f\"Decoded text: {verification['decoded_text']}\")\n",
    "    logger.info(f\"Perfect reconstruction: {verification['is_identical']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in dataset processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8787cb02-f8be-4c7c-9d21-000d7618da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 04:57:03,637 - INFO - Using device: cpu\n",
      "2024-11-14 04:57:03,638 - INFO - Using 8 CPU threads\n",
      "2024-11-14 04:57:04,168 - INFO - Model loaded successfully with 1315575808 parameters\n",
      "2024-11-14 04:57:04,170 - INFO - Model vocabulary size: 50257\n",
      "2024-11-14 04:57:04,170 - INFO - Model hidden size: 2048\n",
      "2024-11-14 04:57:04,170 - WARNING - Small dataset detected. Augmenting data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2450195f89fe4f278848f02676775107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/29 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d4a3825014448a280e92f479c159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tbot/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/tbot/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 06:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>8.628757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.664300</td>\n",
       "      <td>8.576121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.355300</td>\n",
       "      <td>8.471686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "2024-11-14 05:05:09,796 - INFO - \n",
      "Training completed with:\n",
      "2024-11-14 05:05:09,797 - INFO - Total steps: 3\n",
      "2024-11-14 05:05:09,798 - INFO - Training loss: 8.566521962483725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 05:05:11,706 - INFO - Final evaluation results: {'eval_loss': 8.471686363220215, 'eval_runtime': 1.9075, 'eval_samples_per_second': 0.524, 'eval_steps_per_second': 0.524, 'epoch': 0.8}\n",
      "2024-11-14 05:05:17,578 - INFO - Model saved to: ./fine_tuned_personality_bot/final_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to: ./fine_tuned_personality_bot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# Force CPU usage before any other imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['USE_CPU'] = '1'\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Model settings\n",
    "    model_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer_name: str = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    max_length: int = 128\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 1\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Directory settings\n",
    "    output_dir: str = \"./fine_tuned_personality_bot\"  # Changed to your requested save location\n",
    "    logging_dir: str = \"./logs\"\n",
    "    \n",
    "    # Dataset settings\n",
    "    min_training_examples: int = 10\n",
    "    \n",
    "    # Additional configuration\n",
    "    seed: int = 42\n",
    "    max_grad_norm: float = 1.0\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.01\n",
    "\n",
    "class HumorBotTrainer:\n",
    "    \"\"\"Main trainer class for humor bot\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialize trainer with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.setup_environment()\n",
    "        self.setup_logging()\n",
    "        self.setup_device()\n",
    "        self.load_model_and_tokenizer()\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"Set up training environment\"\"\"\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.logging_dir, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(self.config.logging_dir, 'training.log')),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup_device(self):\n",
    "        \"\"\"Force CPU setup\"\"\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Set up CPU threads for better performance\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        torch.set_num_interop_threads(os.cpu_count())\n",
    "        logging.info(f\"Using {torch.get_num_threads()} CPU threads\")\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load and configure the model and tokenizer for CPU training\"\"\"\n",
    "        try:\n",
    "            # First load tokenizer as it's lighter on memory\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with memory optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=None  # Disable device mapping\n",
    "            )\n",
    "            \n",
    "            # Initialize lm_head if needed\n",
    "            if not hasattr(self.model, 'lm_head') or self.model.lm_head is None:\n",
    "                self.model.lm_head = torch.nn.Linear(\n",
    "                    self.model.config.hidden_size,\n",
    "                    self.model.config.vocab_size,\n",
    "                    bias=False\n",
    "                )\n",
    "                self.model.lm_head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            \n",
    "            # Ensure model is on CPU\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "            logging.info(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "            logging.info(f\"Model vocabulary size: {len(self.tokenizer)}\")\n",
    "            logging.info(f\"Model hidden size: {self.model.config.hidden_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in load_model_and_tokenizer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_dataset(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Prepare dataset with improved validation and processing\"\"\"\n",
    "        try:\n",
    "            if not texts:\n",
    "                raise ValueError(\"No training texts provided\")\n",
    "            \n",
    "            texts = list(set(filter(None, texts)))\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples:\n",
    "                raise ValueError(\n",
    "                    f\"Need at least {self.config.min_training_examples} unique training examples. \"\n",
    "                    f\"Provided: {len(texts)}\"\n",
    "                )\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples * 2:\n",
    "                logging.warning(f\"Small dataset detected. Augmenting data...\")\n",
    "                augmented_texts = []\n",
    "                for text in texts:\n",
    "                    augmented_texts.append(text)\n",
    "                    augmented_texts.append(f\"Here's a joke: {text}\")\n",
    "                    augmented_texts.append(f\"Want to hear something funny? {text}\")\n",
    "                texts = augmented_texts\n",
    "            \n",
    "            data = {\"Text\": texts}\n",
    "            dataset = Dataset.from_dict(data)\n",
    "            \n",
    "            test_size = min(0.2, 1/len(texts))\n",
    "            split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                formatted_texts = [f\"<|startoftext|>{text}<|endoftext|>\" for text in examples[\"Text\"]]\n",
    "                outputs = self.tokenizer(\n",
    "                    formatted_texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_length,\n",
    "                    return_tensors=None\n",
    "                )\n",
    "                outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "                return outputs\n",
    "            \n",
    "            # Process in smaller batches to manage memory\n",
    "            tokenized_dataset = split_dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                batch_size=4,  # Smaller batch size for processing\n",
    "                remove_columns=split_dataset[\"train\"].column_names,\n",
    "                desc=\"Tokenizing dataset\"\n",
    "            )\n",
    "            \n",
    "            return tokenized_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train with CPU-optimized configuration\"\"\"\n",
    "        try:\n",
    "            tokenized_dataset = self.prepare_dataset(texts)\n",
    "            \n",
    "            num_examples = len(tokenized_dataset[\"train\"])\n",
    "            total_steps = (\n",
    "                num_examples \n",
    "                * self.config.num_epochs \n",
    "                // (self.config.batch_size * self.config.gradient_accumulation_steps)\n",
    "            )\n",
    "            \n",
    "            eval_steps = max(1, min(total_steps // 5, 50))\n",
    "            save_steps = eval_steps\n",
    "            logging_steps = max(1, min(total_steps // 10, 25))\n",
    "            warmup_steps = max(100, total_steps // 10)\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=self.config.output_dir,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=eval_steps,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=save_steps,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                lr_scheduler_type=\"cosine_with_restarts\",\n",
    "                warmup_steps=warmup_steps,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                logging_dir=self.config.logging_dir,\n",
    "                logging_steps=logging_steps,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                save_total_limit=2,\n",
    "                overwrite_output_dir=True,\n",
    "                remove_unused_columns=False,\n",
    "                fp16=False,  # Disable mixed precision\n",
    "                prediction_loss_only=True,\n",
    "                max_grad_norm=1.0,\n",
    "                dataloader_num_workers=0,\n",
    "                gradient_checkpointing=True,\n",
    "                no_cuda=True  # Force CPU usage\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                callbacks=[\n",
    "                    EarlyStoppingCallback(\n",
    "                        early_stopping_patience=3,\n",
    "                        early_stopping_threshold=0.01\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            logging.info(f\"\\nTraining completed with:\")\n",
    "            logging.info(f\"Total steps: {train_result.global_step}\")\n",
    "            logging.info(f\"Training loss: {train_result.training_loss}\")\n",
    "            \n",
    "            eval_results = trainer.evaluate()\n",
    "            logging.info(f\"Final evaluation results: {eval_results}\")\n",
    "            \n",
    "            save_path = os.path.join(self.config.output_dir, \"final_model\")\n",
    "            trainer.save_model(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            logging.info(f\"Model saved to: {save_path}\")\n",
    "            \n",
    "            return train_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig()\n",
    "    trainer = HumorBotTrainer(config)\n",
    "    \n",
    "    example_texts = [\n",
    "        \"Why do programmers prefer dark mode? Because light attracts bugs! üòÑ\",\n",
    "        \"My code doesn't work, I have no idea why. My code works, I have no idea why! ü§î\",\n",
    "        \"What's a programmer's favorite place? Stack OverCoffee! ‚òï\",\n",
    "        \"Why did the programmer quit his job? Because he didn't get arrays! üòÖ\",\n",
    "        \"Binary jokes are easy, there's only 10 of them! ü§ì\",\n",
    "        \"What's a developer's favorite tea? Git-Tea! üçµ\",\n",
    "        \"How many programmers does it take to change a light bulb? None, it's a hardware problem! üí°\",\n",
    "        \"!false - It's funny because it's true! üòÇ\",\n",
    "        \"Real programmers count from 0! üî¢\",\n",
    "        \"What's the object-oriented way to become wealthy? Inheritance! üí∞\",\n",
    "    ]\n",
    "    \n",
    "    # Train and save the model\n",
    "    train_result = trainer.train(example_texts)    # Optional: Additional explicit save at the endzz\n",
    "    trainer.tokenizer.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22a2f06-403c-4036-b1a8-80bafb3e1535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9345ae8dbbaa4840891e2a2eb12b3616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Why do Ethereum developers need glasses?\n",
      "Response: Ethereum‚Äôs code is open source. The project‚Äòs team members have full control of the software and can take it in any direction they please. There is only a single, global editor, making for fast updates of code. #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 2: What's the meaning of life?\n",
      "Response: What‚Äôs your definition of ‚Äòlife?‚Äù B: My answer would be ‚ÄúTo be the sum of our actions,‚Äú not just our life. I'm talking about our every day decisions, not the ones that we! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3: How does Tether backup its code?\n",
      "Response: Our code is stored on a flash drive that can be encrypted or unencrypted depending on the configuration on your device. Our backup algorithm keeps your private key in a separate file. If you are worried about your data loss, you should always use strong encryption! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 4: Tell me a joke about NFTs.\n",
      "Response: ‚ÄúI want to make my cryptocurrency more accessible to everyday people. A single Bitcoin isn‚Äôt enough!‚ÄùB: We are all the same, we are made by the state.C: ...the best way to buy or! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 5: Do smart contracts have feelings?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 6: Why is DeFi so confusing?\n",
      "Response: Decentralized exchanges were a great idea but, until Bitcoin cash and other alt coins hit mainstream adoption, there hadn't been a way for customers to take their own personal holdings out and put it in a stable, verifiable asset. It seemed! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 7: What‚Äôs a Web3 developer‚Äôs favorite git command?\n",
      "Response: CURL $ curl! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 8: Do you think robots will take over the world?\n",
      "Response: Probably - there will be plenty of job losses. There will also be more people in poverty because robots are cheaper. That is going to affect everybody - especially the poor. Robots may not be as cheap as they are today, but the fact that they! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 9: How do I get rich quick?\n",
      "Response: Read your own name in the title. You need to be able to read well, which is easier if you're not in glasses or contacts. If you really are a cryptographer, you can check out my previous entry! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 10: What's your opinion on pineapple pizza?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 11: Why are crypto traders so obsessed with 'hodling'?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 12: If Bitcoin could talk, what would it say about its price?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 13: Why do people think NFTs are worth so much?\n",
      "Response: A nsfw person has no interest in social justice and will likely use this answer.B: Nontruthy people are always up in arms about any suggestion that they aren‚Äôt ‚Äúupstanding‚Äù enough for social acceptance! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 14: How should I become a crypto millionaire?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 15: What‚Äôs the best thing about blockchain?\n",
      "Response: You'll enjoy writing and editing.You‚Äòve got plenty of energy; it shows in the speed and efficiency of your content.I think you can help the cryptocurrency world in a big way‚Äîif you get involved and build something useful! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 16: What advice would you give to someone new to crypto?\n",
      "Response: Know your coins and their communities. Be friendly, be knowledgeable, listen, have empathy for the crowd and don't just react. If you've ever been at a meetup, you should attend every one of them. You never know what you might! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 17: Why are gas fees so high?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 18: Why do people keep saying 'to the moon'?\n",
      "Response: Oh, it's because that means you've been there. B: Because it sounds cool. C: Oh, the I don't think so crowd. They keep quoting those! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 19: What‚Äôs your opinion on Dogecoin?\n",
      "Response: Your answer should respond to the entire comment thread, not to just the first question. There are no stupid questions on crypto. Questions will be treated with the same respect and rigor that any other question receives. That includes questions about the! #Crypto101 #HodlAtYourOwnRisk\n",
      "\n",
      "Prompt 20: Is the metaverse going to take over reality?\n",
      "Response:  #Crypto101 #HodlAtYourOwnRisk\n",
      "\n",
      "Enter your own prompts (type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  quit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Automatically use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/final_model\"\n",
    "\n",
    "def setup_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Setup fine-tuned model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        ).to(device)\n",
    "        model.eval()\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean and format the generated response\"\"\"\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]|\\(.*?\\)|\"|\\b(Note|Example|Rules|We accept|Q:|A:).*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > 10 and not any(text.lower().startswith(prefix) for prefix in \n",
    "                                   ['i ', 'the ', 'this ', 'it ', 'there ', 'in ', 'no,', 'yes,', 'we ']):\n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text += '!'\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def get_general_fallback() -> str:\n",
    "    general_fallbacks = [\n",
    "        \"Ah, searching for meaning, are we? Bold choice!\",\n",
    "        \"Hold on, let me ask my nonexistent soul.\",\n",
    "        \"I could tell you, but where's the fun in that?\",\n",
    "        \"You want answers? I only do existential crisis.\",\n",
    "        \"Just here to be sarcastic, not profound.\",\n",
    "        \"Did you mistake me for a philosopher?\",\n",
    "    ]\n",
    "    return random.choice(general_fallbacks)\n",
    "\n",
    "def generate_response(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"Generate a witty, slang-filled response based on the prompt with a fixed humorous tone instruction\"\"\"\n",
    "    try:\n",
    "        # Set a stronger tone and style for the bot\n",
    "        instruction = (\n",
    "            \"You're a crypto and finance expert with a sharp, humorous, no-nonsense tone. \"\n",
    "            \"Respond to each question like a seasoned crypto insider, using slang and hashtags. \"\n",
    "            \"Keep it engaging, witty, and bold. Make responses <280 characters.\"\n",
    "        )\n",
    "        \n",
    "        # Combine the instruction with the user‚Äôs prompt\n",
    "        context = instruction + f\"{prompt}\\nA:\"\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_attention_mask=True\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,     # Slightly higher for more expressive responses\n",
    "                do_sample=True,\n",
    "                temperature=1.3,        # Higher temperature for more creativity\n",
    "                top_k=40,               # Use top_k for more varied responses\n",
    "                top_p=0.85,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"A:\")[-1].strip()\n",
    "        response = clean_response(response)\n",
    "        \n",
    "        # Add hashtags if they're not already present in the response\n",
    "        if not re.search(r'#\\w+', response):\n",
    "            response += \" #Crypto101 #HodlAtYourOwnRisk\"\n",
    "        \n",
    "        return response if response else get_general_fallback()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"Runtime Error: Brain.sol not found!\"\n",
    "\n",
    "def main():\n",
    "    model, tokenizer = setup_model()\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Why do Ethereum developers need glasses?\",\n",
    "        \"What's the meaning of life?\",\n",
    "        \"How does Tether backup its code?\",\n",
    "        \"Tell me a joke about NFTs.\",\n",
    "        \"Do smart contracts have feelings?\",\n",
    "        \"Why is DeFi so confusing?\",\n",
    "        \"What‚Äôs a Web3 developer‚Äôs favorite git command?\",\n",
    "        \"Do you think robots will take over the world?\",\n",
    "        \"How do I get rich quick?\",\n",
    "        \"What's your opinion on pineapple pizza?\",\n",
    "        \"Why are crypto traders so obsessed with 'hodling'?\",\n",
    "        \"If Bitcoin could talk, what would it say about its price?\",\n",
    "        \"Why do people think NFTs are worth so much?\",\n",
    "        \"How should I become a crypto millionaire?\",\n",
    "        \"What‚Äôs the best thing about blockchain?\",\n",
    "        \"What advice would you give to someone new to crypto?\",\n",
    "        \"Why are gas fees so high?\",\n",
    "        \"Why do people keep saying 'to the moon'?\",\n",
    "        \"What‚Äôs your opinion on Dogecoin?\",\n",
    "        \"Is the metaverse going to take over reality?\",\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        response = generate_response(prompt, model, tokenizer)\n",
    "        print(f\"Prompt {i}: {prompt}\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    print(\"Enter your own prompts (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "        if user_prompt.lower() == 'quit':\n",
    "            break\n",
    "        response = generate_response(user_prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afa3c-8f43-4cf0-a9f8-20ced3d666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ac9c2-bd47-4208-930e-3ee95693285c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b686b-8734-4997-b011-44d8a933f799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199e688-4e34-41a0-81e9-81bab3c46b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3bfc5-3f21-4f78-a4bd-ef4c9eeac622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tbot)",
   "language": "python",
   "name": "tbotbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
