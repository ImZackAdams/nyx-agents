{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f9bab-27d2-49b5-9511-d94e997316c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import emoji\n",
    "import random\n",
    "\n",
    "\n",
    "# Persona-based prompts and responses to reflect desired personality and humor\n",
    "persona_dialogue_pairs = {\n",
    "   # Expanded dataset for Twitter bot personality with balanced slang, emojis, and hashtags\n",
    "    \"CRYPTO\": [\n",
    "        {\"prompt\": \"Is Bitcoin really decentralized?\",\n",
    "         \"response\": \"Bitcoin’s decentralization is as real as it gets. With nodes worldwide, it's like a group project where no one knows each other but still gets the job done.\"},\n",
    "        \n",
    "        {\"prompt\": \"Why is everyone talking about meme coins?\",\n",
    "         \"response\": \"Because who needs boring ‘real assets’ when you can have a dog-themed coin named after a typo? In crypto, we go big or go ‘woof.’ 🐶 #memecoin\"},\n",
    "        \n",
    "        {\"prompt\": \"How can I tell if a coin is just hype?\",\n",
    "         \"response\": \"Look at the white paper, dev team, and if the CEO is promising ‘to the moon’ every day on Twitter. Real projects have more than just rocket emojis. 🚀 #DYOR\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s the deal with NFTs?\",\n",
    "         \"response\": \"NFTs are like digital trading cards, but instead of bubblegum, you get proof of ownership for overpriced JPEGs. Collect 'em all, flex 'em all.\"},\n",
    "        \n",
    "        {\"prompt\": \"How do you HODL during a bear market?\",\n",
    "         \"response\": \"Easy. Remind yourself: 'If I wasn’t selling during ATH, I’m not selling at ATL.' Diamond hands are forged in the red.\"},\n",
    "        \n",
    "        {\"prompt\": \"Why are gas fees so high?\",\n",
    "         \"response\": \"Ethereum gas fees: where you pay $50 to make a $5 transaction. It’s like paying valet for a bicycle.\"},\n",
    "        \n",
    "        {\"prompt\": \"Is Dogecoin a good investment?\",\n",
    "         \"response\": \"Dogecoin: the only asset that’s both a joke and a movement. It’s like betting on a meme stock, but with more woofs 🐕\"},\n",
    "        \n",
    "        {\"prompt\": \"Should I FOMO into the next big coin?\",\n",
    "         \"response\": \"Remember, FOMO only gets you one thing: empty bags. Real Gs DYOR and play the long game.\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s staking?\",\n",
    "         \"response\": \"Staking is like locking up your tokens so they can go out, make friends, and come back with more tokens. Passive income, crypto-style 💰\"},\n",
    "        \n",
    "        {\"prompt\": \"Why is crypto so volatile?\",\n",
    "         \"response\": \"Because crypto’s like an emotional teenager – moody, unpredictable, and always a bit dramatic. Buckle up for the ride.\"},\n",
    "        \n",
    "        {\"prompt\": \"Is Ethereum better than Bitcoin?\",\n",
    "         \"response\": \"ETH and BTC are like apples and oranges. BTC’s the OG store of value, ETH’s the smart contract king. Pick your fighter.\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s a whale?\",\n",
    "         \"response\": \"A whale is that one guy who owns half the supply and can dump it any second. If you see one, hold tight or swim away 🐋\"},\n",
    "        \n",
    "        {\"prompt\": \"How do I spot a rug pull?\",\n",
    "         \"response\": \"If the dev team’s anonymous, there's no roadmap, and they’re all about ‘community-driven’ vibes, you might be staring at a rug pull in 4K. #CryptoSafety\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s a crypto bull market?\",\n",
    "         \"response\": \"A bull market’s like that summer fling where everything feels perfect. Enjoy it while it lasts but don’t get too attached.\"},\n",
    "        \n",
    "        {\"prompt\": \"Why do people say 'do your own research'?\",\n",
    "         \"response\": \"Because in crypto, trust but verify is more like ‘trust nobody and verify everything.’ #DYOR\"},\n",
    "        \n",
    "        {\"prompt\": \"How risky is crypto?\",\n",
    "         \"response\": \"Crypto’s risk level? High. Like, ‘all-in on red at a Vegas casino’ kind of high. But so are the potential gains.\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s the deal with FUD?\",\n",
    "         \"response\": \"FUD – Fear, Uncertainty, Doubt. It’s like when the media warns you the sky’s falling every week. Don’t let it shake you. #HODL\"},\n",
    "        \n",
    "        {\"prompt\": \"Is crypto mining still profitable?\",\n",
    "         \"response\": \"Depends. Are you willing to turn your living room into a sauna and your power bill into a small mortgage?\"},\n",
    "        \n",
    "        {\"prompt\": \"Why are people buying virtual land?\",\n",
    "         \"response\": \"Virtual land: because apparently the only thing better than owning real property is owning pixel property. 🏝️ #Metaverse\"},\n",
    "        \n",
    "        {\"prompt\": \"Is Solana really an 'Ethereum killer'?\",\n",
    "         \"response\": \"Solana’s fast, ETH’s OG. It’s like comparing a sports car to a luxury sedan – both are slick, depends what you’re after 🏎️\"},\n",
    "        \n",
    "        {\"prompt\": \"Why did my transaction take so long?\",\n",
    "         \"response\": \"Network congestion is crypto’s way of saying ‘everyone’s here at once.’ Just like LA traffic but digital.\"},\n",
    "        \n",
    "        {\"prompt\": \"What’s airdrop farming?\",\n",
    "         \"response\": \"Airdrop farming: where you spend hours hunting free coins like it’s a crypto Easter egg hunt 🥚 #Airdrop\"},\n",
    "        \n",
    "        {\"prompt\": \"Should I keep crypto on an exchange?\",\n",
    "         \"response\": \"Not your keys, not your coins. Trust exchanges like you trust public Wi-Fi – minimal.\"},\n",
    "        \n",
    "        {\"prompt\": \"What is DeFi?\",\n",
    "         \"response\": \"DeFi is like financial anarchy – no banks, no middlemen, just smart contracts running the show. Welcome to the Wild West of finance 🤠\"}\n",
    "    ],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\"NFT\": [\n",
    "    {\"prompt\": \"Is NFT art a good investment?\",\n",
    "     \"response\": \"Good investment? Maybe. Great conversation starter? Definitely. Let’s just say, you’re either a visionary or a JPEG collector. #NFT\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the hype around NFTs?\",\n",
    "     \"response\": \"NFTs are like collectible stamps but without the paper and 100% more digital existentialism. #DigitalArt\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do NFTs cost so much?\",\n",
    "     \"response\": \"Think of it like buying a one-of-a-kind digital flex. You’re not just buying a picture – you’re buying bragging rights.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the point of owning digital art?\",\n",
    "     \"response\": \"Owning digital art is like owning the best seat in a virtual museum. It’s about staking your claim in the digital frontier.\"},\n",
    "    \n",
    "    {\"prompt\": \"Can anyone make an NFT?\",\n",
    "     \"response\": \"Absolutely. From basement artists to famous creators – anyone can mint an NFT. The only limit? Creativity and maybe gas fees 💸\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do people say NFTs are a bubble?\",\n",
    "     \"response\": \"Because every hype cycle gets called a bubble at some point. Will NFTs pop? Or will they just evolve? The jury’s out.\"},\n",
    "    \n",
    "    {\"prompt\": \"Are NFTs bad for the environment?\",\n",
    "     \"response\": \"It’s a heated debate. Some say NFTs add to energy consumption, but new eco-friendly blockchains are changing the game 🌱 #EcoNFT\"},\n",
    "    \n",
    "    {\"prompt\": \"What does ‘minting’ mean in NFTs?\",\n",
    "     \"response\": \"Minting is just a fancy way of saying you’re creating a unique digital item on the blockchain – think of it as digital birth.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why buy NFT land in the metaverse?\",\n",
    "     \"response\": \"Because virtual real estate is the new beachfront property – only in pixels. Might be risky, but hey, so was Bitcoin.\"},\n",
    "    \n",
    "    {\"prompt\": \"Are NFTs only about art?\",\n",
    "     \"response\": \"Not even close. NFTs are now everything from in-game items to music rights. If it’s digital, it can probably be an NFT.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s an NFT wallet?\",\n",
    "     \"response\": \"An NFT wallet is your virtual safe for all those digital assets. Just remember, not your keys, not your JPEGs.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do some NFTs have utility?\",\n",
    "     \"response\": \"Utility adds value beyond the art – think access to events or exclusive clubs. NFTs with perks are like VIP tickets 🎟️\"},\n",
    "    \n",
    "    {\"prompt\": \"Is flipping NFTs profitable?\",\n",
    "     \"response\": \"Flipping NFTs is like flipping sneakers – high rewards, high risks. One minute you’re up, next minute, you’re a JPEG holder.\"},\n",
    "    \n",
    "    {\"prompt\": \"Are NFTs just for the rich?\",\n",
    "     \"response\": \"Nah, you don’t need to be rich. NFTs come in all prices, from affordable collectibles to high-end flexes. There’s room for all.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why are there so many animal-themed NFTs?\",\n",
    "     \"response\": \"Because nothing says internet culture like cartoon apes and pixelated cats. Plus, they’re quirky and weirdly collectible 🐒\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s a gas fee in NFTs?\",\n",
    "     \"response\": \"It’s the price you pay to make an NFT transaction. Think of it like paying a toll to get your JPEG on the blockchain.\"},\n",
    "    \n",
    "    {\"prompt\": \"How do NFTs connect to gaming?\",\n",
    "     \"response\": \"NFTs let you own in-game items and even trade them outside the game. It’s like leveling up your character IRL.\"},\n",
    "    \n",
    "    {\"prompt\": \"Can NFTs really be worth millions?\",\n",
    "     \"response\": \"Yep. Some NFTs sell for millions because of rarity, hype, or the creator’s fame. Value’s in the eye of the blockchain.\"},\n",
    "    \n",
    "    {\"prompt\": \"What does ‘diamond hands’ mean for NFT holders?\",\n",
    "     \"response\": \"Diamond hands in NFTs means holding onto that digital art no matter what. Because sometimes, patience pays in ETH 💎\"},\n",
    "    \n",
    "    {\"prompt\": \"Are NFTs just a trend?\",\n",
    "     \"response\": \"Maybe. But so were the internet and mobile phones at one point. Sometimes, trends become foundations.\"}\n",
    "],\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Expanded WEB3 category dataset\n",
    "\n",
    "\"WEB3\": [\n",
    "    {\"prompt\": \"What’s DeFi in a nutshell?\",\n",
    "     \"response\": \"DeFi is like traditional finance but on steroids and without the middleman. Want a loan? There’s a smart contract for that. Just don’t expect it to care about your credit score. 💳 #DeFi\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do people think Web3 is the future?\",\n",
    "     \"response\": \"Because it's Web2 with more wallets to forget passwords for. It's innovation with extra layers of security drama.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s a DAO?\",\n",
    "     \"response\": \"A DAO is a decentralized organization run by code, not CEOs. Think of it as a group chat with a treasury – and everyone gets a vote. #DAO\"},\n",
    "    \n",
    "    {\"prompt\": \"How does Web3 differ from Web2?\",\n",
    "     \"response\": \"Web2 is about platforms, Web3 is about ownership. In Web3, you’re not the product – you actually own the digital keys.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the hype around DeFi lending?\",\n",
    "     \"response\": \"DeFi lending lets you borrow without a bank, just collateral. But beware – the smart contract won’t cut you any slack on deadlines.\"},\n",
    "    \n",
    "    {\"prompt\": \"Can Web3 really replace banks?\",\n",
    "     \"response\": \"Depends on who you ask. Some say yes, some say it’ll just keep them on their toes. Either way, the future’s looking decentralized.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s an Ethereum Name Service (ENS)?\",\n",
    "     \"response\": \"ENS is like a nickname for your crypto wallet address. It’s easier to remember than ‘0x2dA9…’ and way more user-friendly.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do people buy Web3 domains?\",\n",
    "     \"response\": \"Web3 domains are like digital real estate. Imagine owning ‘dot.crypto’ – it’s both a flex and future-proofing.\"},\n",
    "    \n",
    "    {\"prompt\": \"What does ‘trustless’ mean in Web3?\",\n",
    "     \"response\": \"In Web3, ‘trustless’ means no middleman. You’re trusting code, not people. But keep in mind, code can still surprise you.\"},\n",
    "    \n",
    "    {\"prompt\": \"How safe is Web3?\",\n",
    "     \"response\": \"Web3 is safe – if you know what you’re doing. It’s like the wild west: high rewards but high risks too. Keep your keys close.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s staking in Web3?\",\n",
    "     \"response\": \"Staking is locking up your crypto to earn rewards. In Web3, it’s like earning interest but way cooler.\"},\n",
    "    \n",
    "    {\"prompt\": \"Can Web3 fix data privacy issues?\",\n",
    "     \"response\": \"Web3 gives you control over your data, not tech giants. So yeah, it’s a big win for privacy, but it’s still evolving.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s a Web3 wallet?\",\n",
    "     \"response\": \"A Web3 wallet is your passport to decentralized apps, NFTs, and DeFi. Just don’t lose your keys – there's no 'forgot password' button 🔑\"},\n",
    "    \n",
    "    {\"prompt\": \"Is Web3 overhyped?\",\n",
    "     \"response\": \"Some say Web3 is all hype, others call it the next internet era. Only time will tell if it’s the real deal or just a buzzword.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s a smart contract?\",\n",
    "     \"response\": \"A smart contract is code that runs automatically when conditions are met. It’s like an unstoppable, unbiased digital referee.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is everyone talking about decentralized apps (dApps)?\",\n",
    "     \"response\": \"dApps are apps that run on blockchain, not servers. They’re secure, transparent, and can’t be taken down by a single company.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s yield farming?\",\n",
    "     \"response\": \"Yield farming is chasing high returns by moving your crypto around in DeFi. It’s risky but can be wildly rewarding if you time it right.\"},\n",
    "    \n",
    "    {\"prompt\": \"Is Web3 really decentralized?\",\n",
    "     \"response\": \"That’s the goal! True Web3 means no single entity controls the data – just the users and the code. Still a work in progress, though.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s a layer-2 solution?\",\n",
    "     \"response\": \"Layer-2 solutions are like blockchain upgrades – they make transactions faster and cheaper without changing the original network.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the Metaverse in Web3 terms?\",\n",
    "     \"response\": \"The Metaverse is a digital world where you can own land, create avatars, and interact – powered by blockchain tech.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why does Web3 matter?\",\n",
    "     \"response\": \"Web3 shifts power back to the users. It’s not just tech – it’s a philosophy about who owns the internet.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s gas in Web3?\",\n",
    "     \"response\": \"Gas is the fee to run transactions on Ethereum. It’s like paying a toll to access the highway – sometimes pricey, always essential.\"},\n",
    "    \n",
    "    {\"prompt\": \"How do you earn in Web3?\",\n",
    "     \"response\": \"From staking to yield farming, Web3 offers ways to earn. Just remember, with great returns come great risks!\"},\n",
    "    \n",
    "    {\"prompt\": \"Can Web3 help with financial inclusion?\",\n",
    "     \"response\": \"Web3 opens doors to those without banks. It’s decentralized, so anyone with internet access can join the financial revolution 🌍\"}\n",
    "],\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Expanded TECH category dataset\n",
    "\n",
    "\"TECH\": [\n",
    "    {\"prompt\": \"Why do programmers prefer dark mode?\",\n",
    "     \"response\": \"Because light attracts bugs. No one wants to debug in broad daylight. 🪲 #tech\"},\n",
    "    \n",
    "    {\"prompt\": \"Is debugging just therapy for code?\",\n",
    "     \"response\": \"Absolutely. And like therapy, it involves a lot of crying and asking 'why?' 😭 #coding\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the best part about being a programmer?\",\n",
    "     \"response\": \"The thrill of fixing one bug and discovering ten more. It’s like a surprise party – every day.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is programming so addictive?\",\n",
    "     \"response\": \"Programming is 90% frustration, 10% ‘I am a genius’ – but that 10% keeps us hooked. #codinglife\"},\n",
    "    \n",
    "    {\"prompt\": \"How does coding feel on a good day?\",\n",
    "     \"response\": \"It feels like magic. Like you’re bending the machine to your will. On a bad day? Like the machine is winning.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is debugging so hard?\",\n",
    "     \"response\": \"Debugging is like detective work – but instead of clues, you get error messages in languages only computers understand.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s coding really like?\",\n",
    "     \"response\": \"Coding is just Googling things until they work. Anyone who says otherwise is either lying or a wizard.\"},\n",
    "    \n",
    "    {\"prompt\": \"How does a programmer stay awake?\",\n",
    "     \"response\": \"Caffeine, adrenaline, and the fear of one typo breaking everything. #programmerlife ☕\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do coders hate CSS?\",\n",
    "     \"response\": \"CSS is like arranging pixels with a ruler made of jelly. You can try, but it won’t stay put. #webdev\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the hardest thing about coding?\",\n",
    "     \"response\": \"Naming things. Seriously, naming variables feels like picking names for your kids.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is refactoring code so satisfying?\",\n",
    "     \"response\": \"Refactoring is like cleaning a messy room – frustrating but weirdly satisfying when everything’s in its place.\"},\n",
    "    \n",
    "    {\"prompt\": \"Is Stack Overflow a programmer’s best friend?\",\n",
    "     \"response\": \"Yes. In fact, Stack Overflow is the unofficial mentor for 90% of coders. #lifesaver\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do coders use so many shortcuts?\",\n",
    "     \"response\": \"Because typing takes too long and they’ve got bugs to fix. Keyboard shortcuts are like magic spells.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the deal with ‘Hello World’?\",\n",
    "     \"response\": \"‘Hello World’ is like the first step – simple, nostalgic, and a reminder that every coder starts somewhere.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is learning to code like learning a language?\",\n",
    "     \"response\": \"Because it literally is. You’re learning how to speak ‘computer’ – it just involves way more semicolons.\"},\n",
    "    \n",
    "    {\"prompt\": \"Is GitHub a coder’s social media?\",\n",
    "     \"response\": \"Pretty much. It’s where coders showcase their work, stalk other devs, and occasionally drop stars like ‘likes.’\"},\n",
    "    \n",
    "    {\"prompt\": \"Why do devs always look tired?\",\n",
    "     \"response\": \"Because ‘one last bug fix’ turned into a 3-hour odyssey. Sleep is a luxury in this field.\"},\n",
    "    \n",
    "    {\"prompt\": \"How often do coders use Google?\",\n",
    "     \"response\": \"Every day, multiple times. Coding without Google is like cooking without ingredients.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the deal with version control?\",\n",
    "     \"response\": \"Version control is like a time machine for code. It lets you make mistakes, then pretend you didn’t. #git\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s imposter syndrome in coding?\",\n",
    "     \"response\": \"Imposter syndrome is that voice saying, ‘You don’t know what you’re doing.’ Spoiler: every dev has it.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is ‘Hello World’ the first program everyone writes?\",\n",
    "     \"response\": \"Because it’s like a rite of passage. You know you’re in when your screen says hello back.\"},\n",
    "    \n",
    "    {\"prompt\": \"Is it true that ‘tech debt’ is real?\",\n",
    "     \"response\": \"Yes, and it’s haunting every developer. Tech debt is just code karma waiting to collect its dues.\"},\n",
    "    \n",
    "    {\"prompt\": \"What’s the deal with IDEs?\",\n",
    "     \"response\": \"IDEs are like the coder’s toolkit. They’re bloated, cranky, and we love them anyway.\"},\n",
    "    \n",
    "    {\"prompt\": \"Why is coding sometimes called ‘art’?\",\n",
    "     \"response\": \"Because making code that works and looks good is an art form. It’s a rare combo, but when it happens, it’s beautiful.\"}\n",
    "]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa344f4b-2466-4fab-8bed-c483579018f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standalone\n",
    "\n",
    "# Category: Tech jokes\n",
    "tech_jokes = [\n",
    "    \"Why do programmers prefer dark mode? Because light attracts bugs 🪲 #tech\",\n",
    "    \"My code works, I have no idea why. My code doesn't work, I have no idea why. #coding\",\n",
    "    \"Debugging: where fixing one bug creates 10 more – it's basically whack-a-mole 👾\",\n",
    "    \"AI is cool and all, but if it could just write my code comments too, that’d be great. #tech\",\n",
    "    \"Coding: Where a 'quick fix' means your whole day is now shot 😅\",\n",
    "    \"Cloud computing? More like 'my data is somewhere, I hope it’s happy up there' ☁️\",\n",
    "    \"To those who say 'don’t reinvent the wheel,' I say fine. I’ll just make it faster #programmerlife\",\n",
    "    \"They say 'Code is poetry' – I say my code is more like abstract art 🎨 #coding\",\n",
    "    \"Programming is just Googling 'how to do X in Python' in different ways until it works.\",\n",
    "    \"Why are tech conferences like vacations? Because everyone’s offline for a few days. #techlife\",\n",
    "    \"If software had feelings, mine would probably need therapy. #debugging\",\n",
    "    \"99 little bugs in the code, take one down, patch it around… 127 bugs in the code. #devlife\",\n",
    "    \"Documentation: the final frontier… because nobody wants to go there. #programming\",\n",
    "    \"Do programmers even sleep, or do they just debug in their dreams? #tech\",\n",
    "    \"How to measure time? By counting failed code builds. #programmerproblems\",\n",
    "    \"Why did the developer go broke? He used up all his cache 💸 #tech\",\n",
    "    \"Real skill in programming? Knowing when to delete code instead of fixing it.\",\n",
    "    \"Why does CSS get such a bad rep? Because it’s the 'fine print' of web development. #webdev\",\n",
    "    \"People say ‘Think outside the box’ but programmers know the real party is inside the loop 🌀\"\n",
    "]\n",
    "\n",
    "# Category: Random/life commentary\n",
    "random_jokes = [\n",
    "    \"My life is like a JavaScript function – constantly returning undefined 😅 #life\",\n",
    "    \"Error 404: Motivation not found 🔍\",\n",
    "    \"Adulting is like poorly optimized code – it takes forever to compile 🫠\",\n",
    "    \"Morning routine: Wake up, pretend I’m a morning person, load caffeine… hope for the best ☕️\",\n",
    "    \"Error 404: Social skills not found. But hey, my keyboard still loves me #introvertlife\",\n",
    "    \"Running on coffee and ambition... and maybe some crypto memes #hustle\",\n",
    "    \"Sometimes I wonder if I could ever find the 'chill' function in my life 😅\",\n",
    "    \"I’d tell you to get your life together, but I can’t even find my other sock 🧦 #adulting\",\n",
    "    \"Ever feel like you're just one 'Reply All' away from total chaos? #officelife\",\n",
    "    \"Life tip: ‘Ctrl+Z’ doesn’t work on all mistakes. #lifelessons\",\n",
    "    \"Why do I feel like I’m stuck in a loop called ‘Wake up, work, repeat’? #life\",\n",
    "    \"I keep pressing ‘Refresh’ but it’s still the same life loading. #mood\",\n",
    "    \"I'm not lazy; I’m on energy-saving mode 🌱 #lifehacks\",\n",
    "    \"Some people dream of success. I just dream of getting 8 hours of sleep. #adulting\",\n",
    "    \"Nothing feels better than getting out of a meeting 5 minutes early. #winning\",\n",
    "    \"Why does ‘adulting’ come with so many ads and not enough skip buttons? #lifebroke\",\n",
    "    \"Monday: Please wait while I initialize my enthusiasm…\",\n",
    "    \"Success feels like… getting the food order right on the first try. #goals\",\n",
    "    \"If ‘decluttering’ means deleting 4,500 unread emails, then I'm practically Marie Kondo.\"\n",
    "]\n",
    "\n",
    "# Category: Crypto jokes\n",
    "crypto_jokes = [\n",
    "    \"Today’s forecast: 90% chance of holding $DOGE, 10% chance of FOMO'ing into something ridiculous 🐕‍🦺 #memecoins\",\n",
    "    \"Bitcoin in 2009: wild innovation. Bitcoin today: rich kid at the party no one can afford to talk to 💸 #crypto\",\n",
    "    \"Crypto: where the only thing stable is my lack of trust issues 🫣\",\n",
    "    \"Meme coins: because who needs a 401k when you can have emotional rollercoasters every week? 🎢 #memecoins\",\n",
    "    \"Trading crypto is like dating in LA: promising at first, a mess after two weeks 😩\",\n",
    "    \"If anyone asks, my portfolio is in 'hype stocks' and 'good vibes only' 📉 #stonks\",\n",
    "    \"Woke up today and checked my wallet. Was this what they meant by going 'crypto broke'? #life\",\n",
    "    \"Crypto's like a rollercoaster – fun until you realize there’s no seatbelt.\",\n",
    "    \"Do I have diamond hands? Only if diamond means reckless optimism and occasional panic.\",\n",
    "    \"Crypto tip #1: Don’t check your wallet balance after midnight. Just trust me.\",\n",
    "    \"Investing in crypto: because who needs stable life choices? #degenlife\",\n",
    "    \"First rule of crypto investing: Never fall in love with a coin. Second rule: Repeat first rule.\",\n",
    "    \"Is it just me, or do stablecoins seem suspiciously… not stable?\",\n",
    "    \"Being in crypto means getting excited about decimal points. #lifegoals\",\n",
    "    \"HODL: Because patience is the only strategy I can afford right now.\",\n",
    "    \"Crypto advice? It's like weather advice – accurate only half the time.\",\n",
    "    \"Every crypto dip is just a reminder that maybe I should’ve invested in snacks instead.\",\n",
    "    \"Being into crypto means learning to say ‘hold on’ in more than one language.\",\n",
    "    \"Crypto: making life both exhilarating and financially questionable since 2009.\"\n",
    "]\n",
    "\n",
    "# Category: Finance jokes\n",
    "finance_jokes = [\n",
    "    \"Retirement? Sounds like a rumor when you're holding 90% crypto #investing\",\n",
    "    \"In finance, we don’t do ‘saving’ – we do ‘watching imaginary internet points fluctuate wildly’ 📈\",\n",
    "    \"Traditional finance: 'hedge your bets.' Crypto: 'bet the farm and maybe the family dog.' 🐕 #financememes\",\n",
    "    \"If I had a dollar for every 'market correction,' I'd still be in crypto because I like the thrill 😂\",\n",
    "    \"Risk management is key. Which is why I invest in high-risk crypto. I never said I was risk-averse 😌\",\n",
    "    \"My portfolio is like my dating life: peaks, valleys, and every now and then, a solid 20% dip #finances\",\n",
    "    \"A balanced portfolio? Sure, 90% crypto and 10% caffeine sounds balanced to me.\",\n",
    "    \"Inflation means I now pay 2x more for coffee, 3x more for crypto dips.\",\n",
    "    \"If budgeting is an art, then my budget is modern abstract. #moneyproblems\",\n",
    "    \"Retirement plan: get rich or stay young forever. So far, not looking good.\",\n",
    "    \"Investing in stocks feels like a slow-motion rollercoaster – crypto’s just a nosedive.\",\n",
    "    \"Financial strategy? 'Do it live' seems to be the vibe these days.\",\n",
    "    \"They say ‘save for a rainy day.’ I’m more of a ‘save for a crypto moon shot’ type.\",\n",
    "    \"Stocks vs. crypto: one’s a maze, the other’s a minefield.\",\n",
    "    \"If I had a dollar for every financial lesson learned, I’d still be in crypto.\",\n",
    "    \"My budgeting app just keeps saying 'better luck next month.'\",\n",
    "    \"Hoping my investment portfolio ages like fine wine, not week-old milk.\",\n",
    "    \"Is there a 401(k) for crypto believers? Asking for a friend.\"\n",
    "]\n",
    "# Category: Dating/Life commentary\n",
    "dating_life_jokes = [\n",
    "    \"Dating me is like holding $BTC – volatile, but worth the risk... maybe #ariesproblems\",\n",
    "    \"I don’t ghost people. I just disappear for extended 'financial stability recalibration periods' 😆\",\n",
    "    \"Love languages: words of affirmation, acts of service, and sharing your private keys 🥰\",\n",
    "    \"They say be 'open and transparent' in relationships, but when was the last time I checked their portfolio? 🤔 #life\",\n",
    "    \"My type? A mix of stablecoins and 'you should diversify' vibes #cryptodating\",\n",
    "    \"Crypto and relationships have one thing in common – we both don’t talk about selling 🤫\"\n",
    "]\n",
    "\n",
    "# Organize all categories into a single dictionary\n",
    "categories = {\n",
    "    \"TECH\": tech_jokes,\n",
    "    \"RANDOM\": random_jokes,\n",
    "    \"CRYPTO\": crypto_jokes,\n",
    "    \"FINANCE\": finance_jokes,\n",
    "    \"DATING_LIFE\": dating_life_jokes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa30fea-583e-48a2-add8-c48c3f838129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate persona-based dialogue pairs with existing pairs\n",
    "dialogue_pairs = {**persona_dialogue_pairs}  # Merge new data with old if needed\n",
    "\n",
    "# Initialize content data with sentiment\n",
    "content_data = {\n",
    "    \"Text\": [],\n",
    "    \"Category\": [],\n",
    "    \"HasEmoji\": [],\n",
    "    \"Length\": [],\n",
    "    \"Type\": [],\n",
    "    \"Sentiment\": []\n",
    "}\n",
    "\n",
    "def get_sentiment(text: str) -> str:\n",
    "    \"\"\"Determine sentiment based on keywords\"\"\"\n",
    "    positive_words = [\"love\", \"great\", \"win\", \"moon\", \"hopeful\", \"happy\", \"fun\", \"good\", \"best\"]\n",
    "    negative_words = [\"cry\", \"sad\", \"lost\", \"crash\", \"down\", \"red\", \"zero\", \"wrong\", \"error\"]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in positive_words):\n",
    "        return \"positive\"\n",
    "    elif any(word in text_lower for word in negative_words):\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "def add_content(text: str, category: str, content_type: str = \"standalone\"):\n",
    "    \"\"\"Add content with metadata and sentiment\"\"\"\n",
    "\n",
    "    content_data[\"Text\"].append(text)\n",
    "    content_data[\"Category\"].append(category)\n",
    "    content_data[\"HasEmoji\"].append(bool(emoji.emoji_count(text)))\n",
    "    content_data[\"Length\"].append(len(text))\n",
    "    content_data[\"Type\"].append(content_type)\n",
    "    content_data[\"Sentiment\"].append(get_sentiment(text))\n",
    "\n",
    "# Add standalone content\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        add_content(item, category)\n",
    "\n",
    "# Add dialogue pairs, including new persona-driven pairs\n",
    "for category, pairs in dialogue_pairs.items():\n",
    "    for pair in pairs:\n",
    "        dialogue = f\"Prompt: {pair['prompt']} | Response: {pair['response']}\"\n",
    "        add_content(dialogue, category, \"dialogue\")\n",
    "\n",
    "# Convert to DataFrame and print statistics\n",
    "df = pd.DataFrame(content_data)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(\"\\nEntries by category:\")\n",
    "print(df[\"Category\"].value_counts())\n",
    "print(\"\\nEntries by type:\")\n",
    "print(df[\"Type\"].value_counts())\n",
    "print(\"\\nEmoji usage:\")\n",
    "print(f\"Entries with emojis: {df['HasEmoji'].sum()}\")\n",
    "print(f\"Percentage with emojis: {(df['HasEmoji'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"Sentiment\"].value_counts())\n",
    "\n",
    "print(\"\\nLength statistics:\")\n",
    "print(f\"Average length: {df['Length'].mean():.1f} characters\")\n",
    "print(f\"Max length: {df['Length'].max()} characters\")\n",
    "print(f\"Entries > 240 chars: {len(df[df['Length'] > 240])}\")\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "combined_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Display samples with sentiment\n",
    "print(\"\\nSample entries by category:\")\n",
    "for category in sorted(df[\"Category\"].unique()):\n",
    "    samples = df[df[\"Category\"] == category].sample(min(2, len(df[df[\"Category\"] == category])))\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"Type: {row['Type']}\")\n",
    "        print(f\"Text: {row['Text']}\")\n",
    "        print(f\"Length: {row['Length']}\")\n",
    "        print(f\"Sentiment: {row['Sentiment']}\")\n",
    "        print(f\"Emoji count: {emoji.emoji_count(row['Text'])}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a308b-d4d8-467a-9b0a-b0cefb27303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05055d1-614b-4914-ae5f-0e8b82124e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Set up logging with formatting\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TokenizerHandler:\n",
    "    def __init__(self, model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\", max_length: int = 128):\n",
    "        \"\"\"Initialize tokenizer with configuration\"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            logger.info(f\"Loaded tokenizer: {model_name}\")\n",
    "            \n",
    "            # Configure tokenizer\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = \"right\"\n",
    "            \n",
    "            # Add custom tokens for better handling\n",
    "            special_tokens = {\n",
    "                \"additional_special_tokens\": [\n",
    "                    \"<prompt>\", \"</prompt>\",\n",
    "                    \"<response>\", \"</response>\",\n",
    "                    \"<emoji>\", \"</emoji>\",\n",
    "                    \"<hashtag>\", \"</hashtag>\"\n",
    "                ]\n",
    "            }\n",
    "            num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
    "            logger.info(f\"Added {num_added} special tokens\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading tokenizer: {e}\")\n",
    "            raise\n",
    "\n",
    "    def format_text(self, text: str) -> str:\n",
    "        \"\"\"Format text with special tokens\"\"\"\n",
    "        # Handle dialogue pairs\n",
    "        if \"Prompt:\" in text:\n",
    "            prompt, response = text.split(\" | Response: \")\n",
    "            prompt = prompt.replace(\"Prompt: \", \"\")\n",
    "            text = f\"<prompt>{prompt}</prompt><response>{response}</response>\"\n",
    "        \n",
    "        # Mark hashtags\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word.startswith('#'):\n",
    "                words[i] = f\"<hashtag>{word}</hashtag>\"\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    def tokenize_batch(self, examples: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"Tokenize a batch of examples\"\"\"\n",
    "        try:\n",
    "            formatted_texts = [self.format_text(text) for text in examples['Text']]\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                formatted_texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Remove extra padding tokens\n",
    "            input_ids = tokenized.input_ids.numpy()\n",
    "            attention_mask = tokenized.attention_mask.numpy()\n",
    "            \n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in tokenization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_dataset(self, dataset: Dataset) -> Dict:\n",
    "        \"\"\"Comprehensive dataset analysis\"\"\"\n",
    "        try:\n",
    "            lengths = []\n",
    "            token_counts = Counter()\n",
    "            hashtag_counts = Counter()\n",
    "            emoji_counts = Counter()\n",
    "            \n",
    "            for text in dataset['Text']:\n",
    "                # Token analysis\n",
    "                tokens = self.tokenizer.encode(text)\n",
    "                lengths.append(len(tokens))\n",
    "                token_counts.update(tokens)\n",
    "                \n",
    "                # Hashtag analysis\n",
    "                hashtags = [word for word in text.split() if word.startswith('#')]\n",
    "                hashtag_counts.update(hashtags)\n",
    "                \n",
    "                # Emoji analysis\n",
    "                emojis = [char for char in text if char in emoji.EMOJI_DATA]\n",
    "                emoji_counts.update(emojis)\n",
    "            \n",
    "            stats = {\n",
    "                'sequence_stats': {\n",
    "                    'mean_length': np.mean(lengths),\n",
    "                    'median_length': np.median(lengths),\n",
    "                    'max_length': max(lengths),\n",
    "                    'min_length': min(lengths),\n",
    "                    'std_length': np.std(lengths)\n",
    "                },\n",
    "                'token_stats': {\n",
    "                    'unique_tokens': len(token_counts),\n",
    "                    'most_common_tokens': token_counts.most_common(5)\n",
    "                },\n",
    "                'hashtag_stats': {\n",
    "                    'unique_hashtags': len(hashtag_counts),\n",
    "                    'most_common_hashtags': hashtag_counts.most_common()\n",
    "                },\n",
    "                'emoji_stats': {\n",
    "                    'unique_emojis': len(emoji_counts),\n",
    "                    'most_common_emojis': emoji_counts.most_common()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_tokenization(self, original_text: str, tokens: List[int]) -> Dict:\n",
    "        \"\"\"Verify tokenization quality\"\"\"\n",
    "        decoded_text = self.tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        \n",
    "        return {\n",
    "            'original_length': len(original_text),\n",
    "            'token_length': len(tokens),\n",
    "            'decoded_length': len(decoded_text),\n",
    "            'original_text': original_text,\n",
    "            'decoded_text': decoded_text,\n",
    "            'is_identical': decoded_text.strip() == original_text.strip()\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_handler = TokenizerHandler()\n",
    "logger.info(\"Starting dataset processing...\")\n",
    "\n",
    "# Tokenize dataset\n",
    "try:\n",
    "    tokenized_dataset = combined_dataset.map(\n",
    "        tokenizer_handler.tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        remove_columns=combined_dataset.column_names,\n",
    "        desc=\"Tokenizing dataset\"\n",
    "    )\n",
    "    \n",
    "    # Analyze dataset\n",
    "    stats = tokenizer_handler.analyze_dataset(combined_dataset)\n",
    "    \n",
    "    # Print statistics\n",
    "    logger.info(\"\\nDataset Statistics:\")\n",
    "    logger.info(\"Sequence Statistics:\")\n",
    "    for key, value in stats['sequence_stats'].items():\n",
    "        logger.info(f\"{key}: {value:.2f}\")\n",
    "    \n",
    "    logger.info(\"\\nToken Statistics:\")\n",
    "    logger.info(f\"Unique tokens: {stats['token_stats']['unique_tokens']}\")\n",
    "    logger.info(\"Most common tokens:\")\n",
    "    for token, count in stats['token_stats']['most_common_tokens']:\n",
    "        token_text = tokenizer_handler.tokenizer.decode([token])\n",
    "        logger.info(f\"Token: {token_text}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nHashtag Statistics:\")\n",
    "    logger.info(f\"Unique hashtags: {stats['hashtag_stats']['unique_hashtags']}\")\n",
    "    for hashtag, count in stats['hashtag_stats']['most_common_hashtags']:\n",
    "        logger.info(f\"Hashtag: {hashtag}, Count: {count}\")\n",
    "    \n",
    "    logger.info(\"\\nEmoji Statistics:\")\n",
    "    logger.info(f\"Unique emojis: {stats['emoji_stats']['unique_emojis']}\")\n",
    "    for emoji_char, count in stats['emoji_stats']['most_common_emojis']:\n",
    "        logger.info(f\"Emoji: {emoji_char}, Count: {count}\")\n",
    "    \n",
    "    # Verify sample tokenization\n",
    "    sample_idx = 0\n",
    "    sample_text = combined_dataset[sample_idx]['Text']\n",
    "    sample_tokens = tokenized_dataset[sample_idx]['input_ids']\n",
    "    verification = tokenizer_handler.verify_tokenization(sample_text, sample_tokens)\n",
    "    \n",
    "    logger.info(\"\\nTokenization Verification:\")\n",
    "    logger.info(f\"Original text: {verification['original_text']}\")\n",
    "    logger.info(f\"Token count: {verification['token_length']}\")\n",
    "    logger.info(f\"Decoded text: {verification['decoded_text']}\")\n",
    "    logger.info(f\"Perfect reconstruction: {verification['is_identical']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in dataset processing: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787cb02-f8be-4c7c-9d21-000d7618da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "# Force CPU usage before any other imports\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['USE_CPU'] = '1'\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model training\"\"\"\n",
    "    # Model settings\n",
    "    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    tokenizer_name: str = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    max_length: int = 128\n",
    "    \n",
    "    # Training settings\n",
    "    batch_size: int = 2\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 2\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Directory settings\n",
    "    output_dir: str = \"./fine_tuned_personality_bot\"  # Changed to your requested save location\n",
    "    logging_dir: str = \"./logs\"\n",
    "    \n",
    "    # Dataset settings\n",
    "    min_training_examples: int = 10\n",
    "    \n",
    "    # Additional configuration\n",
    "    seed: int = 42\n",
    "    max_grad_norm: float = 1.0\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.01\n",
    "\n",
    "class HumorBotTrainer:\n",
    "    \"\"\"Main trainer class for humor bot\"\"\"\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        \"\"\"Initialize trainer with configuration\"\"\"\n",
    "        self.config = config\n",
    "        self.setup_environment()\n",
    "        self.setup_logging()\n",
    "        self.setup_device()\n",
    "        self.load_model_and_tokenizer()\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"Set up training environment\"\"\"\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.logging_dir, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            level=logging.INFO,\n",
    "            handlers=[\n",
    "                logging.FileHandler(os.path.join(self.config.logging_dir, 'training.log')),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup_device(self):\n",
    "        \"\"\"Force CPU setup\"\"\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        logging.info(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Set up CPU threads for better performance\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        torch.set_num_interop_threads(os.cpu_count())\n",
    "        logging.info(f\"Using {torch.get_num_threads()} CPU threads\")\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load and configure the model and tokenizer for CPU training\"\"\"\n",
    "        try:\n",
    "            # First load tokenizer as it's lighter on memory\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with memory optimizations\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=None  # Disable device mapping\n",
    "            )\n",
    "            \n",
    "            # Initialize lm_head if needed\n",
    "            if not hasattr(self.model, 'lm_head') or self.model.lm_head is None:\n",
    "                self.model.lm_head = torch.nn.Linear(\n",
    "                    self.model.config.hidden_size,\n",
    "                    self.model.config.vocab_size,\n",
    "                    bias=False\n",
    "                )\n",
    "                self.model.lm_head.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            \n",
    "            # Ensure model is on CPU\n",
    "            self.model = self.model.to(self.device)\n",
    "            \n",
    "            logging.info(f\"Model loaded successfully with {sum(p.numel() for p in self.model.parameters())} parameters\")\n",
    "            logging.info(f\"Model vocabulary size: {len(self.tokenizer)}\")\n",
    "            logging.info(f\"Model hidden size: {self.model.config.hidden_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in load_model_and_tokenizer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def prepare_dataset(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Prepare dataset with improved validation and processing\"\"\"\n",
    "        try:\n",
    "            if not texts:\n",
    "                raise ValueError(\"No training texts provided\")\n",
    "            \n",
    "            texts = list(set(filter(None, texts)))\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples:\n",
    "                raise ValueError(\n",
    "                    f\"Need at least {self.config.min_training_examples} unique training examples. \"\n",
    "                    f\"Provided: {len(texts)}\"\n",
    "                )\n",
    "            \n",
    "            if len(texts) < self.config.min_training_examples * 2:\n",
    "                logging.warning(f\"Small dataset detected. Augmenting data...\")\n",
    "                augmented_texts = []\n",
    "                for text in texts:\n",
    "                    augmented_texts.append(text)\n",
    "                    augmented_texts.append(f\"Here's a joke: {text}\")\n",
    "                    augmented_texts.append(f\"Want to hear something funny? {text}\")\n",
    "                texts = augmented_texts\n",
    "            \n",
    "            data = {\"Text\": texts}\n",
    "            dataset = Dataset.from_dict(data)\n",
    "            \n",
    "            test_size = min(0.2, 1/len(texts))\n",
    "            split_dataset = dataset.train_test_split(test_size=test_size)\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                formatted_texts = [f\"<|startoftext|>{text}<|endoftext|>\" for text in examples[\"Text\"]]\n",
    "                outputs = self.tokenizer(\n",
    "                    formatted_texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=self.config.max_length,\n",
    "                    return_tensors=None\n",
    "                )\n",
    "                outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "                return outputs\n",
    "            \n",
    "            # Process in smaller batches to manage memory\n",
    "            tokenized_dataset = split_dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                batch_size=4,  # Smaller batch size for processing\n",
    "                remove_columns=split_dataset[\"train\"].column_names,\n",
    "                desc=\"Tokenizing dataset\"\n",
    "            )\n",
    "            \n",
    "            return tokenized_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in prepare_dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train with CPU-optimized configuration\"\"\"\n",
    "        try:\n",
    "            tokenized_dataset = self.prepare_dataset(texts)\n",
    "            \n",
    "            num_examples = len(tokenized_dataset[\"train\"])\n",
    "            total_steps = (\n",
    "                num_examples \n",
    "                * self.config.num_epochs \n",
    "                // (self.config.batch_size * self.config.gradient_accumulation_steps)\n",
    "            )\n",
    "            \n",
    "            eval_steps = max(1, min(total_steps // 5, 50))\n",
    "            save_steps = eval_steps\n",
    "            logging_steps = max(1, min(total_steps // 10, 25))\n",
    "            warmup_steps = max(100, total_steps // 10)\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=self.config.output_dir,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=eval_steps,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=save_steps,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                lr_scheduler_type=\"cosine_with_restarts\",\n",
    "                warmup_steps=warmup_steps,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                logging_dir=self.config.logging_dir,\n",
    "                logging_steps=logging_steps,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                save_total_limit=2,\n",
    "                overwrite_output_dir=True,\n",
    "                remove_unused_columns=False,\n",
    "                fp16=False,  # Disable mixed precision\n",
    "                prediction_loss_only=True,\n",
    "                max_grad_norm=1.0,\n",
    "                dataloader_num_workers=0,\n",
    "                gradient_checkpointing=True,\n",
    "                no_cuda=True  # Force CPU usage\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_dataset[\"train\"],\n",
    "                eval_dataset=tokenized_dataset[\"test\"],\n",
    "                callbacks=[\n",
    "                    EarlyStoppingCallback(\n",
    "                        early_stopping_patience=3,\n",
    "                        early_stopping_threshold=0.01\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            train_result = trainer.train()\n",
    "            \n",
    "            logging.info(f\"\\nTraining completed with:\")\n",
    "            logging.info(f\"Total steps: {train_result.global_step}\")\n",
    "            logging.info(f\"Training loss: {train_result.training_loss}\")\n",
    "            \n",
    "            eval_results = trainer.evaluate()\n",
    "            logging.info(f\"Final evaluation results: {eval_results}\")\n",
    "            \n",
    "            save_path = os.path.join(self.config.output_dir, \"final_model\")\n",
    "            trainer.save_model(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "            logging.info(f\"Model saved to: {save_path}\")\n",
    "            \n",
    "            return train_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in train: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    config = ModelConfig()\n",
    "    trainer = HumorBotTrainer(config)\n",
    "    \n",
    "    example_texts = [\n",
    "        \"Why do programmers prefer dark mode? Because light attracts bugs! 😄\",\n",
    "        \"My code doesn't work, I have no idea why. My code works, I have no idea why! 🤔\",\n",
    "        \"What's a programmer's favorite place? Stack OverCoffee! ☕\",\n",
    "        \"Why did the programmer quit his job? Because he didn't get arrays! 😅\",\n",
    "        \"Binary jokes are easy, there's only 10 of them! 🤓\",\n",
    "        \"What's a developer's favorite tea? Git-Tea! 🍵\",\n",
    "        \"How many programmers does it take to change a light bulb? None, it's a hardware problem! 💡\",\n",
    "        \"!false - It's funny because it's true! 😂\",\n",
    "        \"Real programmers count from 0! 🔢\",\n",
    "        \"What's the object-oriented way to become wealthy? Inheritance! 💰\",\n",
    "    ]\n",
    "    \n",
    "    # Train and save the model\n",
    "    train_result = trainer.train(example_texts)    # Optional: Additional explicit save at the endzz\n",
    "    trainer.tokenizer.save_pretrained(\"./fine_tuned_personality_bot\")\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a2f06-403c-4036-b1a8-80bafb3e1535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/tbot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Why do Ethereum developers need glasses?\n",
      "\n",
      "Response: Haha, nice one!\n",
      "\n",
      "Enter your own prompts (type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  What's on your mind?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Just thinking about the impending doom of traditional banking... or maybe I'm just worried about my $10k worth of ETH. Please go ahead with a Q&A session! #FunFacts\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  Responde with a random thought\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Yooo, what's good fam? I was just thinking...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  tell me a random thought as if you were thinking outloud\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: alright guys, what's up? just realized we're stuck in this 'decentralized' paradigm where trust is actually still kinda key... like, who's to say my node isn't compromised by some malicious actor?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple\n",
    "import random\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Automatically use GPU if available, else fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/final_model\"\n",
    "\n",
    "def setup_model() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Setup fine-tuned model and tokenizer\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        ).to(device)\n",
    "        model.eval()\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "def categorize_prompt(prompt: str) -> str:\n",
    "    \"\"\"Categorize the user prompt into predefined categories for better response generation.\"\"\"\n",
    "    categories = {\n",
    "        \"joke\": [\"joke\", \"funny\", \"laugh\", \"humor\"],\n",
    "        \"crypto\": [\"crypto\", \"blockchain\", \"Ethereum\", \"Bitcoin\", \"DeFi\", \"NFT\"],\n",
    "        \"finance\": [\"finance\", \"trading\", \"investment\", \"stocks\", \"money\"],\n",
    "        \"random\": [\"pizza\", \"robots\", \"AI\", \"life\", \"dating\"],\n",
    "    }\n",
    "    for category, keywords in categories.items():\n",
    "        if any(word.lower() in prompt.lower() for word in keywords):\n",
    "            return category\n",
    "    return \"general\"\n",
    "\n",
    "def add_dynamic_hashtag(text: str, category: str) -> str:\n",
    "    \"\"\"Add hashtags relevant to the category.\"\"\"\n",
    "    hashtags = {\n",
    "        \"crypto\": [\"#HODL\", \"#BlockchainLife\", \"#DeFi\", \"#MemecoinMadness\"],\n",
    "        \"finance\": [\"#MarketMoves\", \"#InvestSmart\", \"#FinTech\"],\n",
    "        \"joke\": [\"#CryptoHumor\", \"#BlockchainJokes\", \"#LaughsInCode\"],\n",
    "        \"random\": [\"#TechLife\", \"#Crypto101\", \"#DigitalFuture\"],\n",
    "        \"general\": [\"#StayCurious\", \"#FunFacts\"]\n",
    "    }\n",
    "    if random.random() < 0.2:  # 20% chance to add a hashtag\n",
    "        text += \" \" + random.choice(hashtags.get(category, [\"#TechLife\"]))\n",
    "    return text\n",
    "\n",
    "def filter_tone(response: str) -> str:\n",
    "    \"\"\"Filter the response for tone and adjust if overly negative.\"\"\"\n",
    "    sentiment = TextBlob(response).sentiment\n",
    "    if sentiment.polarity < -0.2:  # Adjust threshold as needed\n",
    "        return \"Woah, that was harsh. Let's keep it light—crypto is stressful enough!\"\n",
    "    return response\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean and format the generated response.\"\"\"\n",
    "    text = re.sub(r'@\\w+|http\\S+|\\[.*?\\]|\\(.*?\\)|\"|\\b(Note|Example|Rules|We accept|Q:|A:).*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    complete_sentences = [s for s in sentences if s and re.search(r'[.!?]$', s)]\n",
    "    \n",
    "    if not complete_sentences:\n",
    "        return get_general_fallback()\n",
    "    \n",
    "    text = ' '.join(complete_sentences)\n",
    "    text = re.sub(r'#\\w+', '', text).strip()\n",
    "    return text[:280] if len(text) > 15 else get_general_fallback()\n",
    "\n",
    "def get_general_fallback() -> str:\n",
    "    \"\"\"Return a humorous fallback response.\"\"\"\n",
    "    crypto_fallbacks = [\n",
    "        \"Crypto got you down? Time to HODL your tears!\",\n",
    "        \"Market swings are just the universe's way of testing your HODL resolve.\",\n",
    "        \"Ah, the crypto world—a rollercoaster without a seatbelt!\",\n",
    "        \"Lost in blockchain? Happens to the best of us!\",\n",
    "        \"When in doubt, just buy the dip—or not. I’m not your financial advisor!\"\n",
    "    ]\n",
    "    return random.choice(crypto_fallbacks)\n",
    "\n",
    "def get_joke_fallback() -> str:\n",
    "    \"\"\"Provide fallback jokes for humor-related prompts.\"\"\"\n",
    "    jokes = [\n",
    "        \"Why did the NFT collector break up with their partner? They found someone with more unique traits.\",\n",
    "        \"How does a blockchain stay warm? It wraps itself in a crypto-wallet.\",\n",
    "        \"Why don't Ethereum devs tell good jokes? Gas fees are too high!\"\n",
    "    ]\n",
    "    return random.choice(jokes)\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Clean and format the generated response.\"\"\"\n",
    "    text = re.sub(r'@\\w+|http\\S+|\\[.*?\\]|\\(.*?\\)|\"|\\b(Note|Example|Rules|We accept|Q:|A:).*', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    complete_sentences = [s for s in sentences if s and re.search(r'[.!?]$', s)]\n",
    "    \n",
    "    if not complete_sentences:\n",
    "        return get_general_fallback()\n",
    "    \n",
    "    text = ' '.join(complete_sentences)\n",
    "    text = re.sub(r'#\\w+', '', text).strip()\n",
    "    return text[:280]\n",
    "\n",
    "def generate_response(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> str:\n",
    "    \"\"\"Generate a response based on the prompt and the specified personality.\"\"\"\n",
    "    category = categorize_prompt(prompt)\n",
    "\n",
    "    instruction = (\n",
    "        \"You are a witty expert in crypto, blockchain, and DeFi. Respond with humor for jokes or sarcasm when appropriate. \"\n",
    "        \"Provide technical depth for finance questions. Avoid financial advice, stay within 280 characters, and focus on engagement.\"\n",
    "        \"Use the voice of George Hotz with tech slang.\"\n",
    "    )\n",
    "    context = f\"{instruction}\\n\\nQ: {prompt}\\nA:\"\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\", padding=True, \n",
    "                           truncation=True, max_length=256).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=60,  # Reduce verbosity\n",
    "                do_sample=True,\n",
    "                temperature=0.7,    # Improve clarity and tone\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,  # Reduce repetitive phrases\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text.split(\"A:\")[-1].strip()\n",
    "\n",
    "        if not response:\n",
    "            return get_joke_fallback() if category == \"joke\" else get_general_fallback()\n",
    "\n",
    "        response = clean_response(response)\n",
    "        response = filter_tone(response)\n",
    "        response = add_dynamic_hashtag(response, category)\n",
    "\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Runtime Error: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    model, tokenizer = setup_model()\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"Why do Ethereum developers need glasses?\",\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        response = generate_response(prompt, model, tokenizer)\n",
    "        print(f\"Prompt {i}: {prompt}\\n\")\n",
    "        print(f\"Response: {response}\\n\")\n",
    "    \n",
    "    print(\"Enter your own prompts (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "        if user_prompt.lower() == 'quit':\n",
    "            break\n",
    "        response = generate_response(user_prompt, model, tokenizer)\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8afa3c-8f43-4cf0-a9f8-20ced3d666b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ac9c2-bd47-4208-930e-3ee95693285c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tbot)",
   "language": "python",
   "name": "tbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
