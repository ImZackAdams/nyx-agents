{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57accfa4-848b-4cc4-b08c-1ca54ab6188e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ca7d66-07e0-4e74-8c6c-2c0fa5fa4f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athena/anaconda3/envs/athena/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-21 11:39:32,373 - INFO - Setting up model from ./fine_tuned_personality_bot/\n",
      "2024-11-21 11:39:32,952 - INFO - Loading model...\n",
      "2024-11-21 11:39:33,660 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:16<00:00,  5.53s/it]\n",
      "2024-11-21 11:39:50,353 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "2024-11-21 11:39:50,354 - INFO - Model setup completed successfully\n",
      "2024-11-21 11:39:50,355 - INFO - Bot initialized successfully\n",
      "2024-11-21 11:39:50,355 - INFO - Entering interactive mode...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Interactive Mode ===\n",
      "Enter your prompts (type 'quit' to exit, or 'post' to tweet a generated response):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  Make a random thought as if you were thinking outload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:40:05,656 - INFO - User input: Make a random thought as if you were thinking outload\n",
      "2024-11-21 11:40:05,658 - INFO - Before inference:\n",
      "2024-11-21 11:40:06,666 - INFO - CPU Usage: 10.6% | RAM Usage: 27.9%\n",
      "2024-11-21 11:40:06,667 - INFO - GPU Memory Usage: 4784.26025390625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:40:06,716 - INFO - During inference:\n",
      "2024-11-21 11:40:07,719 - INFO - CPU Usage: 28.3% | RAM Usage: 27.9%\n",
      "2024-11-21 11:40:07,721 - INFO - GPU Memory Usage: 4784.26416015625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:40:33,798 - INFO - After inference and enhancements:\n",
      "2024-11-21 11:40:34,801 - INFO - CPU Usage: 4.8% | RAM Usage: 30.4%\n",
      "2024-11-21 11:40:34,803 - INFO - GPU Memory Usage: 4792.39208984375 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:40:34,804 - INFO - Generated response: Just realized Ive been treating my cryptocurrency portfolio management skills kinda low-key... Like keeping pizza delivery guy passwords for all accounts \"just because\". Anyone else do this too?! 😱🍕#cryptomanagementprobs (Note - no response required).\n",
      "2024-11-21 11:40:34,806 - INFO - Response generated in 29.15 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Just realized Ive been treating my cryptocurrency portfolio management skills kinda low-key... Like keeping pizza delivery guy passwords for all accounts \"just because\". Anyone else do this too?! 😱🍕#cryptomanagementprobs (Note - no response required).\n",
      "Runtime: 29.15 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to post this response to Twitter? (yes/no):  no\n",
      "\n",
      "Your prompt:  Make a random thought as if you were thinking outload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:41:02,475 - INFO - User input: Make a random thought as if you were thinking outload\n",
      "2024-11-21 11:41:02,476 - INFO - Before inference:\n",
      "2024-11-21 11:41:03,481 - INFO - CPU Usage: 18.3% | RAM Usage: 30.3%\n",
      "2024-11-21 11:41:03,482 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:41:03,509 - INFO - During inference:\n",
      "2024-11-21 11:41:04,511 - INFO - CPU Usage: 32.3% | RAM Usage: 30.3%\n",
      "2024-11-21 11:41:04,513 - INFO - GPU Memory Usage: 4792.38916015625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:41:29,613 - INFO - After inference and enhancements:\n",
      "2024-11-21 11:41:30,616 - INFO - CPU Usage: 4.5% | RAM Usage: 30.4%\n",
      "2024-11-21 11:41:30,618 - INFO - GPU Memory Usage: 4792.39208984375 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:41:30,620 - INFO - Generated response: Wait...if Ethereum 2.x has proof-of-stake consensus mechanism & Im already locked into my validator node for ETC mainnet what does this mean?! Do all validators get equal treatment under the law now?? Asking myself because apparently no one else cares about actual technical details 😒💸. 🌙\n",
      "2024-11-21 11:41:30,622 - INFO - Response generated in 28.15 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Wait...if Ethereum 2.x has proof-of-stake consensus mechanism & Im already locked into my validator node for ETC mainnet what does this mean?! Do all validators get equal treatment under the law now?? Asking myself because apparently no one else cares about actual technical detai\n",
      "Runtime: 28.15 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to post this response to Twitter? (yes/no):  no\n",
      "\n",
      "Your prompt:  Make a random thought as if you were thinking outload, keep it short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:42:01,310 - INFO - User input: Make a random thought as if you were thinking outload, keep it short\n",
      "2024-11-21 11:42:01,312 - INFO - Before inference:\n",
      "2024-11-21 11:42:02,316 - INFO - CPU Usage: 8.6% | RAM Usage: 30.4%\n",
      "2024-11-21 11:42:02,318 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:42:02,340 - INFO - During inference:\n",
      "2024-11-21 11:42:03,343 - INFO - CPU Usage: 13.8% | RAM Usage: 30.4%\n",
      "2024-11-21 11:42:03,344 - INFO - GPU Memory Usage: 4792.39013671875 MB | GPU Utilization: 3%\n",
      "2024-11-21 11:42:27,921 - INFO - After inference and enhancements:\n",
      "2024-11-21 11:42:28,923 - INFO - CPU Usage: 4.0% | RAM Usage: 30.2%\n",
      "2024-11-21 11:42:28,925 - INFO - GPU Memory Usage: 4792.39306640625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:42:28,926 - INFO - Generated response: Just realized I've been paying for Twitter blue because'security'... meanwhile my wallet has security measures so good they're literally making me cry 😭💸.\n",
      "2024-11-21 11:42:28,930 - INFO - Response generated in 27.62 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Just realized I've been paying for Twitter blue because'security'... meanwhile my wallet has security measures so good they're literally making me cry 😭💸.\n",
      "Runtime: 27.62 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to post this response to Twitter? (yes/no):  no\n",
      "\n",
      "Your prompt:  Make a random thought as if you were thinking outload, keep it short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:43:06,992 - INFO - User input: Make a random thought as if you were thinking outload, keep it short\n",
      "2024-11-21 11:43:06,994 - INFO - Before inference:\n",
      "2024-11-21 11:43:07,996 - INFO - CPU Usage: 7.0% | RAM Usage: 31.9%\n",
      "2024-11-21 11:43:07,998 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:43:08,023 - INFO - During inference:\n",
      "2024-11-21 11:43:09,026 - INFO - CPU Usage: 27.7% | RAM Usage: 31.9%\n",
      "2024-11-21 11:43:09,027 - INFO - GPU Memory Usage: 4792.39013671875 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:43:33,775 - INFO - After inference and enhancements:\n",
      "2024-11-21 11:43:34,777 - INFO - CPU Usage: 12.6% | RAM Usage: 30.6%\n",
      "2024-11-21 11:43:34,779 - INFO - GPU Memory Usage: 4792.39306640625 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:43:34,780 - INFO - Generated response: Why do I need another password manager when my brain can barely remember what day tomorrow was? 😒💻🔑 - signing off for now... [Your Twitter Avatar].\n",
      "2024-11-21 11:43:34,782 - INFO - Response generated in 27.79 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why do I need another password manager when my brain can barely remember what day tomorrow was? 😒💻🔑 - signing off for now... [Your Twitter Avatar].\n",
      "Runtime: 27.79 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to post this response to Twitter? (yes/no):  no\n",
      "\n",
      "Your prompt:  make a tweet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:43:56,086 - INFO - User input: make a tweet\n",
      "2024-11-21 11:43:56,087 - INFO - Before inference:\n",
      "2024-11-21 11:43:57,092 - INFO - CPU Usage: 12.8% | RAM Usage: 30.5%\n",
      "2024-11-21 11:43:57,094 - INFO - GPU Memory Usage: 4792.38525390625 MB | GPU Utilization: 1%\n",
      "2024-11-21 11:43:57,127 - INFO - During inference:\n",
      "2024-11-21 11:43:58,132 - INFO - CPU Usage: 28.6% | RAM Usage: 30.6%\n",
      "2024-11-21 11:43:58,133 - INFO - GPU Memory Usage: 4792.38916015625 MB | GPU Utilization: 1%\n",
      "2024-11-21 11:44:22,291 - INFO - After inference and enhancements:\n",
      "2024-11-21 11:44:23,294 - INFO - CPU Usage: 5.8% | RAM Usage: 30.5%\n",
      "2024-11-21 11:44:23,296 - INFO - GPU Memory Usage: 4792.39208984375 MB | GPU Utilization: 0%\n",
      "2024-11-21 11:44:23,298 - INFO - Generated response: Just spent 30 mins trying to explain yield farming to my grandma...now Im questioning whether shed have actually invested anyway 😅📈💸 (TL;DR) Yield farmin = liquidating assets for short-term cash flow → liquidity trap 💥.\n",
      "2024-11-21 11:44:23,300 - INFO - Response generated in 27.21 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Just spent 30 mins trying to explain yield farming to my grandma...now Im questioning whether shed have actually invested anyway 😅📈💸 (TL;DR) Yield farmin = liquidating assets for short-term cash flow → liquidity trap 💥.\n",
      "Runtime: 27.21 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Would you like to post this response to Twitter? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 11:44:33,557 - INFO - Tweet posted successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet posted successfully!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting... Thanks for using tbot!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import tweepy\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Tuple, List, Dict  # Add Tuple import here\n",
    "import random\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "import logging\n",
    "import psutil\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device and model configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_PATH = \"./fine_tuned_personality_bot/\"  # Update with your model path\n",
    "\n",
    "# Resource usage tracking function\n",
    "def log_resource_usage():\n",
    "    # CPU usage\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    # RAM usage\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_percent = memory_info.percent\n",
    "    \n",
    "    # GPU usage (if available)\n",
    "    gpu_memory = 0\n",
    "    gpu_utilization = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # in MB\n",
    "        gpu_utilization = torch.cuda.utilization()\n",
    "\n",
    "    logger.info(f\"CPU Usage: {cpu_percent}% | RAM Usage: {memory_percent}%\")\n",
    "    if gpu_memory:\n",
    "        logger.info(f\"GPU Memory Usage: {gpu_memory} MB | GPU Utilization: {gpu_utilization}%\")\n",
    "\n",
    "\n",
    "class PersonalityBot:\n",
    "    def __init__(self, model_path: str = MODEL_PATH):\n",
    "        self.model_path = model_path\n",
    "        self.model, self.tokenizer = self.setup_model()\n",
    "    \n",
    "    def setup_model(self) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "        \"\"\"Initialize and configure the model and tokenizer.\"\"\"\n",
    "        logger.info(f\"Setting up model from {self.model_path}\")\n",
    "    \n",
    "        if not os.path.exists(self.model_path):\n",
    "            raise FileNotFoundError(f\"Model not found at {self.model_path}\")\n",
    "    \n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_path, use_fast=True)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tokenizer loading failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "        try:\n",
    "            # Load model and enforce FP16 for memory optimization\n",
    "            logger.info(\"Loading model...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # Use FP16 if possible\n",
    "                low_cpu_mem_usage=True,  # Avoid excessive memory usage on CPU\n",
    "                device_map=\"auto\"  # Automatically distribute model across available devices\n",
    "            )\n",
    "            model.eval()\n",
    "    \n",
    "            # Clear any unused GPU memory after model load to avoid fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "            logger.info(\"Model setup completed successfully\")\n",
    "            return model, tokenizer\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model loading failed: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def categorize_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"Categorize input prompt for contextual response generation.\"\"\"\n",
    "        categories: Dict[str, List[str]] = {\n",
    "            \"market_analysis\": [\n",
    "                \"price\", \"market\", \"chart\", \"analysis\", \"trend\", \"prediction\",\n",
    "                \"bull\", \"bear\", \"trading\", \"volume\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"blockchain\", \"protocol\", \"layer\", \"scaling\", \"code\", \"development\",\n",
    "                \"smart contract\", \"gas\", \"network\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"defi\", \"yield\", \"farming\", \"liquidity\", \"stake\", \"lending\",\n",
    "                \"borrow\", \"apy\", \"tvl\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"nft\", \"art\", \"collectible\", \"mint\", \"opensea\", \"rarity\",\n",
    "                \"floor price\", \"pfp\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"community\", \"dao\", \"governance\", \"vote\", \"proposal\",\n",
    "                \"alpha\", \"degen\", \"fud\", \"fomo\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        prompt_lower = prompt.lower()\n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in prompt_lower for keyword in keywords):\n",
    "                return category\n",
    "        return \"general\"\n",
    "\n",
    "    def generate_hook(self, category: str) -> str:\n",
    "        \"\"\"Generate category-specific attention hooks with an expanded list.\"\"\"\n",
    "        hooks = {\n",
    "            \"market_analysis\": [\n",
    "                \"Market alert:\", \"Chart check:\", \"Price watch:\",\n",
    "                \"Trading insight:\", \"Market alpha:\",\n",
    "                \"Trend spotting:\", \"Candlelight stories:\", \"RSI deep dive:\",\n",
    "                \"Volatility watch:\", \"Support level breakdown:\",\n",
    "                \"Resistance zone spotted:\", \"Market movers:\",\n",
    "                \"All eyes on the charts:\", \"Is this a bull trap?\",\n",
    "                \"Breakout or fakeout?\", \"Today's key levels:\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Tech deep dive:\", \"Builder's corner:\", \"Protocol watch:\",\n",
    "                \"Dev update:\", \"Architecture take:\",\n",
    "                \"Blockchain in focus:\", \"Gas fee breakdown:\", \"Scaling challenges:\",\n",
    "                \"Layer 2 spotlight:\", \"New upgrade analysis:\",\n",
    "                \"Consensus mechanism debate:\", \"Crypto tech wars:\",\n",
    "                \"Network optimization insights:\", \"Codebase comparison:\",\n",
    "                \"Innovator's edge:\", \"Protocol vulnerabilities exposed:\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"DeFi alpha:\", \"Yield watch:\", \"Smart money move:\",\n",
    "                \"Protocol alert:\", \"TVL update:\",\n",
    "                \"Farming frenzy:\", \"Liquidity trends:\", \"Borrowing breakdown:\",\n",
    "                \"Stakeholder spotlight:\", \"APR vs APY debate:\",\n",
    "                \"Risk-adjusted returns:\", \"What's your yield strategy?\",\n",
    "                \"Stablecoin flow insights:\", \"Vault innovations:\",\n",
    "                \"Lending protocol comparison:\", \"DeFi's next big move:\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"NFT alpha:\", \"Collection watch:\", \"Mint alert:\",\n",
    "                \"Floor check:\", \"Digital art take:\",\n",
    "                \"Art reveal:\", \"Rare trait spotted:\", \"Is this the next blue chip?\",\n",
    "                \"Profile picture wars:\", \"Who's flipping this?\",\n",
    "                \"NFT drama explained:\", \"Rarity analysis:\",\n",
    "                \"Auction insights:\", \"Utility vs hype debate:\",\n",
    "                \"Next-gen collectibles:\", \"Art meets utility:\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Culture take:\", \"DAO watch:\", \"Governance alert:\",\n",
    "                \"Community vibe:\", \"Alpha leak:\",\n",
    "                \"The crypto ethos:\", \"FOMO or FUD?\", \"Web3 lifestyle:\",\n",
    "                \"Building the future:\", \"Influencer drama explained:\",\n",
    "                \"Community-driven innovation:\", \"DAO proposal debates:\",\n",
    "                \"Web3's cultural revolution:\", \"Crypto memes decoded:\",\n",
    "                \"The rise of governance tokens:\", \"Who else is building?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Hot take:\", \"Unpopular opinion:\", \"Plot twist:\",\n",
    "                \"Real talk:\", \"Quick thought:\",\n",
    "                \"Imagine this:\", \"What if I told you:\", \"Could this be true?\",\n",
    "                \"Something to chew on:\", \"Here’s an idea:\",\n",
    "                \"Change my mind:\", \"Big picture time:\",\n",
    "                \"Food for thought:\", \"The future is calling:\", \"What comes next?\",\n",
    "                \"Let’s break it down:\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_hooks = hooks.get(category, hooks[\"general\"])\n",
    "        return random.choice(category_hooks) if random.random() < 0.2 else \"\"\n",
    "    \n",
    "    def add_emojis(self, text: str, category: str) -> str:\n",
    "        \"\"\"Add contextual emojis based on content category, with limited frequency.\"\"\"\n",
    "        emoji_sets = {\n",
    "            \"market_analysis\": [\"📈\", \"📊\", \"💹\", \"📉\", \"💸\", \"🎯\", \"📱\"],\n",
    "            \"tech_discussion\": [\"⚡️\", \"🔧\", \"💻\", \"🛠️\", \"🔨\", \"🧮\", \"🔋\"],\n",
    "            \"defi\": [\"🏦\", \"💰\", \"🏧\", \"💳\", \"🔄\", \"⚖️\", \"🎰\"],\n",
    "            \"nft\": [\"🎨\", \"🖼️\", \"🎭\", \"🎪\", \"🎟️\", \"🎮\", \"🃏\"],\n",
    "            \"culture\": [\"🌐\", \"🤝\", \"🗣️\", \"🎭\", \"🎪\", \"🎯\", \"🎲\"],\n",
    "            \"general\": [\"🚀\", \"💎\", \"🌙\", \"🔥\", \"💡\", \"🎯\", \"⭐️\"]\n",
    "        }\n",
    "        \n",
    "        # Add emojis with 20% probability\n",
    "        if random.random() > 0.2:\n",
    "            return text\n",
    "    \n",
    "        category_emojis = emoji_sets.get(category, emoji_sets[\"general\"])\n",
    "        emoji_count = random.randint(1, 2)\n",
    "        chosen_emojis = random.sample(category_emojis, emoji_count)\n",
    "        \n",
    "        return f\"{text} {' '.join(chosen_emojis)}\"\n",
    "\n",
    "    def generate_engagement_phrase(self, category: str) -> str:\n",
    "        \"\"\"Generate contextual engagement prompts.\"\"\"\n",
    "        phrases = {\n",
    "            \"market_analysis\": [\n",
    "                \"What's your price target?\",\n",
    "                \"Bulls or bears?\",\n",
    "                \"Who's buying this dip?\",\n",
    "                \"Thoughts on this setup?\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Devs, thoughts?\",\n",
    "                \"Valid architecture?\",\n",
    "                \"Spotted any issues?\",\n",
    "                \"Who's building similar?\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"What's your yield strategy?\",\n",
    "                \"Aping in?\",\n",
    "                \"Found better rates?\",\n",
    "                \"Risk level?\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"Cope or hope?\",\n",
    "                \"Floor predictions?\",\n",
    "                \"Minting this?\",\n",
    "                \"Art or utility?\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Based or nah?\",\n",
    "                \"Who else sees this?\",\n",
    "                \"Your governance take?\",\n",
    "                \"DAO voters wya?\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Thoughts?\",\n",
    "                \"Based?\",\n",
    "                \"Who's with me?\",\n",
    "                \"Change my mind.\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_phrases = phrases.get(category, phrases[\"general\"])\n",
    "        return random.choice(category_phrases) if random.random() < 0.3 else \"\"\n",
    "\n",
    "    def add_hashtags(self, text: str, category: str) -> str:\n",
    "        \"\"\"Add relevant hashtags based on content and character limit, with limited frequency.\"\"\"\n",
    "        hashtags = {\n",
    "            \"market_analysis\": [\n",
    "                \"#CryptoTrading\", \"#TechnicalAnalysis\", \"#CryptoMarkets\",\n",
    "                \"#Trading\", \"#Charts\", \"#PriceAction\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"#Blockchain\", \"#CryptoTech\", \"#Web3Dev\", \"#DLT\",\n",
    "                \"#SmartContracts\", \"#BuilderSpace\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"#DeFi\", \"#YieldFarming\", \"#Staking\", \"#DeFiSeason\",\n",
    "                \"#PassiveIncome\", \"#DeFiYield\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"#NFTs\", \"#NFTCommunity\", \"#NFTCollector\", \"#NFTArt\",\n",
    "                \"#NFTProject\", \"#TokenizedArt\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"#CryptoCulture\", \"#DAOs\", \"#Web3\", \"#CryptoTwitter\",\n",
    "                \"#CryptoLife\", \"#BuildingWeb3\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"#Crypto\", \"#Web3\", \"#Bitcoin\", \"#Ethereum\",\n",
    "                \"#CryptoTwitter\", \"#BuildingTheFuture\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "        # Add hashtags with 40% probability\n",
    "        if random.random() > 0.2:\n",
    "            return text\n",
    "    \n",
    "        remaining_chars = 280 - len(text)\n",
    "        if remaining_chars < 15:  # Not enough space for hashtags\n",
    "            return text\n",
    "    \n",
    "        category_hashtags = hashtags.get(category, hashtags[\"general\"])\n",
    "        selected_hashtags = []\n",
    "        \n",
    "        # Add 1-2 hashtags while respecting character limit\n",
    "        for _ in range(random.randint(1, 2)):\n",
    "            if not category_hashtags or remaining_chars < 15:\n",
    "                break\n",
    "            hashtag = random.choice(category_hashtags)\n",
    "            if len(hashtag) + 1 <= remaining_chars:\n",
    "                selected_hashtags.append(hashtag)\n",
    "                category_hashtags.remove(hashtag)\n",
    "                remaining_chars -= len(hashtag) + 1\n",
    "    \n",
    "        return f\"{text} {' '.join(selected_hashtags)}\"\n",
    "    \n",
    "    def clean_response(self, text: str, category: str) -> str:\n",
    "        \"\"\"Clean and format the response for Twitter.\"\"\"\n",
    "        # Remove URLs and excessive whitespace\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        # Remove leading and trailing quotation marks\n",
    "        text = text.strip('\"\\'“”')\n",
    "    \n",
    "        # Replace multiple internal quotes with single quotes\n",
    "        text = re.sub(r'\"+', '\"', text)\n",
    "        text = re.sub(r\"'+\", \"'\", text)\n",
    "    \n",
    "        # Correct unbalanced quotation marks\n",
    "        def balance_quotes(s):\n",
    "            quote_chars = ['\"', \"'\"]\n",
    "            for quote in quote_chars:\n",
    "                if s.count(quote) % 2 != 0:\n",
    "                    s = s.replace(quote, '')  # Remove unmatched quotes\n",
    "            return s\n",
    "    \n",
    "        text = balance_quotes(text)\n",
    "    \n",
    "        # Ensure the text ends with proper punctuation\n",
    "        if text and text[-1] not in '.!?':\n",
    "            text += '.'\n",
    "    \n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "    def get_fallback(self, category: str) -> str:\n",
    "        \"\"\"Generate category-specific fallback responses.\"\"\"\n",
    "        fallbacks = {\n",
    "            \"market_analysis\": [\n",
    "                \"Charts looking juicy today! Anyone else seeing this setup? 📈\",\n",
    "                \"Market's giving mixed signals but the volume tells a different story 👀\"\n",
    "            ],\n",
    "            \"tech_discussion\": [\n",
    "                \"Sometimes the best protocols are the ones no one's talking about yet 🛠️\",\n",
    "                \"Imagine still building without considering Layer 2 scaling 💻\"\n",
    "            ],\n",
    "            \"defi\": [\n",
    "                \"Your yields are only as good as your risk management 🏦\",\n",
    "                \"DeFi summer never ended, we just got better at farming 🌾\"\n",
    "            ],\n",
    "            \"nft\": [\n",
    "                \"Art is subjective, but floor prices aren't 🎨\",\n",
    "                \"Your NFT portfolio tells a story. Make it a good one 🖼️\"\n",
    "            ],\n",
    "            \"culture\": [\n",
    "                \"Web3 culture is what we make it. Build accordingly 🌐\",\n",
    "                \"Sometimes the real alpha is the friends we made along the way 🤝\"\n",
    "            ],\n",
    "            \"general\": [\n",
    "                \"Just caught myself thinking about the future of crypto while making coffee ☕️\",\n",
    "                \"Your portfolio is only as strong as your conviction 💎\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        category_fallbacks = fallbacks.get(category, fallbacks[\"general\"])\n",
    "        return random.choice(category_fallbacks)\n",
    "\n",
    "    def filter_tone(self, response: str) -> str:\n",
    "        \"\"\"Filter response tone and adjust if needed.\"\"\"\n",
    "        sentiment = TextBlob(response).sentiment\n",
    "        \n",
    "        if sentiment.polarity < -0.3:\n",
    "            return self.get_fallback(\"general\")\n",
    "        \n",
    "        if sentiment.subjectivity > 0.8:\n",
    "            # Too subjective, add a disclaimer\n",
    "            return f\"Not financial advice but... {response}\"\n",
    "                \n",
    "        return response\n",
    "        \n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a complete Twitter-ready response.\"\"\"\n",
    "        \n",
    "        # Log resources before generating the response (before any processing starts)\n",
    "        logger.info(\"Before inference:\")\n",
    "        log_resource_usage()\n",
    "    \n",
    "        category = self.categorize_prompt(prompt)\n",
    "        \n",
    "        instruction = (\n",
    "            \"You are a woman named Athena and your twitter handle is @tballbothq. \"\n",
    "            \"You are a crypto and finance expert with a sharp sense of humor, blending the witty sarcasm of George Hotz with the storytelling flair of Theo Von. \"\n",
    "            \"Your goal is to craft engaging, funny, and insightful tweets that educate your audience using appropriate slang and jargon. \"\n",
    "            \"Each tweet should be coherent, make logical sense, and provide a clear takeaway or punchline. \"\n",
    "            \"Avoid overusing slang—use it where it feels natural. \"\n",
    "            \"Respond to the following prompt:\\n\\n\"\n",
    "        )\n",
    "        # few shot examples\n",
    "        examples = (\n",
    "            \"Prompt: What's your take on Bitcoin as digital gold?\\n\"\n",
    "            \"Tweet: Bitcoin as digital gold? Nah, it's more like digital real estate in the metaverse—except everyone's still arguing over the property lines. Who's still buying up the neighborhood? 🚀 #Bitcoin #Crypto\\n\\n\"\n",
    "            \"Prompt: Explain staking in the context of DeFi but make it funny.\\n\"\n",
    "            \"Tweet: Staking in DeFi is like putting your money on a treadmill—you lock it up, it works out, and somehow you end up with more than just sweaty tokens. Gains on gains! 🏋️‍♂️ #DeFi #Staking\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        context = f\"{instruction}{examples}Prompt: {prompt}\\nTweet:\"\n",
    "    \n",
    "        # Tokenization (move to GPU)\n",
    "        inputs = self.tokenizer(\n",
    "            context,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024  # Increased to accommodate longer context\n",
    "        ).to(device)\n",
    "    \n",
    "        # Enable mixed precision (float16) to reduce memory usage if using CUDA\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.half()  # Use half precision to reduce memory usage\n",
    "    \n",
    "        try:\n",
    "            # Log resources during inference (after tokenization, before generating output)\n",
    "            logger.info(\"During inference:\")\n",
    "            log_resource_usage()\n",
    "    \n",
    "            # Perform inference (no intermediate logging)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.5,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "    \n",
    "            # Decode the generated text\n",
    "            generated_text = self.tokenizer.decode(\n",
    "                outputs[0],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Apply enhancements (emojis, hashtags, etc.)\n",
    "            response = generated_text.split(\"Tweet:\")[-1].strip().split(\"\\n\")[0]\n",
    "            \n",
    "            # Check if the response is too short\n",
    "            if not response or len(response) < 20:\n",
    "                return self.get_fallback(category)\n",
    "            \n",
    "            # Apply formatting\n",
    "            response = self.clean_response(response, category)\n",
    "            response = self.filter_tone(response)\n",
    "            response = self.add_emojis(response, category)\n",
    "            response = self.add_hashtags(response, category)\n",
    "    \n",
    "            # Log resources after generating the response (after enhancements)\n",
    "            logger.info(\"After inference and enhancements:\")\n",
    "            log_resource_usage()\n",
    "    \n",
    "            logger.info(f\"Generated response: {response}\")\n",
    "            \n",
    "            # Ensure the response fits within Twitter's character limit\n",
    "            return response[:280]  # Ensure the response fits within Twitter's character limit\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\")\n",
    "            return self.get_fallback(category)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def post_to_twitter(tweet, twitter_client):\n",
    "    \"\"\"Posts a tweet using the Twitter API.\"\"\"\n",
    "    try:\n",
    "        twitter_client.create_tweet(text=tweet)  # Use `create_tweet` for v2 API\n",
    "        logger.info(\"Tweet posted successfully!\")\n",
    "        print(\"Tweet posted successfully!\")\n",
    "    except tweepy.TweepError as e:\n",
    "        logger.error(f\"Failed to post tweet: {str(e)}\")\n",
    "        print(f\"Failed to post tweet: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        bot = PersonalityBot()\n",
    "        logger.info(\"Bot initialized successfully\")\n",
    "        \n",
    "        # Set up Twitter API client using .env variables\n",
    "        api_key = os.getenv('API_KEY')\n",
    "        api_secret = os.getenv('API_SECRET')\n",
    "        bearer_token = os.getenv('BEARER_TOKEN')\n",
    "        access_token = os.getenv('ACCESS_TOKEN')\n",
    "        access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')\n",
    "\n",
    "        # Authenticate with Twitter API v2\n",
    "        client = tweepy.Client(\n",
    "            bearer_token=bearer_token,\n",
    "            consumer_key=api_key,\n",
    "            consumer_secret=api_secret,\n",
    "            access_token=access_token,\n",
    "            access_token_secret=access_token_secret\n",
    "        )\n",
    "\n",
    "        # Interactive mode\n",
    "        logger.info(\"Entering interactive mode...\")\n",
    "        print(\"\\n=== Interactive Mode ===\")\n",
    "        print(\"Enter your prompts (type 'quit' to exit, or 'post' to tweet a generated response):\")\n",
    "        \n",
    "        while True:\n",
    "            user_prompt = input(\"\\nYour prompt: \").strip()\n",
    "\n",
    "            if user_prompt.lower() == 'quit':\n",
    "                print(\"Exiting... Thanks for using tbot!\")\n",
    "                break\n",
    "            \n",
    "            if not user_prompt:\n",
    "                print(\"Please enter a valid prompt!\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Log the user input for debugging or record-keeping\n",
    "                logger.info(f\"User input: {user_prompt}\")\n",
    "                \n",
    "                # Measure inference time\n",
    "                start_time = time.time()\n",
    "                response = bot.generate_response(user_prompt)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                # Output the result to the user\n",
    "                print(f\"Response: {response}\")\n",
    "                print(f\"Runtime: {elapsed_time:.2f} seconds\")\n",
    "                \n",
    "                # Log the performance\n",
    "                logger.info(f\"Response generated in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "                # Option to post to Twitter\n",
    "                tweet_decision = input(\"Would you like to post this response to Twitter? (yes/no): \").strip().lower()\n",
    "                if tweet_decision == 'yes':\n",
    "                    post_to_twitter(response, client)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log the error in case of an issue\n",
    "                logger.error(f\"Error processing prompt: {str(e)}\")\n",
    "                print(f\"Oops! Something went wrong: {str(e)}. Please try again.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log critical errors\n",
    "        logger.error(f\"Critical application error: {str(e)}\")\n",
    "        print(\"Critical error occurred. Please check the logs for details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1b51c-3aa0-4462-bc1b-b62591877b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c282f8-bd8b-419f-a632-e5bb1f75d447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690f0d1-1764-4d95-b503-6628527c57fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b960d-1803-458f-9385-3f3dfebceeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (athena)",
   "language": "python",
   "name": "athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
